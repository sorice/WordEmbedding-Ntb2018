{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tf-Idf Text Representation Model\n",
    "\n",
    "**Prerequisites:** Skills in tokenization with nltk, conceptual knowledge about TfIdf Text Representation model.\n",
    "\n",
    "**data:** Gutenberg Corpus\n",
    "\n",
    "## Outline\n",
    "\n",
    "**Main Goal:** To practice how to create Tf-Idf model with Gensim Tf-Idf implementations, using NLTK preprocessing. Then introduce how to extract features from this text representation model, and finally how to measure text similarity using the previous result.\n",
    "\n",
    "- Acquiring and wrangling data for model initialization. \n",
    "- Gensim TfIdf model generation/loading example\n",
    "- Sklearn, Scipy text similarity measures examples\n",
    "- Gensim original measures examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About Gensim\n",
    "\n",
    "Gensim is a Python library for *topic modelling*, *document indexing*\n",
    "and *similarity retrieval* with large corpora. Target audience is the\n",
    "*natural language processing* (NLP) and *information retrieval* (IR)\n",
    "community. [Gensim Documentation](https://radimrehurek.com/gensim/tutorial.html)\n",
    "\n",
    "## About NLTK\n",
    "\n",
    "Natural Language ToolKit (NLTK) is a comprehensive Python library for natural language\n",
    "processing and text analytics. Originally designed for teaching, it has been adopted in the\n",
    "industry for research and development due to its usefulness and breadth of coverage. NLTK\n",
    "is often used for rapid prototyping of text processing programs and can even be used in\n",
    "production applications. [(Perkins2014)](#Perkins2014)\n",
    "\n",
    "## What is TfIdf?\n",
    "\n",
    "In information retrieval, tf–idf or TFIDF, short for term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus [(Salton1983)](#Salton1983)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import numpy as np\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.corpora import TextCorpus, MmCorpus, Dictionary\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_path = '/media/DATA/wiki_es/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Acquiring & Wrangling Data\n",
    "\n",
    "From txt collection to a list of strings, and from string-list to a list of word-list by sentence-list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_collection = []\n",
    "file_path = 'data/gutenberg/'\n",
    "file_list = list(os.popen('ls '+ file_path).read().split('\\n'))\n",
    "for file in file_list:\n",
    "    if file:\n",
    "        with open(os.path.join(file_path,file)) as doc:\n",
    "            doc_collection.append(doc.read().lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n"
     ]
    }
   ],
   "source": [
    "tokenized_text = [[word for word in nltk.word_tokenize(doc)] for doc in doc_collection]\n",
    "\n",
    "print(len(tokenized_text))\n",
    "# load nltk's English stopwords as variable called 'stopwords'\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "#remove stop words\n",
    "texts = [[word for word in text if word not in stopwords] for text in tokenized_text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Generating the Tf-Idf Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-generated model TfIdf in 1.897 seconds.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    tfidf = TfidfModel.load('models/gutenberg_tfidf.model')\n",
    "    id2word = Dictionary(texts)\n",
    "    print('Pre-generated model TfIdf in 1.897 seconds.')\n",
    "\n",
    "except:\n",
    "    init = time.time()\n",
    "    # Create dictionary with tid to token mappings (or alternatively load one)\n",
    "    id2word = Dictionary(texts)\n",
    "\n",
    "    #remove extremes (similar to the min/max df step used when creating the tf-idf matrix)\n",
    "    #id2word.filter_extremes(no_below=2, no_above=0.6)\n",
    "\n",
    "    #convert the dictionary to a bag of words corpus for reference\n",
    "    bow_corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "    #generating the tf-idf model\n",
    "    tfidf = TfidfModel(bow_corpus,id2word=id2word)\n",
    "    end = time.time()-init\n",
    "    tfidf._smart_save('models/gutenberg_tfidf.model')\n",
    "    print('Total time %f segundos.' % end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(26576, 1)]\n",
      "2.3923174227787602\n"
     ]
    }
   ],
   "source": [
    "print(id2word.doc2bow(['alice']))\n",
    "print(tfidf.idfs[id2word.doc2bow(['alice'])[0][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('_joint_', 4.392317422778761),\n",
       " ('_just_', 4.392317422778761),\n",
       " ('_lady_', 4.392317422778761),\n",
       " ('_letting_', 4.392317422778761),\n",
       " ('_little_', 4.392317422778761),\n",
       " ('_man_', 4.392317422778761),\n",
       " ('_married_', 4.392317422778761),\n",
       " ('_marry_', 4.392317422778761),\n",
       " ('_may_', 3.3923174227787602),\n",
       " ('_me_', 4.392317422778761)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(tfidf.id2word[i],tfidf.idfs[i]) for i in range(90,100)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the Tf model is very simple, only the word, ans its related tfidf coefficient. The Gensim implementation contains a method - *\\_\\_getitem\\_\\_* - that return the tfidf representation of an input bag of word vector. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Measuring Similarity between Pair of Sentences\n",
    "\n",
    "This section is made to show the utility of _tfidf model in an applied example. Also to show some native similarity methods of `gensim.model.TfIdfModel` class.\n",
    "\n",
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = 'the girl run into the hall'\n",
    "sentence2 = 'Here Alice run to the hall'\n",
    "\n",
    "sent1 = sentence1.lower().split()\n",
    "sent2 = sentence2.lower().split()\n",
    "\n",
    "sentence1_ws = 'girl run hall'\n",
    "sentence2_ws = 'Alice run hall'\n",
    "\n",
    "sent1s = sentence1_ws.lower().split()\n",
    "sent2s = sentence2_ws.lower().split()\n",
    "\n",
    "#If we change the sent1 by a very different meaning sent3\n",
    "sent3 = ['the','boy','eat','a','red','apple']\n",
    "sent3s = ['boy','eat','red','apple']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Wrangling Data\n",
    "\n",
    "* First: From string-sentences to bow representation of a sentence.\n",
    "* Second: From bow representation to numerical-list representation of a sentence.\n",
    "* Third: From numerical-list vector to numerical-vector (numpy) representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bow-Vec with tfidf values of sent 1 [(3236, 0.7916654345238935), (3419, 0.5554386815550553), (6105, 0.2544675044332316)]\n",
      "Numerical list of sent 1 [0.0, 0.2544675044332316, 0.5554386815550553, 0.7916654345238935]\n",
      "Numpy vector of sent 1 [[0.         0.2544675  0.55543868 0.79166543]]\n"
     ]
    }
   ],
   "source": [
    "from scripts.preprocess import wrang_tfidf\n",
    "\n",
    "bowvec_sent1_tfidf,bowvec_sent2_tfidf,nvec1,nvec2, A, B = wrang_tfidf(sent1,sent2,tfidf, id2word)\n",
    "print('Bow-Vec with tfidf values of sent 1', bowvec_sent1_tfidf)\n",
    "print('Numerical list of sent 1',nvec1)\n",
    "print('Numpy vector of sent 1', A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the []\n",
      "girl [(3236, 1)]\n",
      "run [(6105, 1)]\n",
      "into []\n",
      "the []\n",
      "hall [(3419, 1)]\n"
     ]
    }
   ],
   "source": [
    "for word in sent1:\n",
    "    print(word,id2word.doc2bow([word]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note_: seems like if this model filter the stopwords automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Sklearn TfIdf-Cosine Sentence Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1330855207623347\n",
      "0.1330855207623347\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "print(cosine_similarity(A,B)[0][0])\n",
    "\n",
    "#Filtering stopwords\n",
    "bowvec_sent1s_tfidf,bowvec_sent2s_tfidf,nvec1s,nvec2s, As, Bs = wrang_tfidf(sent1s,sent2s,tfidf,id2word)\n",
    "print(cosine_similarity(As,Bs)[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Scipy TfIdf-Cosine sentence similarity\n",
    "\n",
    "$Note: cosine_{Scipy\\ distance} = 1 - cosine_{Sklearn\\ similarity}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8669144792376653\n",
      "0.8669144792376653\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import cosine as cosine_scipy\n",
    "print(cosine_scipy(nvec1,nvec2))\n",
    "print(cosine_scipy(nvec1s,nvec2s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Best Pair Word Overlap\n",
    "\n",
    "Lets try a different way to compound a sentence similarity, based on WordNet-Augmented-Word-Overlap similarity idea.\n",
    "\n",
    "$p = {\\sum_{w\\in\\ sent_1}max(df[w][w']) \\over len(sent_1)} \\ \\ \\ \\forall\\ w' \\in\\ sent_2$\n",
    "\n",
    "$q = {\\sum_{w'\\in\\ sent_2}max(df[w][w']) \\over len(sent_2)} \\ \\ \\ \\forall\\ w \\in\\ sent_1$\n",
    "\n",
    "$sim = \\left\\{ \\begin{array}{rcl} \n",
    "0  & if\\ p+q = 0\\\\\n",
    "{2 p*q \\over (p+q)}  & others\\\\\n",
    "\\end{array}\n",
    "\\right.$\n",
    "\n",
    "Due to the unmanagability of TfIdf Gensim object, and the few examples I could get, I decided to create de TfIdf with sklearn and then manipulated, to get the tfidf vector o a word, see the example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(53415, 21)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_files\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "vectorizer = TfidfVectorizer(min_df=1, lowercase=True) #note: by default CountVectorizer, TfidfVectorizer use lowercase=True\n",
    "corpus = load_files('data/',categories=['gutenberg'])\n",
    "TfIdfMatrix = vectorizer.fit_transform(corpus.data)\n",
    "pdTfIdf = pd.DataFrame(TfIdfMatrix.toarray(), columns=vectorizer.get_feature_names())\n",
    "pdTfIdf = pdTfIdf.T\n",
    "pdTfIdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3923174227787602\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[3.24275368e-01, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        3.97026332e-04, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 1.10916056e-03, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        3.10535868e-04]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tfidf.idfs[id2word.doc2bow(['alice'])[0][0]])\n",
    "pdTfIdf.loc['alice'].values.reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dissimilar sentences tfidf_harmonic_best_pair_word similarity 0.7190818951347712\n",
      "Dissimilar sentences without stopwords tfidf_harmonic_best_pair_word similarity 0.6051764379445688\n",
      "Similar sentences tfidf_harmonic_best_pair_word 0.9119930077479885\n",
      "Similar sentences tfidf_harmonic_best_pair_word without stopwords 0.8445119942483112\n"
     ]
    }
   ],
   "source": [
    "from scripts.distances import best_pair_word_overlap\n",
    "\n",
    "print('Dissimilar sentences tfidf_harmonic_best_pair_word similarity', \n",
    "      best_pair_word_overlap(sent3, sent2, pdTfIdf))\n",
    "print('Dissimilar sentences without stopwords tfidf_harmonic_best_pair_word similarity',\n",
    "      best_pair_word_overlap(sent3s, sent2s, pdTfIdf))\n",
    "print('Similar sentences tfidf_harmonic_best_pair_word', \n",
    "      best_pair_word_overlap(sent1, sent2, pdTfIdf))\n",
    "print('Similar sentences tfidf_harmonic_best_pair_word without stopwords',\n",
    "      best_pair_word_overlap(sent1s, sent2s, pdTfIdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Gensim Original Measures\n",
    "\n",
    "Gensim has some native similarity measures, some of them are only implemented in some models.\n",
    "\n",
    "## 4.1 Gensim tfidf.n_similarity\n",
    "\n",
    "The `n_similarity` method will be showed in word2vec and paragraph2vec models. TfIdf doesn't have this kind of internal similarity methods - at least in Gensim package - (Press 'Tab' key in the next cell to check it!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-13-70292b2a3dfb>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-13-70292b2a3dfb>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    tfidf.\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "tfidf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Gensim TfIdf-Hellinger sentence similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9744522138374331\n",
      "0.9744522138374331\n",
      "inf\n"
     ]
    }
   ],
   "source": [
    "from gensim.matutils import kullback_leibler, jaccard, hellinger, cossim\n",
    "\n",
    "print(hellinger(bowvec_sent1_tfidf,bowvec_sent2_tfidf))\n",
    "print(hellinger(A,B))\n",
    "print(kullback_leibler(A, B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gensim Cosine: 0.13308552076233474\n",
      "Gensim Cosine, filtering stopwords: 0.13308552076233474\n",
      "Gensim Jaccard: 0.8992553737425453\n"
     ]
    }
   ],
   "source": [
    "print('Gensim Cosine:',cossim(bowvec_sent1_tfidf,bowvec_sent2_tfidf))\n",
    "print('Gensim Cosine, filtering stopwords:',cossim(bowvec_sent1_tfidf,bowvec_sent2_tfidf))\n",
    "print('Gensim Jaccard:',jaccard(bowvec_sent1_tfidf,bowvec_sent2_tfidf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "* As you can test the TfIdf doesn't have a fast or parallel solution.\n",
    "* In Gensim TfIdf model is generated from bowvecs.\n",
    "* There is a good variation between Cosine, Word Overlap and Hellinger, this could be interesting to analize in a big dataset.\n",
    "* Interesting too is that Gensim and Sklearn cosine have the same result.\n",
    "* TfIdf Model filter stopword automatically, then the similarity comparison between original sentences and preprocessed sentences are equal.\n",
    "* ``harmonic_best_pair_word_sim`` distance separated very well between similar and dissimilar sentences without stopwords.\n",
    "\n",
    "# Recommendations\n",
    "\n",
    "* Made the same example with Wikipedia dump data, to test the similarity difference according to data. This experiment is showed in the [2.2-TfIdf-Wikipedia](2.2-TfIdf-Wikipedia)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='References'></a>\n",
    "# References\n",
    "\n",
    "<a id='Perkins2014'></a>\n",
    "*[Perkins2014]* Jacov Perkins. \n",
    "Book **Python 3 Text Processing with NLTK 3 Cookbook**. 2014. \n",
    "p. 7 **ISBN**: 978-1-78216-785-3\n",
    "\n",
    "*[Salton1983]* Salton, G; McGill, M. J. (1986). **Introduction to modern information retrieval**. McGraw-Hill. \n",
    "**ISBN**: 978-0-07-054484-0.\n",
    "<a id='Salton1983'></a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity in Paragraph2Vec Text Representation\n",
    "\n",
    "*Gensim software examples.*\n",
    "\n",
    "**Prerequisites:** Skills in tokenization with nltk, knowledge of Word2Vec Text Representation model.\n",
    "\n",
    "## Outline\n",
    "\n",
    "**Main Goal:** To practice how to create paragraph2vec models with Gensim and NLTK. Then introduce how to extract information from both text representation, and finally how to measure word similarity.\n",
    "\n",
    "- Acquiring and wrangling data for model initialization. \n",
    "- Gensim paragraph2vec model generation/loading example\n",
    "- Sklearn, Scipy text similarity measures examples\n",
    "- Gensim original measures examples\n",
    "\n",
    "**About Paragrah2Vec**: Read the [2.9-Paragraph2Vec](2.9-Paragraph2Vec.ipynb) notebook.\n",
    "\n",
    "**Note**: About Gensim and NLTK software please read the introductions notes about them in [2.1-TfIdf Notebook](02.1-TfIdf.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import smart_open\n",
    "import gensim\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.corpora import TextCorpus, MmCorpus, Dictionary\n",
    "from gensim.corpora.wikicorpus import WikiCorpus\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change this configurations with your paths to Wiki corpus\n",
    "data_path = '/opt/wiki_es/'\n",
    "output_path = '/media/abelma/SSD2/wiki_es/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Acquiring & Wrangling Data\n",
    "\n",
    "From txt collection to a WikiCorpus object, then to a TaggedWikiDocument that it is the input-type for the collection of docs that accept `p2v.build_vocab` and `p2v.train` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1523542334.6557834\n",
      "----------- 100000\n",
      "1523542834.5501635\n",
      "----------- 200000\n",
      "1523543149.6039762\n",
      "----------- 300000\n",
      "1523543429.2024624\n",
      "----------- 400000\n",
      "1523543773.0984907\n",
      "----------- 500000\n",
      "1523544246.1291332\n",
      "----------- 600000\n",
      "1523544657.153675\n",
      "----------- 700000\n",
      "1523544998.0905125\n",
      "----------- 800000\n",
      "1523545415.5849423\n",
      "----------- 900000\n",
      "1523545860.8929534\n",
      "----------- 1000000\n",
      "1523546300.8007684\n",
      "----------- 1100000\n",
      "1523546702.1207345\n"
     ]
    }
   ],
   "source": [
    "init = time.time()\n",
    "print(init)\n",
    "wiki_corpus = WikiCorpus(data_path+'dump/eswiki-20161201-pages-articles-multistream.xml.bz2',\n",
    "                         lemmatize=False,\n",
    "                         article_min_tokens=50,     #Minimum tokens in article.\n",
    "                         article_max_tokens=5000,\n",
    "                         dictionary={})\n",
    "end = time.time()-init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After 69.5min the WikiCorpus object of 2016-12_Spanish_Wikipedia dump was uploaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaggedWikiDocument(object):\n",
    "    def __init__(self, wiki):\n",
    "        self.wiki = wiki\n",
    "        self.wiki.metadata = True\n",
    "    def __iter__(self):\n",
    "        for content, (page_id, title) in self.wiki.get_texts():\n",
    "            yield TaggedDocument([c for c in content], [title])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tagging docs to generate input-corpus for paragraph2vec model in 0.000130 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abelma/wordembd/lib/python3.5/site-packages/gensim/interfaces.py:60: UserWarning: corpus.save() stores only the (tiny) iteration object; to serialize the actual corpus content, use e.g. MmCorpus.serialize(corpus)\n",
      "  \"corpus.save() stores only the (tiny) iteration object; \"\n"
     ]
    }
   ],
   "source": [
    "init = time.time()\n",
    "documents = TaggedWikiDocument(wiki_corpus)\n",
    "end = time.time()-init\n",
    "print('Tagging docs to generate input-corpus for paragraph2vec model in %f seconds' % end)\n",
    "documents.wiki.save(co 'TaggedWiki.documents')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Generating the Paragraph2Vec Model\n",
    "\n",
    "_Note:_ This model apply a tokens lowercarse automatically, for that reason sentences are lowercased."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/abelma/SSD2/wiki_es/wiki-p2v.model\n",
      "Paragraph2Vec Model vocabulary loaded in 139.031553 seconds\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    init = time.time()\n",
    "    p2v = Doc2Vec.load(output_path+'wiki-p2v.model')\n",
    "    end = time.time()-init\n",
    "    print('Paragraph2Vec Model vocabulary loaded in %f seconds' % end)\n",
    "    \n",
    "except:\n",
    "    init = time.time()\n",
    "    print('no debo entrar aquí')\n",
    "    p2v = Doc2Vec(epochs=1,                #Number of iterations (epochs) over the corpus.\n",
    "                   min_count=20,           #Ignores all words with total frequency lower than this.\n",
    "                   vector_size=300,        #Dimensionality of the feature vectors.\n",
    "                   max_vocab_size=2000000, #Limits the RAM during vocabulary building.\n",
    "                   dm=0,                   #Defines the training algorithm.\n",
    "                   workers=cores,\n",
    "                  )\n",
    "    p2v.build_vocab(documents)\n",
    "    end = time.time()\n",
    "    p2v.save(output_path+'wiki-p2v.model')\n",
    "    print('Paragraph2Vec Model vocabulary scaned in %f seconds' % end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First round: paragraph2vec vocabulary built in 52.45 min.\n",
    "\n",
    "Second round: paragraph2vec vocabulary loaded in 1.76 min.\n",
    "\n",
    "### Generating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1523548085.7003894\n",
      "----------- 100000\n",
      "1523548720.1758692\n",
      "----------- 200000\n",
      "1523549139.410583\n",
      "----------- 300000\n",
      "1523549470.3297775\n",
      "----------- 400000\n",
      "1523549827.492253\n",
      "----------- 500000\n",
      "1523550194.3657181\n",
      "----------- 600000\n",
      "1523550491.8750749\n",
      "----------- 700000\n",
      "1523550731.400874\n",
      "----------- 800000\n",
      "1523551038.9913855\n",
      "----------- 900000\n",
      "1523551356.006913\n",
      "----------- 1000000\n",
      "1523551666.4491296\n",
      "----------- 1100000\n",
      "1523551954.6985068\n",
      "Paragraph2Vec Model Generated in 3877.760819 seconds\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    init = time.time()\n",
    "    p2v = Doc2Vec.load(output_path+'wiki-p2v.model')\n",
    "    end = time.time()-init\n",
    "    print('Paragraph2Vec Model loaded in %f seconds' % end)\n",
    "    \n",
    "except:\n",
    "    init = time.time()\n",
    "    print(init)\n",
    "    p2v.train(documents,\n",
    "              total_examples=p2v.corpus_count,\n",
    "              epochs=1,)\n",
    "    end = time.time()-init\n",
    "    p2v.save(output_path+'wiki-p2v.model')\n",
    "    print('Paragraph2Vec Model Generated in %f seconds' % end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First round: paragraph2vec model trained in 65 min."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00084364,  0.00100843, -0.0013037 ,  0.00095026,  0.00102746,\n",
       "       -0.00099448,  0.00164336,  0.00095534,  0.00066178, -0.00122448],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p2v.wv['niño'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('República del Congo', 0.9746325016021729), ('Nunavut', 0.963964581489563), ('Chechenia', 0.9632564783096313), ('Burundi', 0.9623643755912781), ('Libertador San Martín', 0.9616067409515381), ('Cuetzala del Progreso', 0.9613605737686157), ('Islas Baleares', 0.9603628516197205), ('Puerto Maldonado', 0.9602200388908386), ('Caquetá', 0.9595517516136169), ('Isla Grande de Chiloé', 0.9593847393989563)]\n",
      "[('estadística', 0.26488637924194336), ('magari', 0.25142616033554077), ('swordfish', 0.24946871399879456), ('kiotenses', 0.24445898830890656), ('afroecuatoriano', 0.2428331971168518), ('peraleja', 0.23987072706222534), ('revin', 0.23189276456832886), ('illapa', 0.22987493872642517), ('horatio', 0.22923138737678528), ('aculturación', 0.22797220945358276)]\n"
     ]
    }
   ],
   "source": [
    "print(p2v.docvecs.most_similar(positive='Argentina'))\n",
    "print(p2v.wv.most_similar(positive='argentina'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in the above example the docvecs is useful for expressions that appier in `p2v.docvecs.doctags`, otherwise the model will rise an error. If you use word vector similarity (`wv.most_similar`) the results are worst than `docvecs.most_similar`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8469448991224805"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p2v.docvecs.n_similarity(['Argentina'],['Congo'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Measuring Similarity between Pair of Sentences\n",
    "\n",
    "This section is made to show the utility of _paragraph2vec_ model in an applied example. Also to show some native similarity methods of `gensim.Doc2Vec` class.\n",
    "\n",
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = 'la niña corrió hacia el hueco'\n",
    "sentence2 = 'Alicia corrió hacia el hueco'\n",
    "\n",
    "sent1 = sentence1.lower().split()\n",
    "sent2 = sentence2.lower().split()\n",
    "\n",
    "#Filtering stopwords by hand\n",
    "sent1s = 'niña corrió hueco'\n",
    "sent2s = 'Alicia corrió hueco'\n",
    "\n",
    "sent1sl = sent1s.lower().split()\n",
    "sent2sl = sent2s.lower().split()\n",
    "\n",
    "#If we change the sent1 by a very different meaning sent3\n",
    "sent3 = ['el','niño','comió','una','manzana','roja']\n",
    "sent3s = ['niño','comió','manzana','roja']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Wrangling Data\n",
    "\n",
    "From string sentences to word for word paragraph2vec model numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def preproc_data(sent, model):\n",
    "    \n",
    "    vec_sent = []\n",
    "\n",
    "    for i,word in enumerate(sent):\n",
    "        try:\n",
    "            vec_sent.append(model.wv[word])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    vec_sent = sum(np.asarray(vec_sent))\n",
    "    result = vec_sent.reshape(1,-1)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.00198484, -0.00161264, -0.00392045, -0.00552214,  0.00131036,\n",
       "        0.0005118 ,  0.00136406,  0.0017021 , -0.00151024, -0.00087941],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p2v_sent1 = preproc_data(sent1,p2v)\n",
    "p2v_sent2 = preproc_data(sent2,p2v)\n",
    "print(len(p2v_sent1[0]))\n",
    "p2v_sent2[0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Sklearn Paragraph2Vec-Cosine Sentence Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7499604"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cosine_similarity(p2v_sent1,p2v_sent2)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.69232756"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Filtering stopwords\n",
    "p2v_sent1s = preproc_data(sent1sl,p2v)\n",
    "p2v_sent2s = preproc_data(sent2sl,p2v)\n",
    "cosine_similarity(p2v_sent1s,p2v_sent2s)[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Scipy Cosine Similarity\n",
    "\n",
    "$Note: cosine_{Scipy\\ distance} = 1 - cosine_{Sklearn\\ similarity}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2500395178794861\n",
      "0.30767250061035156\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import cosine as cosine_scipy\n",
    "\n",
    "print(cosine_scipy(p2v_sent1,p2v_sent2))\n",
    "print(cosine_scipy(p2v_sent1s,p2v_sent2s)) #Filtering stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Best Pair Word Overlap Similarity\n",
    "\n",
    "Lets try a different way to compound a sentence similarity, based on WordNet-Augmented-Word-Overlap similarity idea.\n",
    "\n",
    "$p = {\\sum_{w\\in\\ sent_1}max(df[w][w']) \\over len(sent_1)} \\ \\ \\ \\forall\\ w' \\in\\ sent_2$\n",
    "\n",
    "$q = {\\sum_{w'\\in\\ sent_2}max(df[w][w']) \\over len(sent_2)} \\ \\ \\ \\forall\\ w \\in\\ sent_1$\n",
    "\n",
    "$sim = \\left\\{ \\begin{array}{rcl} \n",
    "0  & if\\ p+q = 0\\\\\n",
    "{2 p*q \\over (p+q)}  & others\\\\\n",
    "\\end{array}\n",
    "\\right.$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def harmonic_best_pair_word_sim(sent1,sent2,model):\n",
    "    p=0\n",
    "    for wordA in sent1:\n",
    "        m = 0\n",
    "        for wordB in sent2:\n",
    "            try:\n",
    "                m = max(m, model.wv.similarity(wordA,wordB))\n",
    "            except:\n",
    "                pass\n",
    "        p += m\n",
    "    p = p/len(sent1)\n",
    "\n",
    "    q=0\n",
    "    for wordA in sent2:\n",
    "        m = 0\n",
    "        for wordB in sent1:\n",
    "            try:\n",
    "                m = max(m, model.wv.similarity(wordA,wordB))\n",
    "            except:\n",
    "                pass\n",
    "        q += m\n",
    "    q = q/len(sent2)\n",
    "\n",
    "    sim = 2*p*q/(p+q or 1)\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dissimilar sentences w2v_harmonic_best_pair_word similarity 0.039977160321282866\n",
      "Dissimilar sentences without stopwords w2v_harmonic_best_pair_word similarity 0.0\n",
      "Similar sentences w2v_harmonic_best_pair_word 0.7476427998305525\n",
      "Similar sentences w2v_harmonic_best_pair_word without stopwords 0.6856355639043101\n"
     ]
    }
   ],
   "source": [
    "print('Dissimilar sentences w2v_harmonic_best_pair_word similarity', \n",
    "      harmonic_best_pair_word_sim(sent3, sent2, p2v))\n",
    "print('Dissimilar sentences without stopwords w2v_harmonic_best_pair_word similarity',\n",
    "      harmonic_best_pair_word_sim(sent3s, sent2s, p2v))\n",
    "print('Similar sentences w2v_harmonic_best_pair_word', \n",
    "      harmonic_best_pair_word_sim(sent1, sent2, p2v))\n",
    "print('Similar sentences w2v_harmonic_best_pair_word without stopwords',\n",
    "      harmonic_best_pair_word_sim(sent1sl, sent2sl, p2v))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Gensim Original Measures\n",
    "\n",
    "Gensim jaccard and cosine are impossible to measure because the p2v_bow vector is needed, but not exist.\n",
    "\n",
    "## 4.1 Cosine using Gensim p2v of a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p2v sentence vector similarity without transformation 0.0068915365\n",
      "p2v sentence vector similarity without transformation 0.7499604\n"
     ]
    }
   ],
   "source": [
    "vec_sent1 = p2v.wv[sent1]\n",
    "vec_sent2 = p2v.wv[sent2]\n",
    "\n",
    "vec_sent1_ = sum(vec_sent1).reshape(1,-1)\n",
    "vec_sent2_ = sum(vec_sent2).reshape(1,-1)\n",
    "\n",
    "print('p2v sentence vector similarity without transformation',\n",
    "      cosine_similarity(vec_sent1,vec_sent2)[0][0])\n",
    "print('p2v sentence vector similarity without transformation',\n",
    "      cosine_similarity(vec_sent1_,vec_sent2_)[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Gensim p2v.n_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.040381502259109046\n",
      "0.749960442719121\n",
      "0.6923275354116258\n"
     ]
    }
   ],
   "source": [
    "print(p2v.wv.n_similarity(sent3s,sent1))\n",
    "print(p2v.wv.n_similarity(sent1,sent2))\n",
    "print(p2v.wv.n_similarity(sent1sl,sent2sl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Gensim p2v.similarity\n",
    "\n",
    "A score constructed with this method based on an international article.[John2016](#John2016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.07725515726057419"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p2v.wv.similarity('hombre','mujer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_sim_jonh2016(sent1, sent2, model):\n",
    "    \"\"\":type sent1,sent2: list of strings\"\"\"\n",
    "    \n",
    "    sim_vector = []\n",
    "    ALPHA = 0.019\n",
    "\n",
    "    for wordA in sent1:\n",
    "        for wordB in sent2:\n",
    "            try:\n",
    "                sim = p2v.wv.similarity(wordA,wordB)\n",
    "                if sim > ALPHA:\n",
    "                    sim_vector.append(sim)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    return sum(sim_vector)/(len(sim_vector) or 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar sentences w2v.similarity 0.2740338124244419\n",
      "Similar sentences w2v.similarity without stopwords 0.32096874638428086\n"
     ]
    }
   ],
   "source": [
    "print('Similar sentences w2v.similarity', sent_sim_jonh2016(sent1,sent2, p2v))\n",
    "print('Similar sentences w2v.similarity without stopwords', sent_sim_jonh2016(sent1sl,sent2sl, p2v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Gensim Hellinger sentence similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abelma/wordembd/lib/python3.5/site-packages/gensim/matutils.py:906: RuntimeWarning: invalid value encountered in sqrt\n",
      "  sim = np.sqrt(0.5 * ((np.sqrt(vec1) - np.sqrt(vec2))**2).sum())\n"
     ]
    }
   ],
   "source": [
    "from gensim.matutils import kullback_leibler, hellinger\n",
    "print(hellinger(p2v_sent1,p2v_sent2))\n",
    "print(kullback_leibler(p2v_sent1,p2v_sent2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 The case of Infered Vector of a Sentences\n",
    "\n",
    "Paragraph to Vector in Gensim library has this method, which is not present in other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.04579691, -0.09673504, -0.03551013, -0.05200712, -0.01592638,\n",
       "        0.02746712, -0.08295926, -0.03992163,  0.11818437, -0.16485237],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_sent1_infer_p2v = p2v.infer_vector(sent1)\n",
    "vec_sent2_infer_p2v = p2v.infer_vector(sent2)\n",
    "\n",
    "#Stopword filtering\n",
    "vec_sent1s_infer_p2v = p2v.infer_vector(sent1sl)\n",
    "vec_sent2s_infer_p2v = p2v.infer_vector(sent2sl)\n",
    "\n",
    "#print the Paragraph2vector of the sentence 1\n",
    "print(len(vec_sent1_infer_p2v))\n",
    "vec_sent1_infer_p2v[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying Similarity on Infered Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p2v infered vector 0.74639404\n",
      "p2v infered vector without stopwords 0.56761754\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "print('p2v infered vector',\n",
    "      cosine_similarity(vec_sent1_infer_p2v.reshape(1,-1),vec_sent2_infer_p2v.reshape(1,-1))[0][0])\n",
    "print('p2v infered vector without stopwords',\n",
    "      cosine_similarity(vec_sent1s_infer_p2v.reshape(1,-1),vec_sent2s_infer_p2v.reshape(1,-1))[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "Same as Word2Vec this model doesn't works with bow structure, it represent a word as a vector of *size parameter* value length. At the same time this model can infered a vector for a sentence. The experiments shows that with the same corpus and the same sentences the paragraph2vec needs to lowercase all tokens, e.g. 'Alice' or 'Here', this behavior is different to Word2Vec model.\n",
    "\n",
    "- Here the vectors that represent sentences works better than single word vectors. See for example the difference between `p2v.wv.n_similarity(sent1,sent2)` & `p2v.wv.similarity('hombre','mujer')`\n",
    "- The best similarities using this text representation models must be implemented with innovatives ideas. For example: ``sent_sim_jonh2016`` and ``harmonic_best_pair_word_sim``.\n",
    "- In almost all cases the stopword filtering __decrement__ the similarity between similar sentences and diminished similarity between different sentences.\n",
    "- Note that `p2v.docvec.n_similarity` rise an error when have to process expressions with stopwords like \"de, el, yo,...\". Then to use this metric the stopword filtering must be applied first. Is interesting that `p2v.wv.n_similarity` dosn't have this problem.\n",
    "- The results with _infered vectors_ are return less similarity than other sentences word vector based methods like `harmonic_best_pair_word_sim`.\n",
    "- Gensim Hellinger and Kullback Leibler still been useless.\n",
    "\n",
    "\n",
    "# Recomendations\n",
    "\n",
    "* When comparations between expresions (more than one word text construction) is made, you can test what kind of expressions are very similar in the paragraph2vec model and you can play substituting it and then comparing with the other transformed query. (_see Playfull Example_)\n",
    "* Train the paragraph2vec model using the wikipedia articles bigger than 5000 words, in this notebook the parameter `article_max_tokens` was implemented by myself inside Gensim pack for lack of RAM reasons. Then try to train the model with all the articles.\n",
    "* Test the difference between loading wiki_corpus from .xml.bz2 or from .mm serialized corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playfull Example\n",
    "\n",
    "Try this little example to see the difference doing substitution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"tag 'México' not seen in training corpus/invalid\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-105-3e5e054265b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mp2v\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocvecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimilarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Argentina'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'México'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/wordembd/lib/python3.5/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36msimilarity\u001b[0;34m(self, d1, d2)\u001b[0m\n\u001b[1;32m   1268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m         \"\"\"\n\u001b[0;32m-> 1270\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munitvec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munitvec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1272\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mn_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mds1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mds2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/wordembd/lib/python3.5/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m   1081\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors_docs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_int_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoctags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_rawint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1082\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1083\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tag '%s' not seen in training corpus/invalid\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1084\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1085\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__contains__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"tag 'México' not seen in training corpus/invalid\""
     ]
    }
   ],
   "source": [
    "p2v.docvecs.similarity('Argentina','México')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look the transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primera República Federal (México)\n",
      "Movimiento federalista del noreste de México durante la República Centralista\n",
      "Oficina de la Presidencia de la República (México)\n",
      "Relaciones República Dominicana-México\n",
      "República Centralista (México)\n",
      "República Restaurada (México)\n",
      "Misión Permanente de México en República Checa\n",
      "Plaza de la República (Ciudad de México)\n",
      "Procuraduría General de la República (México)\n",
      "República Centroafricana en los Juegos Olímpicos de México 1968\n"
     ]
    }
   ],
   "source": [
    "for tag in p2v.docvecs.doctags:\n",
    "    if 'México' in tag and 'República' in tag:\n",
    "        print(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4049288198423618\n",
      "0.6489121810340391\n"
     ]
    }
   ],
   "source": [
    "exp1 = 'Gobierno de la República Argentina'\n",
    "exp2 = 'Primera República Federal (México)'\n",
    "exp3 = 'Oficina de la Presidencia de la República (México)'\n",
    "print(p2v.docvecs.similarity(exp1,exp2))\n",
    "print(p2v.docvecs.similarity(exp1,exp3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 36\n",
      "[(0.8696115540769269, 'Misión Permanente de México en República Checa', 'Embajada de Argentina en la República Popular China'), (0.8126812468532063, 'Misión Permanente de México en República Checa', 'Estado Mayor Conjunto de las Fuerzas Armadas de la República Argentina'), (0.7662970010674647, 'República Centroafricana en los Juegos Olímpicos de México 1968', 'Asociación de Reporteros Gráficos de la República Argentina')]\n"
     ]
    }
   ],
   "source": [
    "def n_similar(exp1, exp2, model):\n",
    "    similars = []\n",
    "    list1, list2 =[],[]\n",
    "    for tag in model.docvecs.doctags:\n",
    "        exp1s = set(exp1.split())\n",
    "        exp2s = set(exp2.split())\n",
    "        tags = set(tag.split()) \n",
    "        if len(exp1s.intersection(tags)) == len(exp1s):\n",
    "            list1.append(tag)\n",
    "        if len(exp2s.intersection(tags)) == len(exp2s):\n",
    "            list2.append(tag)\n",
    "    print(len(list1),len(list2))\n",
    "\n",
    "    for s1 in list1:\n",
    "        for s2 in list2:\n",
    "            similars.append((model.docvecs.similarity(s1,s2),s1,s2))\n",
    "                \n",
    "    similars = sorted(similars, reverse=True)\n",
    "    return similars[:3]\n",
    "        \n",
    "print(n_similar('República México','República Argentina',p2v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example shows how we can go further but there is a lot of work to do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='references'></a>\n",
    "# References\n",
    "\n",
    "<a id='John2016'></a>\n",
    "*[John2016]* John, Adebayo Kolawole and Caro, Luigi Di and Boella, Guido. \n",
    "**NORMAS at SemEval-2016 Task 1: SEMSIM: A Multi-Feature Approach to Semantic Text Similarity**. \n",
    "Publisher ACM, 2016."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity in Paragraph2Vec Text Representation\n",
    "\n",
    "*Gensim software examples.*\n",
    "\n",
    "**Prerequisites:** Skills in tokenization with nltk, knowledge of Word2Vec Text Representation model.\n",
    "\n",
    "## Outline\n",
    "\n",
    "**Main Goal:** To practice how to create paragraph2vec models with Gensim and NLTK. Then introduce how to extract information from both text representation, and finally how to measure word similarity.\n",
    "\n",
    "- Gensim Corpus Inizialization\n",
    "- paragraph2vec model example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About Gensim\n",
    "\n",
    "Gensim is a Python library for *topic modelling*, *document indexing*\n",
    "and *similarity retrieval* with large corpora. Target audience is the\n",
    "*natural language processing* (NLP) and *information retrieval* (IR)\n",
    "community. [Gensim Documentation](Gensim Doc)\n",
    "\n",
    "## About NLTK\n",
    "\n",
    "Natural Language ToolKit (NLTK) is a comprehensive Python library for natural language\n",
    "processing and text analytics. Originally designed for teaching, it has been adopted in the\n",
    "industry for research and development due to its usefulness and breadth of coverage. NLTK\n",
    "is often used for rapid prototyping of text processing programs and can even be used in\n",
    "production applications. [(Perkins2014)](#Perkins2014)\n",
    "\n",
    "## What is Paragrah2Vec?\n",
    "\n",
    "Paragrah2Vec is an unsupervised framework that learns continuous distributed vector representations for pieces of texts. The texts can be of variable-length, ranging from sentences to documents. The name Paragraph Vector is to emphasize the fact that the method can be applied to variable-length pieces of texts, anything from a phrase or sentence to a large document.[(Lee and Mikolov, 2014)](#Lee2014)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import smart_open\n",
    "import gensim\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.corpora import TextCorpus, MmCorpus, Dictionary\n",
    "from gensim.corpora.wikicorpus import WikiCorpus\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change this configurations with your paths to Wiki corpus\n",
    "data_path = '/media/DATA/wiki_es/'\n",
    "output_path = '/media/abelma/SSD2/wiki_es/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrangling Data\n",
    "\n",
    "From txt collection to a list of strings, and from string-list to a list of word-list by sentence-list.\n",
    "\n",
    "This first method to load the whole text collection is based on \"os\" module, this is only a code snippet to practice a different ways to do it. NLTK, numpy, and other libraries have it's own methods to do the same process.\n",
    "\n",
    "In this case a new corpus with one document per line is generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1523542334.6557834\n",
      "----------- 100000\n",
      "1523542834.5501635\n",
      "----------- 200000\n",
      "1523543149.6039762\n",
      "----------- 300000\n",
      "1523543429.2024624\n",
      "----------- 400000\n",
      "1523543773.0984907\n",
      "----------- 500000\n",
      "1523544246.1291332\n",
      "----------- 600000\n",
      "1523544657.153675\n",
      "----------- 700000\n",
      "1523544998.0905125\n",
      "----------- 800000\n",
      "1523545415.5849423\n",
      "----------- 900000\n",
      "1523545860.8929534\n",
      "----------- 1000000\n",
      "1523546300.8007684\n",
      "----------- 1100000\n",
      "1523546702.1207345\n"
     ]
    }
   ],
   "source": [
    "init = time.time()\n",
    "print(init)\n",
    "wiki_corpus = WikiCorpus(data_path+'dump/eswiki-20161201-pages-articles-multistream.xml.bz2',\n",
    "                         lemmatize=False,\n",
    "                         article_min_tokens=50,     #Minimum tokens in article.\n",
    "                         article_max_tokens=5000,\n",
    "                         dictionary={})\n",
    "end = time.time()-init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After 69.5min the WikiCorpus object of Spanish Wikipedia dump from December 2016 was uploaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Serialize the corpus and save it to HD for further experiments to test velocity loading pretrained corpus\n",
    "MmCorpus.serialize(output_path+'wiki_corpus', wiki_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaggedWikiDocument(object):\n",
    "    def __init__(self, wiki):\n",
    "        self.wiki = wiki\n",
    "        self.wiki.metadata = True\n",
    "    def __iter__(self):\n",
    "        for content, (page_id, title) in self.wiki.get_texts():\n",
    "            yield TaggedDocument([c for c in content], [title])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tagging docs to generate input-corpus for paragraph2vec model in 0.000130 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abelma/wordembd/lib/python3.5/site-packages/gensim/interfaces.py:60: UserWarning: corpus.save() stores only the (tiny) iteration object; to serialize the actual corpus content, use e.g. MmCorpus.serialize(corpus)\n",
      "  \"corpus.save() stores only the (tiny) iteration object; \"\n"
     ]
    }
   ],
   "source": [
    "init = time.time()\n",
    "documents = TaggedWikiDocument(wiki_corpus)\n",
    "end = time.time()-init\n",
    "print('Tagging docs to generate input-corpus for paragraph2vec model in %f seconds' % end)\n",
    "documents.wiki.save(co 'TaggedWiki.documents')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating the Paragraph2Vec Model\n",
    "\n",
    "_Note:_ This model apply a tokens lowercarse automatically, for that reason sentences are lowercased."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "cores = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/abelma/SSD2/wiki_es/wiki-p2v.model\n",
      "Paragraph2Vec Model vocabulary loaded in 139.031553 seconds\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    init = time.time()\n",
    "    p2v = Doc2Vec.load(output_path+'wiki-p2v.model')\n",
    "    end = time.time()-init\n",
    "    print('Paragraph2Vec Model vocabulary loaded in %f seconds' % end)\n",
    "    \n",
    "except:\n",
    "    init = time.time()\n",
    "    print('no debo entrar aquí')\n",
    "    p2v = Doc2Vec(epochs=1,                #Number of iterations (epochs) over the corpus.\n",
    "                   min_count=20,           #Ignores all words with total frequency lower than this.\n",
    "                   vector_size=300,        #Dimensionality of the feature vectors.\n",
    "                   max_vocab_size=2000000, #Limits the RAM during vocabulary building.\n",
    "                   dm=0,                   #Defines the training algorithm.\n",
    "                   workers=cores,\n",
    "                  )\n",
    "    p2v.build_vocab(documents)\n",
    "    end = time.time()\n",
    "    p2v.save(output_path+'wiki-p2v.model')\n",
    "    print('Paragraph2Vec Model vocabulary scaned in %f seconds' % end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First round: paragraph2vec vocabulary built in 52.45 min.\n",
    "\n",
    "Second round: paragraph2vec vocabulary loaded in 1.76 min.\n",
    "\n",
    "### Generating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1523548085.7003894\n",
      "----------- 100000\n",
      "1523548720.1758692\n",
      "----------- 200000\n",
      "1523549139.410583\n",
      "----------- 300000\n",
      "1523549470.3297775\n",
      "----------- 400000\n",
      "1523549827.492253\n",
      "----------- 500000\n",
      "1523550194.3657181\n",
      "----------- 600000\n",
      "1523550491.8750749\n",
      "----------- 700000\n",
      "1523550731.400874\n",
      "----------- 800000\n",
      "1523551038.9913855\n",
      "----------- 900000\n",
      "1523551356.006913\n",
      "----------- 1000000\n",
      "1523551666.4491296\n",
      "----------- 1100000\n",
      "1523551954.6985068\n",
      "Paragraph2Vec Model Generated in 3877.760819 seconds\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    init = time.time()\n",
    "    p2v = Doc2Vec.load(output_path+'wiki-p2v.model')\n",
    "    end = time.time()-init\n",
    "    print('Paragraph2Vec Model in %f seconds' % end)\n",
    "    \n",
    "except:\n",
    "    init = time.time()\n",
    "    print(init)\n",
    "    p2v.train(documents,\n",
    "              total_examples=p2v.corpus_count,\n",
    "              epochs=1,)\n",
    "    end = time.time()-init\n",
    "    p2v.save(output_path+'wiki-p2v.model')\n",
    "    print('Paragraph2Vec Model Generated in %f seconds' % end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First round: paragraph2vec model trained in 65 min.\n",
    "\n",
    "Second round: paragraph2vec model loaded in xxx min."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00084364,  0.00100843, -0.0013037 ,  0.00095026,  0.00102746,\n",
       "       -0.00099448,  0.00164336,  0.00095534,  0.00066178, -0.00122448],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p2v.wv['niño'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('República del Congo', 0.9746325016021729), ('Nunavut', 0.963964581489563), ('Chechenia', 0.9632564783096313), ('Burundi', 0.9623643755912781), ('Libertador San Martín', 0.9616067409515381), ('Cuetzala del Progreso', 0.9613605737686157), ('Islas Baleares', 0.9603628516197205), ('Puerto Maldonado', 0.9602200388908386), ('Caquetá', 0.9595517516136169), ('Isla Grande de Chiloé', 0.9593847393989563)]\n",
      "[('estadística', 0.26488637924194336), ('magari', 0.25142616033554077), ('swordfish', 0.24946871399879456), ('kiotenses', 0.24445898830890656), ('afroecuatoriano', 0.2428331971168518), ('peraleja', 0.23987072706222534), ('revin', 0.23189276456832886), ('illapa', 0.22987493872642517), ('horatio', 0.22923138737678528), ('aculturación', 0.22797220945358276)]\n"
     ]
    }
   ],
   "source": [
    "print(p2v.docvecs.most_similar(positive='Argentina'))\n",
    "print(p2v.wv.most_similar(positive='argentina'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in the above example the docvecs is useful for expressions that appier in `p2v.docvecs.doctags`, otherwise the model will rise an error. If you use word vector similarity (`wv.most_similar`) the results are worst than `docvecs.most_similar`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8469448991224805"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p2v.docvecs.n_similarity(['Argentina'],['Congo'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sklearn Paragraph2Vec-Cosine Sentence Similarity\n",
    "\n",
    "### Wrangling Data\n",
    "\n",
    "From string-sentences to numpy paragraph vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = 'la niña corrió hacia el hueco'\n",
    "sentence2 = 'Alicia corrió hacia el hueco'\n",
    "\n",
    "sent1 = sentence1.lower().split()\n",
    "sent2 = sentence2.lower().split()\n",
    "\n",
    "#Filtering stopwords by hand\n",
    "sent1s = 'niña corrió hueco'\n",
    "sent2s = 'Alicia corrió hueco'\n",
    "\n",
    "sent1sl = sent1s.lower().split()\n",
    "sent2sl = sent2s.lower().split()\n",
    "\n",
    "#If we change the sent1 by a very different meaning sent3\n",
    "sent3 = ['the','boy','eat','a','red','apple']\n",
    "sent3s = ['boy','eat','red','apple']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Infered Vector of a Sentences\n",
    "\n",
    "Paragraph to Vector in Gensim library has this method, which is not present in other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.04579691, -0.09673504, -0.03551013, -0.05200712, -0.01592638,\n",
       "        0.02746712, -0.08295926, -0.03992163,  0.11818437, -0.16485237],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_sent1_infer_p2v = p2v.infer_vector(sent1)\n",
    "vec_sent2_infer_p2v = p2v.infer_vector(sent2)\n",
    "\n",
    "#Stopword filtering\n",
    "vec_sent1s_infer_p2v = p2v.infer_vector(sent1sl)\n",
    "vec_sent2s_infer_p2v = p2v.infer_vector(sent2sl)\n",
    "\n",
    "#print the Paragraph2vector of the sentence 1\n",
    "print(len(vec_sent1_infer_p2v))\n",
    "vec_sent1_infer_p2v[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p2v infered vector 0.74639404\n",
      "p2v infered vector without stopwords 0.56761754\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "print('p2v infered vector',\n",
    "      cosine_similarity(vec_sent1_infer_p2v.reshape(1,-1),vec_sent2_infer_p2v.reshape(1,-1))[0][0])\n",
    "print('p2v infered vector without stopwords',\n",
    "      cosine_similarity(vec_sent1s_infer_p2v.reshape(1,-1),vec_sent2s_infer_p2v.reshape(1,-1))[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrangling Data\n",
    "\n",
    "From string sentences to word for word paragraph2vec model numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def preproc_data(sent, model):\n",
    "    \n",
    "    vec_sent = []\n",
    "\n",
    "    for i,word in enumerate(sent):\n",
    "        try:\n",
    "            vec_sent.append(model.wv[word])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    vec_sent = sum(np.asarray(vec_sent))\n",
    "    result = vec_sent.reshape(1,-1)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.00198484, -0.00161264, -0.00392045, -0.00552214,  0.00131036,\n",
       "        0.0005118 ,  0.00136406,  0.0017021 , -0.00151024, -0.00087941],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p2v_sent1 = preproc_data(sent1,p2v)\n",
    "p2v_sent2 = preproc_data(sent2,p2v)\n",
    "print(len(p2v_sent1[0]))\n",
    "p2v_sent2[0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7499604"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cosine_similarity(p2v_sent1,p2v_sent2)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.69232756"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Filtering stopwords\n",
    "p2v_sent1s = preproc_data(sent1sl,p2v)\n",
    "p2v_sent2s = preproc_data(sent2sl,p2v)\n",
    "cosine_similarity(p2v_sent1s,p2v_sent2s)[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scipy Cosine Similarity\n",
    "\n",
    "$Note: cosine_{Scipy\\ distance} = 1 - cosine_{Sklearn\\ similarity}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2500395178794861\n",
      "0.30767250061035156\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import cosine as cosine_scipy\n",
    "\n",
    "print(cosine_scipy(p2v_sent1,p2v_sent2))\n",
    "print(cosine_scipy(p2v_sent1s,p2v_sent2s)) #Filtering stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gensim Particular Measures\n",
    "\n",
    "Gensim jaccard and cosine are impossible to measure because the p2v_bow vector is needed, but not exist.\n",
    "\n",
    "## Cosine using Gensim p2v of a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p2v sentence vector similarity without transformation 0.0068915365\n",
      "p2v sentence vector similarity without transformation 0.7499604\n"
     ]
    }
   ],
   "source": [
    "vec_sent1 = p2v.wv[sent1]\n",
    "vec_sent2 = p2v.wv[sent2]\n",
    "\n",
    "vec_sent1_ = sum(vec_sent1).reshape(1,-1)\n",
    "vec_sent2_ = sum(vec_sent2).reshape(1,-1)\n",
    "\n",
    "print('p2v sentence vector similarity without transformation',\n",
    "      cosine_similarity(vec_sent1,vec_sent2)[0][0])\n",
    "print('p2v sentence vector similarity without transformation',\n",
    "      cosine_similarity(vec_sent1_,vec_sent2_)[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gensim p2v.n_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.040381502259109046\n",
      "0.749960442719121\n",
      "0.6923275354116258\n"
     ]
    }
   ],
   "source": [
    "print(p2v.wv.n_similarity(sent3s,sent1))\n",
    "print(p2v.wv.n_similarity(sent1,sent2))\n",
    "print(p2v.wv.n_similarity(sent1sl,sent2sl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gensim p2v infer_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.74639404"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Testing initial infer_vector similarity\n",
    "vec_sent1_infer_p2v = vec_sent1_infer_p2v.reshape(1,-1)\n",
    "vec_sent2_infer_p2v = vec_sent2_infer_p2v.reshape(1,-1)\n",
    "cosine_similarity(vec_sent1_infer_p2v,vec_sent2_infer_p2v)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.0"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#infer_vector similarity filtering stopwords\n",
    "sent1s_infer_p2v = vec_sent1s_infer_p2v.reshape(-1,1)\n",
    "sent2s_infer_p2v = vec_sent2s_infer_p2v.reshape(-1,1)\n",
    "cosine_similarity(sent1s_infer_p2v,sent2s_infer_p2v)[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gensim p2v.similarity\n",
    "\n",
    "A score constructed with this method based on an international article.[John2016](#John2016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.07725515726057419"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p2v.wv.similarity('hombre','mujer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_sim_jonh2016(sent1, sent2, model):\n",
    "    \"\"\":type sent1,sent2: list of strings\"\"\"\n",
    "    \n",
    "    sim_vector = []\n",
    "    ALPHA = 0.019\n",
    "\n",
    "    for wordA in sent1:\n",
    "        for wordB in sent2:\n",
    "            try:\n",
    "                sim = p2v.wv.similarity(wordA,wordB)\n",
    "                if sim > ALPHA:\n",
    "                    sim_vector.append(sim)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    return sum(sim_vector)/(len(sim_vector) or 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar sentences w2v.similarity 0.2740338124244419\n",
      "Similar sentences w2v.similarity without stopwords 0.32096874638428086\n"
     ]
    }
   ],
   "source": [
    "print('Similar sentences w2v.similarity', sent_sim_jonh2016(sent1,sent2, p2v))\n",
    "print('Similar sentences w2v.similarity without stopwords', sent_sim_jonh2016(sent1sl,sent2sl, p2v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gensim Hellinger sentence similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abelma/wordembd/lib/python3.5/site-packages/gensim/matutils.py:906: RuntimeWarning: invalid value encountered in sqrt\n",
      "  sim = np.sqrt(0.5 * ((np.sqrt(vec1) - np.sqrt(vec2))**2).sum())\n"
     ]
    }
   ],
   "source": [
    "from gensim.matutils import kullback_leibler, hellinger\n",
    "print(hellinger(p2v_sent1,p2v_sent2))\n",
    "print(kullback_leibler(p2v_sent1,p2v_sent2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Pair Word Overlap Similarity\n",
    "\n",
    "Lets try a different way to compound a sentence similarity, based on WordNet-Augmented-Word-Overlap similarity idea.\n",
    "\n",
    "$p = {\\sum_{w\\in\\ sent_1}max(df[w][w']) \\over len(sent_1)} \\ \\ \\ \\forall\\ w' \\in\\ sent_2$\n",
    "\n",
    "$q = {\\sum_{w'\\in\\ sent_2}max(df[w][w']) \\over len(sent_2)} \\ \\ \\ \\forall\\ w \\in\\ sent_1$\n",
    "\n",
    "$sim = \\left\\{ \\begin{array}{rcl} \n",
    "0  & if\\ p+q = 0\\\\\n",
    "{2 p*q \\over (p+q)}  & others\\\\\n",
    "\\end{array}\n",
    "\\right.$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def harmonic_best_pair_word_sim(sent1,sent2,model):\n",
    "    p=0\n",
    "    for wordA in sent1:\n",
    "        m = 0\n",
    "        for wordB in sent2:\n",
    "            try:\n",
    "                m = max(m, model.wv.similarity(wordA,wordB))\n",
    "            except:\n",
    "                pass\n",
    "        p += m\n",
    "    p = p/len(sent1)\n",
    "\n",
    "    q=0\n",
    "    for wordA in sent2:\n",
    "        m = 0\n",
    "        for wordB in sent1:\n",
    "            try:\n",
    "                m = max(m, model.wv.similarity(wordA,wordB))\n",
    "            except:\n",
    "                pass\n",
    "        q += m\n",
    "    q = q/len(sent2)\n",
    "\n",
    "    sim = 2*p*q/(p+q or 1)\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dissimilar sentences w2v_harmonic_best_pair_word similarity 0.039977160321282866\n",
      "Dissimilar sentences without stopwords w2v_harmonic_best_pair_word similarity 0.0\n",
      "Similar sentences w2v_harmonic_best_pair_word 0.7476427998305525\n",
      "Similar sentences w2v_harmonic_best_pair_word without stopwords 0.6856355639043101\n"
     ]
    }
   ],
   "source": [
    "print('Dissimilar sentences w2v_harmonic_best_pair_word similarity', \n",
    "      harmonic_best_pair_word_sim(sent3, sent2, p2v))\n",
    "print('Dissimilar sentences without stopwords w2v_harmonic_best_pair_word similarity',\n",
    "      harmonic_best_pair_word_sim(sent3s, sent2s, p2v))\n",
    "print('Similar sentences w2v_harmonic_best_pair_word', \n",
    "      harmonic_best_pair_word_sim(sent1, sent2, p2v))\n",
    "print('Similar sentences w2v_harmonic_best_pair_word without stopwords',\n",
    "      harmonic_best_pair_word_sim(sent1sl, sent2sl, p2v))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "Same as Word2Vec this model doesn't works with bow structure, it represent a word as a vector of *size parameter* value length. At the same time this model can infered a vector for a sentence. The experiments shows that with the same corpus and the same sentences the paragraph2vec needs to lowercase all tokens, e.g. 'Alice' or 'Here', this behavior is different to Word2Vec model.\n",
    "\n",
    "- Here the vectors that represent sentences works better than single word vectors. See for example the difference between `p2v.wv.n_similarity(sent1,sent2)` & `p2v.wv.similarity('hombre','mujer')`\n",
    "- The best similarities using this text representation models must be implemented with innovatives ideas. For example: ``sent_sim_jonh2016`` and ``harmonic_best_pair_word_sim``.\n",
    "- In almost all cases the stopword filtering __decrement__ the similarity between similar sentences and diminished similarity between different sentences.\n",
    "- Note that `p2v.docvec.n_similarity` rise an error when have to process expressions with stopwords like \"de, el, yo,...\". Then to use this metric the stopword filtering must be applied first. Is interesting that `p2v.wv.n_similarity` dosn't have this problem.\n",
    "- The results with _infered vectors_ are return less similarity than other sentences word vector based methods like `harmonic_best_pair_word_sim`.\n",
    "- Gensim Hellinger and Kullback Leibler still been useless.\n",
    "\n",
    "\n",
    "# Recomendations\n",
    "\n",
    "* When comparations between expresions (more than one word text construction) is made, you can test what kind of expressions are very similar in the paragraph2vec model and you can play substituting it and then comparing with the other transformed query. (_see Playfull Example_)\n",
    "* Train the paragraph2vec model using the wikipedia articles bigger than 5000 words, in this notebook the parameter `article_max_tokens` was implemented by myself inside Gensim pack for lack of RAM reasons. Then try to train the model with all the articles.\n",
    "* Test the difference between loading wiki_corpus from .xml.bz2 or from .mm serialized corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playfull Example\n",
    "\n",
    "Try this little example to see the difference doing substitution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"tag 'México' not seen in training corpus/invalid\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-105-3e5e054265b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mp2v\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocvecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimilarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Argentina'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'México'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/wordembd/lib/python3.5/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36msimilarity\u001b[0;34m(self, d1, d2)\u001b[0m\n\u001b[1;32m   1268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m         \"\"\"\n\u001b[0;32m-> 1270\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munitvec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munitvec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1272\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mn_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mds1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mds2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/wordembd/lib/python3.5/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m   1081\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors_docs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_int_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoctags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_rawint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1082\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1083\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tag '%s' not seen in training corpus/invalid\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1084\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1085\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__contains__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"tag 'México' not seen in training corpus/invalid\""
     ]
    }
   ],
   "source": [
    "p2v.docvecs.similarity('Argentina','México')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look the transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primera República Federal (México)\n",
      "Movimiento federalista del noreste de México durante la República Centralista\n",
      "Oficina de la Presidencia de la República (México)\n",
      "Relaciones República Dominicana-México\n",
      "República Centralista (México)\n",
      "República Restaurada (México)\n",
      "Misión Permanente de México en República Checa\n",
      "Plaza de la República (Ciudad de México)\n",
      "Procuraduría General de la República (México)\n",
      "República Centroafricana en los Juegos Olímpicos de México 1968\n"
     ]
    }
   ],
   "source": [
    "for tag in p2v.docvecs.doctags:\n",
    "    if 'México' in tag and 'República' in tag:\n",
    "        print(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4049288198423618\n",
      "0.6489121810340391\n"
     ]
    }
   ],
   "source": [
    "exp1 = 'Gobierno de la República Argentina'\n",
    "exp2 = 'Primera República Federal (México)'\n",
    "exp3 = 'Oficina de la Presidencia de la República (México)'\n",
    "print(p2v.docvecs.similarity(exp1,exp2))\n",
    "print(p2v.docvecs.similarity(exp1,exp3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 36\n",
      "[(0.8696115540769269, 'Misión Permanente de México en República Checa', 'Embajada de Argentina en la República Popular China'), (0.8126812468532063, 'Misión Permanente de México en República Checa', 'Estado Mayor Conjunto de las Fuerzas Armadas de la República Argentina'), (0.7662970010674647, 'República Centroafricana en los Juegos Olímpicos de México 1968', 'Asociación de Reporteros Gráficos de la República Argentina')]\n"
     ]
    }
   ],
   "source": [
    "def n_similar(exp1, exp2, model):\n",
    "    similars = []\n",
    "    list1, list2 =[],[]\n",
    "    for tag in model.docvecs.doctags:\n",
    "        exp1s = set(exp1.split())\n",
    "        exp2s = set(exp2.split())\n",
    "        tags = set(tag.split()) \n",
    "        if len(exp1s.intersection(tags)) == len(exp1s):\n",
    "            list1.append(tag)\n",
    "        if len(exp2s.intersection(tags)) == len(exp2s):\n",
    "            list2.append(tag)\n",
    "    print(len(list1),len(list2))\n",
    "\n",
    "    for s1 in list1:\n",
    "        for s2 in list2:\n",
    "            similars.append((model.docvecs.similarity(s1,s2),s1,s2))\n",
    "                \n",
    "    similars = sorted(similars, reverse=True)\n",
    "    return similars[:3]\n",
    "        \n",
    "print(n_similar('República México','República Argentina',p2v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example shows how we can go further but there is a lot of work to do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='references'></a>\n",
    "# References\n",
    "\n",
    "<a id='Perkins2014'></a>\n",
    "[1] *[Perkins2014]* Jacov Perkins. \n",
    "Book **Python 3 Text Processing with NLTK 3 Cookbook**. 2014. \n",
    "p. 7 **ISBN**: 978-1-78216-785-3\n",
    "\n",
    "<a id='Lee2014'></a>\n",
    "[2] *[Lee and Mikolov]* Quoc Le and Tomas Mikolov.\n",
    "**Distributed Representations of Sentences and Documents**. 2014. \n",
    "Proceedings of the 31 st International Conference on Machine Learning, Beijing, China.\n",
    "\n",
    "<a id='John2016'></a>\n",
    "[3] *[John2016]* John, Adebayo Kolawole and Caro, Luigi Di and Boella, Guido. \n",
    "**NORMAS at SemEval-2016 Task 1: SEMSIM: A Multi-Feature Approach to Semantic Text Similarity**. \n",
    "Publisher ACM, 2016."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity in Term-Frecuency Text Representation using SVD\n",
    "\n",
    "**Prerequisites:** Skills in Sklearn, and knowledge of TfIdf Text Representation model.\n",
    "\n",
    "## Outline\n",
    "\n",
    "**Main Goal:**  How to applied Using Singular Value Decomposition as Dimensionality Reduction Technique on TfIdf models with Sklearn. Then introduce how to extract information from this text representation, and finally how to measure word similarity.\n",
    "\n",
    "- TfIdf Sklearn model generation.\n",
    "- Wrangling data from strings to numerical vectors.\n",
    "- Applied some Sklearn, Scipy and Gensim similarity measures.\n",
    "- Applied Okapi BM25 weighting model to compare with previos similarities scores.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.datasets import load_files\n",
    "from scipy.sparse import  coo_matrix\n",
    "import scipy\n",
    "import time\n",
    "import nltk\n",
    "import ssl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data\n",
    "\n",
    "You could have some SSL problems, so first you must solve them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     /Users/sorice/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download('gutenberg')\n",
    "# check the download path for next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change the path to the library if you load it from local path\n",
    "corpus = load_files(f'/Users/sorice/nltk_data/corpora/',categories=['gutenberg'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Left Singular Values Unitary Matrix\n",
    "\n",
    "Algo called in this notebook as Ug, will be used as matrix for calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(63341, 21)\n",
      "Total time: 1.9103684425354004\n"
     ]
    }
   ],
   "source": [
    "init = time.time()\n",
    "gvectorizer = CountVectorizer(min_df=1, lowercase=False)\n",
    "GTfMatrix = gvectorizer.fit_transform(corpus.data)\n",
    "coGTfMatrix = coo_matrix(GTfMatrix.T)\n",
    "coGTfMatrix.data = coGTfMatrix.data*1.0 #a necessary step because without it doesn't work\n",
    "#k must be between 1 and min(coGTfMatrix.shape)\n",
    "print(coGTfMatrix.shape)\n",
    "Ug, Sg, Vg = scipy.sparse.linalg.svds(coGTfMatrix, k=20)\n",
    "end = time.time()-init\n",
    "print('Total time:', end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21, 63341)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Remember that in a huge corpus GTfMatrix will have thousands/millions of documents\n",
    "#Then docs_num(=21) could be >> 63341\n",
    "GTfMatrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(63341, 20)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ug.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-4.40951269e-03,  1.33033600e-02, -4.64223536e-02,  2.42566730e-01,\n",
       "        9.93370503e-03,  3.05290393e-01,  2.85561843e-01, -4.90058307e-02,\n",
       "       -2.59280455e-02,  1.05681402e-01, -6.18374582e-02, -1.12536478e-02,\n",
       "       -7.82374847e-03, -1.27033488e-02,  1.24293473e-02, -5.06080200e-03,\n",
       "        1.83999617e-03,  2.93662481e-04,  2.10870274e-03,  8.89107309e-05])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ug[gvectorizer.get_feature_names().index('Alice')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sklearn Word2Vec-Cosine sentence similarity\n",
    "\n",
    "### Wrangling Data\n",
    "\n",
    "From string-sentences to numerical vector representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = 'the girl run into the hall'\n",
    "sentence2 = 'Here Alice run to the hall'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def preproc_data(sentence1, sentence2, Ug):\n",
    "\n",
    "    sent1 = sentence1.split()\n",
    "    sent2 = sentence2.split()\n",
    "\n",
    "    svd_sent1 = []\n",
    "    svd_sent2 = []\n",
    "\n",
    "    for word in sent1:\n",
    "        try:\n",
    "            svd_sent1.append(Ug[gvectorizer.get_feature_names().index(word)])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    for word in sent2:\n",
    "        try:\n",
    "            svd_sent2.append(Ug[gvectorizer.get_feature_names().index(word)])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "\n",
    "    svd_sent1 = sum(np.asarray(svd_sent1))\n",
    "    svd_sent2 = sum(np.asarray(svd_sent2))\n",
    "    A = svd_sent1.reshape(1,-1)\n",
    "    B = svd_sent2.reshape(1,-1)\n",
    "    \n",
    "    return A, B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 9.27926220e-02, -7.19857997e-02, -1.23356876e-02,\n",
       "        -2.01910199e-01,  3.93403618e-02,  1.46199465e-01,\n",
       "         4.58075277e-04, -2.54710224e-02, -7.45007534e-03,\n",
       "         2.05383102e-01, -4.21420949e-01,  5.08102298e-02,\n",
       "        -1.93733209e-01, -4.41305574e-01,  1.14012140e-01,\n",
       "        -1.65396687e-01,  9.80674791e-01, -1.59028786e-02,\n",
       "        -2.73517252e-01,  1.35984384e+00]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd_sent1, svd_sent2 = preproc_data(sentence1,sentence2,Ug)\n",
    "print(len(svd_sent1[0]))\n",
    "svd_sent1[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7380034007075583"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cosine_similarity(svd_sent1,svd_sent2)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.22301475770675458"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Filtering stopwords\n",
    "sent1s = 'girl run hall'\n",
    "sent2s = 'Alice run hall'\n",
    "svd_sent1s, svd_sent2s = preproc_data(sent1s,sent2s,Ug)\n",
    "cosine_similarity(svd_sent1s,svd_sent2s)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.26199659929244157\n",
      "1.2230147577067545\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import cosine as cosine_scipy\n",
    "\n",
    "print(cosine_scipy(svd_sent1,svd_sent2))\n",
    "print(cosine_scipy(svd_sent1s,svd_sent2s)) #Filtering stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Pair Word Overlap Similarity\n",
    "\n",
    "Lets try a different way to compound a sentence similarity, based on WordNet-Augmented-Word-Overlap similarity idea.\n",
    "\n",
    "$p = {\\sum_{w\\in\\ sent_1}max(df[w][w']) \\over len(sent_1)} \\ \\ \\ \\forall\\ w' \\in\\ sent_2$\n",
    "\n",
    "$q = {\\sum_{w'\\in\\ sent_2}max(df[w][w']) \\over len(sent_2)} \\ \\ \\ \\forall\\ w \\in\\ sent_1$\n",
    "\n",
    "$sim = \\left\\{ \\begin{array}{rcl} \n",
    "0  & if\\ p+q = 0\\\\\n",
    "{2 p*q \\over (p+q)}  & others\\\\\n",
    "\\end{array}\n",
    "\\right.$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.582328641082437\n"
     ]
    }
   ],
   "source": [
    "def svd_sim(word1,word2, Ug, gvectorizer):\n",
    "    try:\n",
    "        a = Ug[gvectorizer.get_feature_names().index(word1)]\n",
    "        b = Ug[gvectorizer.get_feature_names().index(word2)]\n",
    "        a = a.reshape(1,-1)\n",
    "        b = b.reshape(1,-1)\n",
    "        return cosine_similarity(a,b)[0][0]\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "print(svd_sim('wife','husband', Ug, gvectorizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3712829259505216"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd_sim('car','vehicle', Ug, gvectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def harmonic_best_pair_word_sim(sentence1,sentence2, Ug, gvectorizer):\n",
    "    \n",
    "    sent1 = sentence1.split()\n",
    "    sent2 = sentence2.split()\n",
    "    \n",
    "    p=0\n",
    "    for wi in sent1:\n",
    "        m = 0\n",
    "        for wc in sent2:\n",
    "            try:\n",
    "                m = max(m, svd_sim(wi,wc, Ug, gvectorizer))\n",
    "            except:\n",
    "                pass\n",
    "        p += m\n",
    "    p = p/len(sent1)\n",
    "\n",
    "    q=0\n",
    "    for wc in sent2:\n",
    "        m = 0\n",
    "        for wi in sent1:\n",
    "            try:\n",
    "                m = max(m, svd_sim(wi,wc, Ug, gvectorizer))\n",
    "            except:\n",
    "                pass\n",
    "        q += m\n",
    "    q = q/len(sent2)\n",
    "\n",
    "    sim = 2*p*q/(p+q or 1)\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Harmonic mean best pair-word similarity, stopword_filtering=no 0.7383742690439515\n",
      "Harmonic mean best pair-word similarity, stopword_filtering=no 1.0\n",
      "Harmonic mean best pair-word similarity, stopword_filtering=yes 0.7924553499961652\n"
     ]
    }
   ],
   "source": [
    "print('Harmonic mean best pair-word similarity, stopword_filtering=no',\n",
    "      harmonic_best_pair_word_sim(sentence1,sentence2, Ug, gvectorizer))\n",
    "print('Same sentences harmonic best word similarity, stopword_filtering=no',\n",
    "      harmonic_best_pair_word_sim(sentence1,sentence1, Ug, gvectorizer))\n",
    "print('Harmonic mean best pair-word similarity, stopword_filtering=yes',\n",
    "      harmonic_best_pair_word_sim(sent1s,sent2s, Ug, gvectorizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wighting TfIdf model with Okapi BM25\n",
    "\n",
    "This method join with matrix factorization have been showing good results in Information Retrieval, the next cells are a basic experiment to make an initial presentation of future works.\n",
    "\n",
    "## Applying BM25\n",
    "\n",
    "The next implementation appears on [Ben Frederickson article](http://www.benfrederickson.com/matrix-factorization/), the complete code of his implementation can be found on [benfrederickson.github](https://github.com/benfred/implicit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from numpy import log, bincount\n",
    "\n",
    "def bm25_weight(X, K1=100, B=0.8):\n",
    "    \"\"\" Weighs each row of a sparse matrix X  by BM25 weighting \"\"\"\n",
    "    # calculate idf per term (user)\n",
    "    X = coo_matrix(X)\n",
    "\n",
    "    N = float(X.shape[0])\n",
    "    idf = log(N / (1 + bincount(X.col)))\n",
    "\n",
    "    # calculate length_norm per document (artist)\n",
    "    row_sums = numpy.ravel(X.sum(axis=1))\n",
    "    average_length = row_sums.mean()\n",
    "    length_norm = (1.0 - B) + B * row_sums / average_length\n",
    "\n",
    "    # weight matrix rows by bm25\n",
    "    X.data = X.data * (K1 + 1.0) / (K1 * length_norm[X.row] + X.data) * idf[X.col]\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "WGTfMatrix = bm25_weight(coGTfMatrix)\n",
    "Ugt, Sgt, Vgt = scipy.sparse.linalg.svds(WGTfMatrix, k=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def svd_sentence_similarity(sentence1,sentence2,Ugt,gvectorizer):\n",
    "    \n",
    "    sent1 = sentence1.split()\n",
    "    sent2 = sentence2.split()\n",
    "\n",
    "    svd_sent1 = []\n",
    "    svd_sent2 = []\n",
    "\n",
    "    for word in sent1:\n",
    "        try:\n",
    "            svd_sent1.append(Ugt[gvectorizer.get_feature_names().index(word)])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    for word in sent2:\n",
    "        try:\n",
    "            svd_sent2.append(Ugt[gvectorizer.get_feature_names().index(word)])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    svd_sent1 = sum(np.asarray(svd_sent1))\n",
    "    svd_sent2 = sum(np.asarray(svd_sent2))\n",
    "    \n",
    "    A = svd_sent1.reshape(1,-1)\n",
    "    B = svd_sent2.reshape(1,-1)\n",
    "    \n",
    "    return cosine_similarity(A,B)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7380034007075583\n",
      "-0.22301475770675458\n",
      "0.5366959906888554\n",
      "0.43171833850443275\n"
     ]
    }
   ],
   "source": [
    "print(svd_sentence_similarity(sentence1,sentence2,Ug,gvectorizer))\n",
    "print(svd_sentence_similarity(sent1s,sent2s,Ug,gvectorizer))\n",
    "print(svd_sentence_similarity(sentence1,sentence2,Ugt,gvectorizer))\n",
    "print(svd_sentence_similarity(sent1s,sent2s,Ugt,gvectorizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7655415866709938\n",
      "-0.008266910259872418\n"
     ]
    }
   ],
   "source": [
    "sentence3 = 'boy eat red apple'\n",
    "print(svd_sentence_similarity(sentence1,sentence3,Ugt,gvectorizer))\n",
    "print(svd_sentence_similarity(sentence1,sentence3,Ug,gvectorizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 20)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd_sent2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abelm/flask/lib/python3.5/site-packages/gensim/matutils.py:538: RuntimeWarning: invalid value encountered in sqrt\n",
      "  sim = np.sqrt(0.5 * ((np.sqrt(vec1) - np.sqrt(vec2))**2).sum())\n"
     ]
    }
   ],
   "source": [
    "from gensim.matutils import kullback_leibler, hellinger\n",
    "\n",
    "print(hellinger(svd_sent1,svd_sent2))\n",
    "print(kullback_leibler(svd_sent1, svd_sent2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "* As you can test the SVD+Tf doesn't have a fast or parallel solution, but is a fast method.\n",
    "* After some mathematical operations (SVD, Transpose, etc) the word-features matrix is obtained. \n",
    "* There are no variations between scores of different libraries or methods.\n",
    "* The application of BM25 do not improve the results in this set of sentences.\n",
    "\n",
    "# Recomendations\n",
    "\n",
    "* Made the same example with Wikipedia dump data, to test the similarity difference according to data.\n",
    "\n",
    "# Future Works\n",
    "\n",
    "* svd_sentence_similarity get confuse results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

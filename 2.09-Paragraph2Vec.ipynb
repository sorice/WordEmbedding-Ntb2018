{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paragraph2Vec Text Representation Model\n",
    "\n",
    "*Gensim software examples.*\n",
    "\n",
    "**Prerequisites:** Skills in tokenization with nltk, knowledge of Word2Vec Text Representation model.\n",
    "\n",
    "**data:** Gutenberg Corpus\n",
    "\n",
    "## Outline\n",
    "\n",
    "**Main Goal:** To practice how to create paragraph2vec model with Gensim and NLTK. Then introduce how to extract information from this text representation model, and finally how to measure word similarity using the previous result.\n",
    "\n",
    "- Acquiring and wrangling data for model initialization. \n",
    "- Gensim paragraph2vec model generation/loading example\n",
    "- Sklearn, Scipy text similarity measures examples\n",
    "- Gensim original measures examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Paragrah2Vec?\n",
    "\n",
    "Paragrah2Vec is an unsupervised framework that learns continuous distributed vector representations for pieces of texts. The texts can be of variable-length, ranging from sentences to documents. The name Paragraph Vector is to emphasize the fact that the method can be applied to variable-length pieces of texts, anything from a phrase or sentence to a large document.[(Lee and Mikolov, 2014)](#Lee2014)\n",
    "\n",
    "**Note**: About Gensim and NLTK software please read the introduction notes about them in [2.1-TfIdf Notebook](02.1-TfIdf.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import smart_open\n",
    "import gensim\n",
    "from gensim.models import Doc2Vec\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Acquiring & Wrangling Data\n",
    "\n",
    "From txt collection to a list of strings, and from string-list to a list of word-list by sentence-list.\n",
    "\n",
    "This first method to load the whole text collection is based on \"os\" module, this is only a code snippet to practice a different ways to do it. NLTK, numpy, and other libraries have it's own methods to do the same process.\n",
    "\n",
    "In this case a new corpus with one document per line is generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_collection = ''\n",
    "file_path = 'data/gutenberg/'\n",
    "file_list = list(os.popen('ls '+ file_path).read().split('\\n'))\n",
    "for file in file_list:\n",
    "    if file:\n",
    "        with open(os.path.join(file_path,file)) as doc:\n",
    "            doc_collection += doc.read()+'\\n'\n",
    "\n",
    "#Wrangling the data from list of doc-strings -> list of word-list by sentences\n",
    "with open('data/all_gutenberg', 'w') as f:\n",
    "    f.write(doc_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus(fname, tokens_only=False):\n",
    "    with smart_open.smart_open(fname, encoding=\"iso-8859-1\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if tokens_only:\n",
    "                yield gensim.utils.simple_preprocess(line)\n",
    "            else:\n",
    "                # For training data, add tags\n",
    "                yield gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(line), [i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Generating the Paragraph2Vec Model\n",
    "\n",
    "_Note:_ This model apply a tokens lowercarse automatically, for that reason sentences are lowercased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc2Vec Model Generated in 134 seconds\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    p2v= Doc2Vec.load('models/gutenberg-p2v.model')\n",
    "    print('Doc2Vec Model Generated in 134 seconds')\n",
    "except:\n",
    "    init = time.time()\n",
    "    corpus = list(read_corpus('data/all_gutenberg'))\n",
    "    paraph2vec = Doc2Vec(corpus, vector_size=300, window=8, min_count=5, workers=4)\n",
    "    end = time.time()-init\n",
    "    paraph2vec.save('models/gutenberg_p2v.model')\n",
    "    print('Total time:', end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.6150048 , -0.13362435, -0.17901617, -0.13512719, -0.18723272,\n",
       "       -0.7113043 ,  0.3514476 , -0.50977117, -0.5001958 , -0.39531764],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p2v.wv['alice'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Measuring Similarity between Pair of Sentences\n",
    "\n",
    "This section is made to show the utility of _paragraph2vec_ model in an applied example. Also to show some native similarity methods of `gensim.Doc2Vec` class.\n",
    "\n",
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = 'the girl run into the hall'\n",
    "sentence2 = 'Here Alice run to the hall'\n",
    "\n",
    "sent1 = sentence1.lower().split()\n",
    "sent2 = sentence2.lower().split()\n",
    "\n",
    "sent1s = 'girl run hall'\n",
    "sent2s = 'Alice run hall'\n",
    "\n",
    "sent1sl = sent1s.lower().split()\n",
    "sent2sl = sent2s.lower().split()\n",
    "\n",
    "#If we change the sent1 by a very different meaning sent3\n",
    "sent3 = ['the','boy','eat','a','red','apple']\n",
    "sent3s = ['boy','eat','red','apple']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Wrangling Data\n",
    "\n",
    "From string sentences to word for word paragraph2vec model numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def preproc_data(sent, model):\n",
    "    \n",
    "    vec_sent = []\n",
    "\n",
    "    for i,word in enumerate(sent):\n",
    "        try:\n",
    "            vec_sent.append(model.wv[word])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    vec_sent = sum(np.asarray(vec_sent))\n",
    "    result = vec_sent.reshape(1,-1)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.47775584, -2.3387423 , -1.6507049 ,  0.6713754 ,  1.2924995 ,\n",
       "       -3.5926514 ,  0.50806916,  0.16648477, -0.53688073, -0.36386508],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p2v_sent1 = preproc_data(sent1,p2v)\n",
    "p2v_sent2 = preproc_data(sent2,p2v)\n",
    "print(len(p2v_sent1[0]))\n",
    "p2v_sent2[0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Sklearn Paragraph2Vec-Cosine Sentence Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7995827"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cosine_similarity(p2v_sent1,p2v_sent2)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8869797"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Filtering stopwords\n",
    "p2v_sent1s = preproc_data(sent1sl,p2v)\n",
    "p2v_sent2s = preproc_data(sent2sl,p2v)\n",
    "cosine_similarity(p2v_sent1s,p2v_sent2s)[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Scipy Cosine Similarity\n",
    "\n",
    "$Note: cosine_{Scipy\\ distance} = 1 - cosine_{Sklearn\\ similarity}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.20041728019714355\n",
      "0.11302042007446289\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import cosine as cosine_scipy\n",
    "\n",
    "print(cosine_scipy(p2v_sent1,p2v_sent2))\n",
    "print(cosine_scipy(p2v_sent1s,p2v_sent2s)) #Filtering stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Best Pair Word Overlap Similarity\n",
    "\n",
    "Lets try a different way to compound a sentence similarity, based on WordNet-Augmented-Word-Overlap similarity idea.\n",
    "\n",
    "$p = {\\sum_{w\\in\\ sent_1}max(df[w][w']) \\over len(sent_1)} \\ \\ \\ \\forall\\ w' \\in\\ sent_2$\n",
    "\n",
    "$q = {\\sum_{w'\\in\\ sent_2}max(df[w][w']) \\over len(sent_2)} \\ \\ \\ \\forall\\ w \\in\\ sent_1$\n",
    "\n",
    "$sim = \\left\\{ \\begin{array}{rcl} \n",
    "0  & if\\ p+q = 0\\\\\n",
    "{2 p*q \\over (p+q)}  & others\\\\\n",
    "\\end{array}\n",
    "\\right.$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def harmonic_best_pair_word_sim(sent1,sent2,model):\n",
    "    p=0\n",
    "    for wordA in sent1:\n",
    "        m = 0\n",
    "        for wordB in sent2:\n",
    "            try:\n",
    "                m = max(m, model.wv.similarity(wordA,wordB))\n",
    "            except:\n",
    "                pass\n",
    "        p += m\n",
    "    p = p/len(sent1)\n",
    "\n",
    "    q=0\n",
    "    for wordA in sent2:\n",
    "        m = 0\n",
    "        for wordB in sent1:\n",
    "            try:\n",
    "                m = max(m, model.wv.similarity(wordA,wordB))\n",
    "            except:\n",
    "                pass\n",
    "        q += m\n",
    "    q = q/len(sent2)\n",
    "\n",
    "    sim = 2*p*q/(p+q or 1)\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dissimilar sentences w2v_harmonic_best_pair_word similarity 0.5256416051928539\n",
      "Dissimilar sentences without stopwords w2v_harmonic_best_pair_word similarity 0.0\n",
      "Similar sentences w2v_harmonic_best_pair_word 0.7539022594752681\n",
      "Similar sentences w2v_harmonic_best_pair_word without stopwords 0.8636539612550971\n"
     ]
    }
   ],
   "source": [
    "print('Dissimilar sentences w2v_harmonic_best_pair_word similarity', \n",
    "      harmonic_best_pair_word_sim(sent3, sent2, p2v))\n",
    "print('Dissimilar sentences without stopwords w2v_harmonic_best_pair_word similarity',\n",
    "      harmonic_best_pair_word_sim(sent3s, sent2s, p2v))\n",
    "print('Similar sentences w2v_harmonic_best_pair_word', \n",
    "      harmonic_best_pair_word_sim(sent1, sent2, p2v))\n",
    "print('Similar sentences w2v_harmonic_best_pair_word without stopwords',\n",
    "      harmonic_best_pair_word_sim(sent1sl, sent2sl, p2v))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Gensim Particular Measures\n",
    "\n",
    "Gensim jaccard and cosine are impossible to measure because the p2v_bow vector is needed, but not exist.\n",
    "\n",
    "## 4.1 Cosine using Gensim p2v of a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p2v sentence vector similarity without transformation 0.18863933\n",
      "p2v sentence vector similarity without transformation 0.7152189\n"
     ]
    }
   ],
   "source": [
    "vec_sent1 = p2v.wv[sent1]\n",
    "vec_sent2 = p2v.wv[sent2]\n",
    "\n",
    "vec_sent1_ = sum(vec_sent1).reshape(1,-1)\n",
    "vec_sent2_ = sum(vec_sent2).reshape(1,-1)\n",
    "\n",
    "print('p2v sentence vector similarity without transformation',\n",
    "      cosine_similarity(vec_sent1,vec_sent2)[0][0])\n",
    "print('p2v sentence vector similarity without transformation',\n",
    "      cosine_similarity(vec_sent1_,vec_sent2_)[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Gensim p2v.n_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6423358595531631\n",
      "0.7152189931006554\n",
      "0.8741961944624282\n"
     ]
    }
   ],
   "source": [
    "print(p2v.wv.n_similarity(sent3s,sent1))\n",
    "print(p2v.wv.n_similarity(sent1,sent2))\n",
    "print(p2v.wv.n_similarity(sent1sl,sent2sl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Gensim p2v.similarity\n",
    "\n",
    "A score constructed with this method based on an international article.[John2016](#John2016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6345771081022482"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p2v.wv.similarity('woman','man')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_sim_jonh2016(sent1, sent2, model):\n",
    "    \"\"\":type sent1,sent2: list of strings\"\"\"\n",
    "    \n",
    "    sim_vector = []\n",
    "    ALPHA = 0.25\n",
    "\n",
    "    for wordA in sent1:\n",
    "        for wordB in sent2:\n",
    "            try:\n",
    "                sim = p2v.wv.similarity(wordA,wordB)\n",
    "                if sim > ALPHA:\n",
    "                    sim_vector.append(sim)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    return sum(sim_vector)/(len(sim_vector) or 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar sentences w2v.similarity 0.5202314680642451\n",
      "Similar sentences w2v.similarity without stopwords 0.6844719901559259\n"
     ]
    }
   ],
   "source": [
    "print('Similar sentences w2v.similarity', sent_sim_jonh2016(sent1,sent2, p2v))\n",
    "print('Similar sentences w2v.similarity without stopwords', sent_sim_jonh2016(sent1sl,sent2sl, p2v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Gensim Hellinger sentence similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abelma/wordembd/lib/python3.5/site-packages/gensim/matutils.py:906: RuntimeWarning: invalid value encountered in sqrt\n",
      "  sim = np.sqrt(0.5 * ((np.sqrt(vec1) - np.sqrt(vec2))**2).sum())\n"
     ]
    }
   ],
   "source": [
    "from gensim.matutils import kullback_leibler, hellinger\n",
    "print(hellinger(p2v_sent1,p2v_sent2))\n",
    "print(kullback_leibler(p2v_sent1,p2v_sent2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 The case of Infered Vector of a Sentences\n",
    "\n",
    "Paragraph to Vector in Gensim library has this method, which is not present in other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.00455156, -0.0286791 , -0.01700589,  0.04639047, -0.00288303,\n",
       "       -0.04456371,  0.01654539, -0.02167564, -0.00419786,  0.0179023 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_sent1_infer_p2v = p2v.infer_vector(sent1)\n",
    "vec_sent2_infer_p2v = p2v.infer_vector(sent2)\n",
    "\n",
    "#Stopword filtering\n",
    "vec_sent1s_infer_p2v = p2v.infer_vector(sent1sl)\n",
    "vec_sent2s_infer_p2v = p2v.infer_vector(sent2sl)\n",
    "\n",
    "#print the Paragraph2vector of the sentence 1\n",
    "print(len(vec_sent1_infer_p2v))\n",
    "vec_sent1_infer_p2v[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying Similarity on Infered Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p2v infered vector 0.19902407\n",
      "p2v infered vector without stopwords 0.60062534\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "print('p2v infered vector',\n",
    "      cosine_similarity(vec_sent1_infer_p2v.reshape(1,-1),vec_sent2_infer_p2v.reshape(1,-1))[0][0])\n",
    "print('p2v infered vector without stopwords',\n",
    "      cosine_similarity(vec_sent1s_infer_p2v.reshape(1,-1),vec_sent2s_infer_p2v.reshape(1,-1))[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "Same as Word2Vec this model doesn't works with bow structure, it represent a word as a vector of *size parameter* value length. At the same time this model can infered a vector for a sentence. The experiments shows that with the same corpus and the same sentences the paragraph2vec needs to lowercase all tokens, e.g. 'Alice' or 'Here', this behavior is different to Word2Vec model.\n",
    "\n",
    "- The best similarities using this text representation models must be implemented with innovatives ideas. For example: ``sent_sim_jonh2016`` and ``harmonic_best_pair_word_sim``.\n",
    "- In almost all cases the stopword filtering increment the similarity between similar sentences and diminished similarity between different sentences.\n",
    "- Gensim Hellinger and Kullback Leibler still been useless.\n",
    "\n",
    "\n",
    "# Recomendations\n",
    "\n",
    "* Made the same example with Wikipedia dump data, to test the similarity difference according to training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='references'></a>\n",
    "# References\n",
    "\n",
    "<a id='Lee2014'></a>\n",
    "*[Lee and Mikolov]* Quoc Le and Tomas Mikolov.\n",
    "**Distributed Representations of Sentences and Documents**. 2014. \n",
    "Proceedings of the 31 st International Conference on Machine Learning, Beijing, China.\n",
    "\n",
    "<a id='John2016'></a>\n",
    "*[John2016]* John, Adebayo Kolawole and Caro, Luigi Di and Boella, Guido. \n",
    "**NORMAS at SemEval-2016 Task 1: SEMSIM: A Multi-Feature Approach to Semantic Text Similarity**. \n",
    "Publisher ACM, 2016."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

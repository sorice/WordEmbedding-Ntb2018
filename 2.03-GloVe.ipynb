{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GloVe Text Representation Model\n",
    "\n",
    "*Glove+Gensim software example.*\n",
    "\n",
    "**Prerequisites:** Skills in tokenization with nltk, conceptual knowledge about GloVe Text Representation model.\n",
    "\n",
    "**data:** Gutenberg Corpus\n",
    "\n",
    "**Install:**\n",
    "pip install glove_python\n",
    "\n",
    "## Outline\n",
    "\n",
    "**Main Goal:** To practice how to create GloVe model with glove-python and Gensim packages, using NLTK preprocessing. Then introduce how to extract features from this text representation model, and finally how to measure text similarity using the previous result.\n",
    "\n",
    "- Acquiring and wrangling data for model initialization. \n",
    "- GloVe model generation/loading example\n",
    "- Sklearn, Scipy text similarity measures examples\n",
    "- GloVe original measures examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is GloVe?\n",
    "\n",
    "...[(XXX)](#XXX).\n",
    "\n",
    "**Note**: About Gensim and NLTK software please read the introductions notes about them in [2.01-TfIdf Notebook](2.01-TfIdf.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glove import Corpus, Glove\n",
    "import gensim\n",
    "from six import iteritems\n",
    "from gensim.corpora import TextCorpus, MmCorpus, Dictionary\n",
    "import time\n",
    "import nltk\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrangling Data\n",
    "\n",
    "From txt to iterable of lists of strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_collection = []\n",
    "file_path = 'gutenberg/'\n",
    "file_list = list(os.popen('ls '+ file_path).read().split('\\n'))\n",
    "for file in file_list:\n",
    "    if file:\n",
    "        with open(os.path.join(file_path,file)) as doc:\n",
    "            doc_collection.append(doc.read())\n",
    "            \n",
    "#Wrangling the data from list of doc-strings -> list of word-list by sentences\n",
    "sentences = []\n",
    "for doc in range(len(doc_collection)):\n",
    "    for sent in nltk.sent_tokenize(doc_collection[doc]):\n",
    "        sent_words = []\n",
    "        for word in nltk.word_tokenize(sent):\n",
    "            sent_words.append(word)\n",
    "        sentences.append(sent_words)\n",
    "            \n",
    "# load nltk's English stopwords as variable called 'stopwords'\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "texts = [[word for word in sentence if word not in stopwords] for sentence in sentences]\n",
    "\n",
    "id2word = gensim.corpora.Dictionary(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating GloVe Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GloVe Model Generated in 4 seconds\n"
     ]
    }
   ],
   "source": [
    "init = time.time()\n",
    "corpus_model = Corpus()\n",
    "corpus_model.fit(texts, window=10)\n",
    "end = time.time()-init\n",
    "corpus_model.save('gensim_data/glove_Gutenberg_corpus.model')\n",
    "print('GloVe Model Generated in %d seconds' % end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing 0 training epochs with 2 threads\n"
     ]
    }
   ],
   "source": [
    "glove = Glove(no_components=300, learning_rate=0.05)\n",
    "glove.fit(corpus_model.matrix, epochs=0, no_threads=2, verbose=True)\n",
    "glove.add_dictionary(corpus_model.dictionary)\n",
    "glove.save('gensim_data/glove_Gutenberg.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 9.97149822e-04, -6.69243747e-04, -1.99461371e-05,  1.65652432e-03,\n",
       "        7.86864625e-04, -1.22957514e-03,  1.40204706e-03,  8.97689592e-04,\n",
       "        9.06579444e-04, -1.13992907e-03])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Showing a word vector, dictionary-key is needed\n",
    "glove.word_vectors[glove.dictionary['girl']][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sklearn GlobVe-Cosine sentence similarity\n",
    "\n",
    "### Wrangling Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = 'the girl run into the hall'\n",
    "sentence2 = 'Here Alice run to the hall'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def preproc_data(sent1, sent2, model):\n",
    "    sentence1 = sent1.split()\n",
    "    sentence2 = sent2.split()\n",
    "\n",
    "    globvec_sent1 = []\n",
    "    globvec_sent2 = []\n",
    "\n",
    "    for word in sentence1:\n",
    "        try:\n",
    "            glove.dictionary[word]\n",
    "            globvec_sent1.append(glove.word_vectors[glove.dictionary[word]])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    for word in sentence2:\n",
    "        try:\n",
    "            glove.dictionary[word]\n",
    "            globvec_sent2.append(glove.word_vectors[glove.dictionary[word]])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "\n",
    "    globvec_sent1 = sum(np.asarray(globvec_sent1))\n",
    "    globvec_sent2 = sum(np.asarray(globvec_sent2))\n",
    "    \n",
    "    A = globvec_sent1.reshape(1,-1)\n",
    "    B = globvec_sent2.reshape(1,-1)\n",
    "    \n",
    "    return A, B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 300)\n",
      "[-3.77823606e-04  5.52589552e-04 -2.03766040e-04 -2.25560992e-04\n",
      " -2.46303425e-03  1.05855900e-03 -2.60252426e-03 -5.67222960e-04\n",
      "  1.31821014e-05  2.79117383e-03]\n"
     ]
    }
   ],
   "source": [
    "globvec_sent1, globvec_sent2 = preproc_data(sentence1,sentence2,glove)\n",
    "print(globvec_sent1.shape)\n",
    "print(globvec_sent2[0][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6076353473981132"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cosine_similarity(globvec_sent1,globvec_sent2)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7088825862303931"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Filtering stopwords\n",
    "sent1s = 'girl run hall'\n",
    "sent2s = 'Alice run hall'\n",
    "globvec_sent1s, globvec_sent2s = preproc_data(sent1s,sent2s,glove)\n",
    "cosine_similarity(globvec_sent1s,globvec_sent2s)[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scipy Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3923646526018869\n",
      "0.29111741376960687\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import cosine as cosine_scipy\n",
    "\n",
    "print(cosine_scipy(globvec_sent1,globvec_sent2))\n",
    "print(cosine_scipy(globvec_sent1s,globvec_sent2s)) #Filtering stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glove transform_paragraph method\n",
    "\n",
    "Still remains experimental doesn't works.\n",
    "\n",
    "Other methods like Harmonic mean of word-similarity are not possible with this package because this methods are implemented in every Representation Text model in Gensim, but Gensim doesn't have an implementation of GloVe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "1. Like word2vec this model allows parallelism training.\n",
    "2. Like word2vec this model generates a vector for every word appearing in the corpus with length = 'no_components'.\n",
    "3. Applying cumulative sum of ndarrays it is possible to obtain a sentence vector of the same length.\n",
    "4. Then using a regular scipy, sklearn, textsim tokendist or vector similarity distance the similarity between 2 sentences is possible.\n",
    "5. Using this mechanism it is possible to reproduce some **word embedding features** used in Paraphrase Recognition task.\n",
    "6. Generate GlobVe model with English Wikipedia it is no possible in this computer (i7 8Gb RAM).\n",
    "\n",
    "# Recommendations\n",
    "\n",
    "- Test GlobVe with spanish wikipedia."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

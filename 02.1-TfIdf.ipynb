{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Similarity using Tf-Idf Model\n",
    "\n",
    "**Prerequisites:** Skills in tokenization with nltk, knowledge of TfIdf Text Representation model.\n",
    "\n",
    "## Outline\n",
    "\n",
    "**Main Goal:** To practice how to create TfIdf models with Gensim Tf-Idf implementations, using NLTK preprocessing. Then introduce how to extract features from this text representation, and finally how to measure text similarity using previous results.\n",
    "\n",
    "- Gensim Corpus Inizialization\n",
    "- TfIdf model generation\n",
    "- Wrangling data from BOW to numpy objects\n",
    "- Text similarity measures examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About Gensim\n",
    "\n",
    "Gensim is a Python library for *topic modelling*, *document indexing*\n",
    "and *similarity retrieval* with large corpora. Target audience is the\n",
    "*natural language processing* (NLP) and *information retrieval* (IR)\n",
    "community. [Gensim Documentation](https://radimrehurek.com/gensim/tutorial.html)\n",
    "\n",
    "## About NLTK\n",
    "\n",
    "Natural Language ToolKit (NLTK) is a comprehensive Python library for natural language\n",
    "processing and text analytics. Originally designed for teaching, it has been adopted in the\n",
    "industry for research and development due to its usefulness and breadth of coverage. NLTK\n",
    "is often used for rapid prototyping of text processing programs and can even be used in\n",
    "production applications. [(Perkins2014)](#Perkins2014)\n",
    "\n",
    "## What is TfIdf?\n",
    "\n",
    "In information retrieval, tf–idf or TFIDF, short for term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus [(Salton1983)](#Salton1983)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import numpy as np\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.corpora import TextCorpus, MmCorpus, Dictionary\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_path = '/media/DATA/wiki_es/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrangling Data\n",
    "\n",
    "From txt collection to a list of strings, and from string-list to a list of word-list by sentence-list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_collection = []\n",
    "file_path = 'data/gutenberg/'\n",
    "file_list = list(os.popen('ls '+ file_path).read().split('\\n'))\n",
    "for file in file_list:\n",
    "    if file:\n",
    "        with open(os.path.join(file_path,file)) as doc:\n",
    "            doc_collection.append(doc.read().lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n"
     ]
    }
   ],
   "source": [
    "tokenized_text = [[word for word in nltk.word_tokenize(doc)] for doc in doc_collection]\n",
    "\n",
    "print(len(tokenized_text))\n",
    "# load nltk's English stopwords as variable called 'stopwords'\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "#remove stop words\n",
    "texts = [[word for word in text if word not in stopwords] for text in tokenized_text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating the TfIdf Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time 1.701983 segundos.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    tfidf = TfidfModel.load('models/gutenberg_tfidfA.model')\n",
    "    id2word = Dictionary(texts)\n",
    "    id2word.filter_extremes(no_below=2, no_above=0.6)\n",
    "    print('Pre-generated model TfIdf in 1.897 seconds.')\n",
    "\n",
    "except:\n",
    "    init = time.time()\n",
    "    # Create dictionary with tid to token mappings (or alternatively load one)\n",
    "    id2word = Dictionary(texts)\n",
    "\n",
    "    #remove extremes (similar to the min/max df step used when creating the tf-idf matrix)\n",
    "    #id2word.filter_extremes(no_below=2, no_above=0.6)\n",
    "\n",
    "    #convert the dictionary to a bag of words corpus for reference\n",
    "    bow_corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "    #generating the tf-idf model\n",
    "    tfidf = TfidfModel(bow_corpus,id2word=id2word)\n",
    "    end = time.time()-init\n",
    "    tfidf._smart_save('models/gutenberg_tfidfB.model')\n",
    "    print('Total time %f segundos.' % end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(26576, 1)]\n",
      "2.3923174227787602\n"
     ]
    }
   ],
   "source": [
    "print(id2word.doc2bow(['alice']))\n",
    "print(tfidf.idfs[id2word.doc2bow(['alice'])[0][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('_joint_', 4.392317422778761),\n",
       " ('_just_', 4.392317422778761),\n",
       " ('_lady_', 4.392317422778761),\n",
       " ('_letting_', 4.392317422778761),\n",
       " ('_little_', 4.392317422778761),\n",
       " ('_man_', 4.392317422778761),\n",
       " ('_married_', 4.392317422778761),\n",
       " ('_marry_', 4.392317422778761),\n",
       " ('_may_', 3.3923174227787602),\n",
       " ('_me_', 4.392317422778761)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(tfidf.id2word[i],tfidf.idfs[i]) for i in range(90,100)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the Tf model is very simple, only the word, ans its related tfidf coefficient. The Gensim implementation contains a method - *\\_\\_getitem\\_\\_* - that return the tfidf representation of an input bag of word vector. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sklearn TfIdf-Cosine Sentence Similarity\n",
    "\n",
    "### Wrangling Data\n",
    "\n",
    "* First: From string-sentences to bow representation of a sentence.\n",
    "* Second: From bow representation to numerical-list representation of a sentence.\n",
    "* Third: From numerical-list vector to numerical-vector (numpy) representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = 'the girl run into the hall'\n",
    "sentence2 = 'Here Alice run to the hall'\n",
    "\n",
    "sent1 = sentence1.lower().split()\n",
    "sent2 = sentence2.lower().split()\n",
    "\n",
    "sent1s = 'girl run hall'\n",
    "sent2s = 'Alice run hall'\n",
    "\n",
    "sent1sl = sent1s.lower().split()\n",
    "sent2sl = sent2s.lower().split()\n",
    "\n",
    "#If we change the sent1 by a very different meaning sent3\n",
    "sent3 = ['the','boy','eat','a','red','apple']\n",
    "sent3s = ['boy','eat','red','apple']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def preproc_data(sent1, sent2, model):\n",
    "    \n",
    "    #from raw sent to bowvec sent\n",
    "    bowvec_sent1 = id2word.doc2bow(sent1)\n",
    "    bowvec_sent2 = id2word.doc2bow(sent2)\n",
    "\n",
    "    bowvec_sent1_tfidf = tfidf[bowvec_sent1]\n",
    "    bowvec_sent2_tfidf = tfidf[bowvec_sent2]\n",
    "    \n",
    "    #from bowvec to numerical list sent\n",
    "    \n",
    "    nvec1 = []\n",
    "    nvec2 = []\n",
    "    vec1 = dict(bowvec_sent1_tfidf)\n",
    "    vec2 = dict(bowvec_sent2_tfidf)\n",
    "    words = set(vec1.keys()).union(vec2.keys())\n",
    "    for word in words:\n",
    "        nvec1.append(vec1.get(word,0.0))\n",
    "        nvec2.append(vec2.get(word,0.0))\n",
    "        \n",
    "    #from numerical list sent to numpy vec\n",
    "    nvec_sent1_tfidf = np.asarray(nvec1)\n",
    "    nvec_sent2_tfidf = np.asarray(nvec2)\n",
    "    A = nvec_sent1_tfidf.reshape(1,-1)\n",
    "    B = nvec_sent2_tfidf.reshape(1,-1)\n",
    "    \n",
    "    return bowvec_sent1_tfidf,bowvec_sent2_tfidf,nvec1,nvec2, A, B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bow-Vec with tfidf values of sent 1 [(3236, 0.7916654345238935), (3419, 0.5554386815550553), (6105, 0.2544675044332316)]\n",
      "Numerical list of sent 1 [0.0, 0.2544675044332316, 0.5554386815550553, 0.7916654345238935]\n",
      "Numpy vector of sent 1 [[0.         0.2544675  0.55543868 0.79166543]]\n"
     ]
    }
   ],
   "source": [
    "bowvec_sent1_tfidf,bowvec_sent2_tfidf,nvec1,nvec2, A, B = preproc_data(sent1,sent2,tfidf)\n",
    "print('Bow-Vec with tfidf values of sent 1', bowvec_sent1_tfidf)\n",
    "print('Numerical list of sent 1',nvec1)\n",
    "print('Numpy vector of sent 1', A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the []\n",
      "girl [(3236, 1)]\n",
      "run [(6105, 1)]\n",
      "into []\n",
      "the []\n",
      "hall [(3419, 1)]\n"
     ]
    }
   ],
   "source": [
    "for word in sent1:\n",
    "    print(word,id2word.doc2bow([word]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note_: seems like if this model filter the stopwords automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1330855207623347"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cosine_similarity(A,B)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1330855207623347"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bowvec_sent1s_tfidf,bowvec_sent2s_tfidf,nvec1s,nvec2s, As, Bs = preproc_data(sent1sl,sent2sl,tfidf)\n",
    "cosine_similarity(As,Bs)[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scipy TfIdf-Cosine sentence similarity\n",
    "\n",
    "$Note: cosine_{Scipy\\ distance} = 1 - cosine_{Sklearn\\ similarity}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8669144792376653\n",
      "0.8669144792376653\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import cosine as cosine_scipy\n",
    "print(cosine_scipy(nvec1,nvec2))\n",
    "print(cosine_scipy(nvec1s,nvec2s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gensim tfidf.n_similarity\n",
    "\n",
    "Do not exist this kind of method! (Press 'Tab' key in the next cell to check it!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Pair Word Overlap\n",
    "\n",
    "Lets try a different way to compound a sentence similarity, based on WordNet-Augmented-Word-Overlap similarity idea.\n",
    "\n",
    "$p = {\\sum_{w\\in\\ sent_1}max(df[w][w']) \\over len(sent_1)} \\ \\ \\ \\forall\\ w' \\in\\ sent_2$\n",
    "\n",
    "$q = {\\sum_{w'\\in\\ sent_2}max(df[w][w']) \\over len(sent_2)} \\ \\ \\ \\forall\\ w \\in\\ sent_1$\n",
    "\n",
    "$sim = \\left\\{ \\begin{array}{rcl} \n",
    "0  & if\\ p+q = 0\\\\\n",
    "{2 p*q \\over (p+q)}  & others\\\\\n",
    "\\end{array}\n",
    "\\right.$\n",
    "\n",
    "Due to the unmanagability of TfIdf Gensim object, and the few examples I could get, I decided to create de TfIdf with sklearn and then manipulated, to get the tfidf vector o a word, see the example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(53415, 21)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_files\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "vectorizer = TfidfVectorizer(min_df=1, lowercase=True) #note: by default CountVectorizer, TfidfVectorizer use lowercase=True\n",
    "corpus = load_files('data/',categories=['gutenberg'])\n",
    "TfIdfMatrix = vectorizer.fit_transform(corpus.data)\n",
    "pdTfIdf = pd.DataFrame(TfIdfMatrix.toarray(), columns=vectorizer.get_feature_names())\n",
    "pdTfIdf = pdTfIdf.T\n",
    "pdTfIdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.24275368e-01, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        3.97026332e-04, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 1.10916056e-03, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        3.10535868e-04]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdTfIdf.loc['alice'].values.reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def harmonic_best_pair_word_sim(sent1,sent2, pdTfIdf):\n",
    "    p=0\n",
    "    for wi in sent1:\n",
    "        m = 0\n",
    "        for wc in sent2:\n",
    "            try:\n",
    "                winp = pdTfIdf.loc[wi].values.reshape(1,-1)\n",
    "                wcnp = pdTfIdf.loc[wc].values.reshape(1,-1)\n",
    "                m = float(max(m, cosine_similarity(winp,wcnp)))\n",
    "            except:\n",
    "                pass\n",
    "        p += m\n",
    "    p = p/len(sent1)\n",
    "\n",
    "    q=0\n",
    "    for wc in sent2:\n",
    "        m = 0\n",
    "        for wi in sent1:\n",
    "            try:\n",
    "                wcnp = pdTfIdf.loc[wc].values.reshape(1,-1)\n",
    "                winp = pdTfIdf.loc[wi].values.reshape(1,-1)\n",
    "                m = float(max(m, cosine_similarity(winp,wcnp)))\n",
    "                #print(m, type(m))\n",
    "            except:\n",
    "                pass\n",
    "        q += m\n",
    "    q = q/len(sent2)\n",
    "\n",
    "    sim = 2*p*q/(p+q or 1)\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dissimilar sentences tfidf_harmonic_best_pair_word similarity 0.7190818951347712\n",
      "Dissimilar sentences without stopwords tfidf_harmonic_best_pair_word similarity 0.0\n",
      "Similar sentences tfidf_harmonic_best_pair_word 0.9119930077479885\n",
      "Similar sentences tfidf_harmonic_best_pair_word without stopwords 0.8445119942483112\n"
     ]
    }
   ],
   "source": [
    "print('Dissimilar sentences tfidf_harmonic_best_pair_word similarity', \n",
    "      harmonic_best_pair_word_sim(sent3, sent2, pdTfIdf))\n",
    "print('Dissimilar sentences without stopwords tfidf_harmonic_best_pair_word similarity',\n",
    "      harmonic_best_pair_word_sim(sent3s, sent2s, pdTfIdf))\n",
    "print('Similar sentences tfidf_harmonic_best_pair_word', \n",
    "      harmonic_best_pair_word_sim(sent1, sent2, pdTfIdf))\n",
    "print('Similar sentences tfidf_harmonic_best_pair_word without stopwords',\n",
    "      harmonic_best_pair_word_sim(sent1sl, sent2sl, pdTfIdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gensim TfIdf-Hellinger sentence similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9744522138374331\n",
      "0.9744522138374331\n",
      "inf\n"
     ]
    }
   ],
   "source": [
    "from gensim.matutils import kullback_leibler, jaccard, hellinger, cossim\n",
    "\n",
    "print(hellinger(bowvec_sent1_tfidf,bowvec_sent2_tfidf))\n",
    "print(hellinger(A,B))\n",
    "print(kullback_leibler(A, B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gensim Cosine: 0.13308552076233474\n",
      "Gensim Cosine, filtering stopwords: 0.13308552076233474\n",
      "Gensim Jaccard: 0.8992553737425453\n"
     ]
    }
   ],
   "source": [
    "print('Gensim Cosine:',cossim(bowvec_sent1_tfidf,bowvec_sent2_tfidf))\n",
    "print('Gensim Cosine, filtering stopwords:',cossim(bowvec_sent1_tfidf,bowvec_sent2_tfidf))\n",
    "print('Gensim Jaccard:',jaccard(bowvec_sent1_tfidf,bowvec_sent2_tfidf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "* As you can test the TfIdf doesn't have a fast or parallel solution.\n",
    "* In Gensim TfIdf model is generated from bowvecs.\n",
    "* There is a good variation between Cosine, Word Overlap and Hellinger, this could be interesting to analize in a big dataset.\n",
    "* Interesting too is that Gensim and Sklearn cosine have the same result.\n",
    "* TfIdf Model filter stopword automatically, then the similarity comparison between original sentences and preprocessed sentences are equal.\n",
    "* ``harmonic_best_pair_word_sim`` distance separated very well between similar and dissimilar sentences without stopwords.\n",
    "\n",
    "# Recommendations\n",
    "\n",
    "* Made the same example with Wikipedia dump data, to test the similarity difference according to data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='References'></a>\n",
    "# References\n",
    "\n",
    "<a id='Perkins2014'></a>\n",
    "[1] *[Perkins2014]* Jacov Perkins. \n",
    "Book **Python 3 Text Processing with NLTK 3 Cookbook**. 2014. \n",
    "p. 7 **ISBN**: 978-1-78216-785-3\n",
    "\n",
    "[2] *[Salton1983]* Salton, G; McGill, M. J. (1986). **Introduction to modern information retrieval**. McGraw-Hill. \n",
    "**ISBN**: 978-0-07-054484-0.\n",
    "<a id='Salton1983'></a>\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

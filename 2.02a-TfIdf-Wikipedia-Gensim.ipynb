{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tf-Idf with Wikipedia & Gensim Package\n",
    "\n",
    "*Gensim, Scipy, Sklearn software examples.*\n",
    "\n",
    "**Note**: The next sample codes are made using the data obtained after the transformation of Wikipedia dump with `gensim.scripts.make_wikicorpus.py` methods to converted it to Bag of Word model.\n",
    "\n",
    "**Prerequisites:** Skills in tokenization with nltk, knowledge of Word2Vec Text Representation model.\n",
    "\n",
    "## Outline\n",
    "\n",
    "**Main Goal:** To practice how to create _tfidf_ model using Wikipedia corpus. As previous notebook the roadmap it is generate de model, then learn how to extract information from it, and finally how to measure word similarity using the model as base.\n",
    "\n",
    "- Acquiring and wrangling data for model initialization. \n",
    "- Gensim TfIdf model generation/loading example\n",
    "- Sklearn, Scipy text similarity measures examples\n",
    "- Gensim original measures examples\n",
    "\n",
    "This notebook includes the results using `gensim.model.TfIdfModel`, but in order to standarize results with other word-embedding methods a solution with `sklearn.feature_extraction.text.TfidfVectorizer` class is taking as main approach. The TfIdfModel only build a (2,) shape vector instead of  TfidfVectorizer that creates a (300,) shape vector.\n",
    "\n",
    "**Note**: About Gensim and NLTK software please read the introductions notes about them in [2.01-TfIdf Notebook](2.01-TfIdf.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import TfidfModel\n",
    "from gensim.corpora import TextCorpus, MmCorpus, Dictionary\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_path = '/opt/wiki_es/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Acquiring & Wrangling Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Text does not contain num_docs on the first line.\n"
     ]
    }
   ],
   "source": [
    "#Loading resources generated priviously with Gensim package\n",
    "dictionary = Dictionary.load_from_text(corpus_path+'_wordids.txt.bz2')\n",
    "bow_corpus = MmCorpus(corpus_path+'_bow.mm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Generating the Tf-Idf model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfIdf Model Generated in 658.466673374176 seconds\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    tfidf =TfidfModel.load(corpus_path+'wiki-tfidf.model')\n",
    "    print('TfIdf Model Generated in 658.466673374176 seconds')\n",
    "except:\n",
    "    init = time.time()\n",
    "    tfidf = TfidfModel(bow_corpus,dictionary)\n",
    "    end = time.time()-init\n",
    "    print(end)\n",
    "    tfidf._smart_save(corpus_path+'wiki-tfidf.model')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After 11 minutes my 1stG-i7 laptop, 8Gb RAM, finish the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "australianos\n",
      "[(11768, 1), (26452, 1)]\n",
      "[(11768, 0.8147309863935951), (26452, 0.5798391326998545)]\n"
     ]
    }
   ],
   "source": [
    "#print the word with index 1000 in the dictionary\n",
    "print(dictionary[1000])\n",
    "\n",
    "#print the bow vector of the sentence 1\n",
    "sent = \"Yo como pescado\"\n",
    "vec_sent = dictionary.doc2bow(sent.lower().split())\n",
    "print(vec_sent)\n",
    "\n",
    "#print the TFIDF vector of the sentence 1\n",
    "vec_sent_tfidf = tfidf[vec_sent]\n",
    "print(vec_sent_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Measuring Similarity between Pair of Sentences\n",
    "\n",
    "This section is made to show the utility of _tfidf model in an applied example. Also to show some native similarity methods of `gensim.model.TfIdfModel` class.\n",
    "\n",
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = 'la niña corrió hacia el hueco'\n",
    "sentence2 = 'Alicia corrió hacia el hueco'\n",
    "\n",
    "sent1 = sentence1.lower().split()\n",
    "sent2 = sentence2.lower().split()\n",
    "\n",
    "#Filtering stopwords by hand\n",
    "sentence1_ws = 'niña corrió hueco'\n",
    "sentence2_ws = 'Alicia corrió hueco'\n",
    "\n",
    "sent1s = sentence1_ws.lower().split()\n",
    "sent2s = sentence2_ws.lower().split()\n",
    "\n",
    "#If we change the sent1 by a very different meaning sent3\n",
    "sentence3 = 'El niño comió una manzana roja'\n",
    "sentence3_ws = 'niño comió manzana roja'\n",
    "sent3 = ['el','niño','comió','una','manzana','roja']\n",
    "sent3s = ['niño','comió','manzana','roja']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Wrangling Data\n",
    "\n",
    "* First: From string-sentences to bow representation of a sentence.\n",
    "* Second: From bow representation to numerical-list representation of a sentence.\n",
    "* Third: From numerical-list vector to numerical-vector (numpy) representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.preprocess import wrang_tfidf\n",
    "#A = np.asarray(nvec1).reshape((1,-1))\n",
    "bowvec_sent1_tfidf,bowvec_sent2_tfidf,nvec1,nvec2, A, B = wrang_tfidf(sent1,sent2,tfidf, dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Sklearn TfIdf-Cosine sentence similarity\n",
    "\n",
    "The last experiment is made with TfIdf matrix from gensim.\n",
    "Unfortunately to load the Wikipedia dump to make a tf-idf index is to much for this computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7240525705998468\n",
      "0.7240525705998468\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "print(float(cosine_similarity(A,B)))\n",
    "\n",
    "#Filtering stopwords\n",
    "bowvec_sent1s_tfidf,bowvec_sent2s_tfidf,nvec1s,nvec2s, As, Bs = wrang_tfidf(sent1s,sent2s,tfidf,dictionary)\n",
    "print(float(cosine_similarity(As,Bs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Scipy TfIdf-Cosine sentence similarity\n",
    "\n",
    "Testing similarity with Scipy equations. A normalized vector with the above problem is showed to correct it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.27594742940015304\n",
      "0.27594742940015304\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import cosine as cosine_scipy\n",
    "from scipy.spatial.distance import jaccard as jaccard_scipy\n",
    "\n",
    "print(cosine_scipy(A,B))\n",
    "print(cosine_scipy(As,Bs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Best Pair Word Overlap\n",
    "\n",
    "To see the mathematical equations of this propousal read the section _3.4 Best Pair Word Overlap_ in the [2.1-TfIdf Notebook](02.1-TfIdf.ipynb).\n",
    "\n",
    "This implementation do not receibe a TfIdfModel derived variable, only works with vectors as it made in TfidfVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scripts.distances import best_pair_word_overlap\n",
    "\n",
    "print('Dissimilar sentences tfidf_harmonic_best_pair_word similarity', \n",
    "      best_pair_word_overlap(sent3, sent2, pdTfIdf))\n",
    "print('Dissimilar sentences without stopwords tfidf_harmonic_best_pair_word similarity',\n",
    "      best_pair_word_overlap(sent3s, sent2s, pdTfIdf))\n",
    "print('Similar sentences tfidf_harmonic_best_pair_word', \n",
    "      best_pair_word_overlap(sent1, sent2, pdTfIdf))\n",
    "print('Similar sentences tfidf_harmonic_best_pair_word without stopwords',\n",
    "      best_pair_word_overlap(sent1s, sent2s, pdTfIdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Textsim TfIdf-Jaccard sentence similarity\n",
    "\n",
    "Doing similarity with textsim package, and testing dissimilar sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "bowvec_sent4_tfidf,bowvec_sent3_tfidf,nvec4,nvec3, D, C = wrang_tfidf(sent1,sent3,tfidf, dictionary)\n",
    "bowvec_sent4s_tfidf,bowvec_sent3s_tfidf,nvec4s,nvec3s, Ds, Cs = wrang_tfidf(sent1s,sent3s,tfidf, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7240525705998468 TfIdf original Cosine Sklearn\n",
      "0.7240525705998468 TfIdf Textsim Cosine Sklearn\n",
      "0.7302967433402215 String Textsim Cosine Sklearn\n",
      "0.42857142857142855 String Textsim Jaccard\n",
      "0.8571428571428571 TfIdf Textsim Jaccard\n",
      "----------------------------------\n",
      "0.7240525705998468 TfIdf original Cosine Sklearn without stopwords\n",
      "0.7240525705998468 TfIdf Textsim Cosine Sklearn without stopwords\n",
      "0.6666666666666669 String Textsim Cosine Sklearn without stopwords\n",
      "0.5 String Textsim Jaccard without stopwords\n",
      "0.8571428571428571 TfIdf Textsim Jaccard without stopwords\n"
     ]
    }
   ],
   "source": [
    "from textsim.tokendists import jaccard_distance\n",
    "from textsim.tokendists import cosine_similarity_sklearn\n",
    "\n",
    "print(float(cosine_similarity(A,B)),'TfIdf original Cosine Sklearn')\n",
    "print(cosine_similarity_sklearn(A,B),'TfIdf Textsim Cosine Sklearn')\n",
    "print(cosine_similarity_sklearn(sentence1,sentence2),'String Textsim Cosine Sklearn')\n",
    "print(jaccard_distance(sentence1,sentence2), 'String Textsim Jaccard')\n",
    "print(jaccard_distance(nvec1,nvec2), 'TfIdf Textsim Jaccard')\n",
    "print('----------------------------------')\n",
    "#Filtering stopwords\n",
    "print(float(cosine_similarity(As,Bs)),'TfIdf original Cosine Sklearn without stopwords')\n",
    "print(cosine_similarity_sklearn(As,Bs),'TfIdf Textsim Cosine Sklearn without stopwords')\n",
    "print(cosine_similarity_sklearn(sentence1_ws,sentence2_ws),'String Textsim Cosine Sklearn without stopwords')\n",
    "print(jaccard_distance(sentence1_ws,sentence2_ws), 'String Textsim Jaccard without stopwords')\n",
    "print(jaccard_distance(nvec1s,nvec2s), 'TfIdf Textsim Jaccard without stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 TfIdf original Cosine Sklearn\n",
      "0.0 TfIdf Textsim Cosine Sklearn\n",
      "0.1666666666666667 String Textsim Cosine Sklearn\n",
      "1.0 String Textsim Jaccard\n",
      "0.875 TfIdf Textsim Jaccard\n",
      "0.0 TfIdf original Cosine Sklearn\n",
      "0.0 TfIdf Textsim Cosine Sklearn\n",
      "0.0 String Textsim Cosine Sklearn\n",
      "1.0 String Textsim Jaccard\n",
      "0.875 TfIdf Textsim Jaccard\n"
     ]
    }
   ],
   "source": [
    "#Dissimilar sentence\n",
    "print(float(cosine_similarity(D,C)),'TfIdf original Cosine Sklearn')\n",
    "print(cosine_similarity_sklearn(D,C),'TfIdf Textsim Cosine Sklearn')\n",
    "print(cosine_similarity_sklearn(sentence1,sentence3),'String Textsim Cosine Sklearn')\n",
    "print(jaccard_distance(sentence1,sentence3), 'String Textsim Jaccard')\n",
    "print(jaccard_distance(nvec4,nvec3), 'TfIdf Textsim Jaccard')\n",
    "print(float(cosine_similarity(Ds,Cs)),'TfIdf original Cosine Sklearn')\n",
    "print(cosine_similarity_sklearn(Ds,Cs),'TfIdf Textsim Cosine Sklearn')\n",
    "print(cosine_similarity_sklearn(sentence1_ws,sentence3_ws),'String Textsim Cosine Sklearn')\n",
    "print(jaccard_distance(sentence1_ws,sentence3_ws), 'String Textsim Jaccard')\n",
    "print(jaccard_distance(nvec4s,nvec3s), 'TfIdf Textsim Jaccard')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Gensim Original Measures\n",
    "\n",
    "Gensim has some native similarity measures, some of them are only implemented in some models.\n",
    "\n",
    "## 4.1 Gensim TfIdf-Hellinger sentence similarity\n",
    "\n",
    "Testing similarity with Gensim ecuations. Here are all possible calculations:\n",
    "\n",
    "* Kullback Leibler not accept bow_vec.  \n",
    "* Cossim not accept numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inf Gensim Kullback_leibler\n",
      "0.7242183179867516 Gensim Hellinger\n",
      "0.7242183179867516 Gensim Cosine\n",
      "0.724052570599847 Gensim Cosine\n",
      "0.8571428571428572 Gensim Jaccard\n",
      "0.6578982610464937 Gensim Jaccard\n",
      "----------------------------------\n",
      "inf Gensim Kullback_leibler\n",
      "0.7242183179867516 Gensim Hellinger\n",
      "0.7242183179867516 Gensim Cosine\n",
      "0.724052570599847 Gensim Cosine\n",
      "0.8571428571428572 Gensim Jaccard\n",
      "0.6578982610464937 Gensim Jaccard\n"
     ]
    }
   ],
   "source": [
    "from gensim.matutils import kullback_leibler, hellinger, cossim\n",
    "from gensim.matutils import jaccard as gjaccard\n",
    "\n",
    "print(kullback_leibler(A, B),'Gensim Kullback_leibler')\n",
    "print(hellinger(A,B), 'Gensim Hellinger')\n",
    "print(hellinger(bowvec_sent1_tfidf,bowvec_sent2_tfidf), 'Gensim Cosine')\n",
    "print(cossim(bowvec_sent1_tfidf,bowvec_sent2_tfidf),'Gensim Cosine')\n",
    "print(gjaccard(nvec1,nvec2),'Gensim Jaccard')\n",
    "print(gjaccard(bowvec_sent1_tfidf,bowvec_sent2_tfidf),'Gensim Jaccard')\n",
    "print('----------------------------------')\n",
    "#Filtering stopwords\n",
    "print(kullback_leibler(As, Bs),'Gensim Kullback_leibler')\n",
    "print(hellinger(As,Bs), 'Gensim Hellinger')\n",
    "print(hellinger(bowvec_sent1s_tfidf,bowvec_sent2s_tfidf), 'Gensim Cosine')\n",
    "print(cossim(bowvec_sent1s_tfidf,bowvec_sent2s_tfidf),'Gensim Cosine')\n",
    "print(gjaccard(nvec1s,nvec2s),'Gensim Jaccard')\n",
    "print(gjaccard(bowvec_sent1s_tfidf,bowvec_sent2s_tfidf),'Gensim Jaccard')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One problem with Hellinger equation in Gensim is that iterates over the major vector, then in the above example the word 74333(eat) never will affect the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TfidfVectorizer Example Code\n",
    "\n",
    "    import pickle\n",
    "    import gensim                                                        \n",
    "    from gensim.corpora import Dictionary\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from scipy.sparse.linalg import svds\n",
    "\n",
    "    corpus_path = '/media/abelma/SSD2/wiki_es/'\n",
    "    wiki_corpus = corpus_path+'dump/eswiki-20161201-pages-articles-multistream.xml.bz2'\n",
    "    dictionary = Dictionary.load\\_from\\_text(corpus_path+'\\_wordids.txt.bz2')\n",
    "    article_min_tokens=50\n",
    "    article_max_tokens=5000\n",
    "\n",
    "    def read_wikipedia_corpus(filename, \n",
    "                              dictionary=dictionary, \n",
    "                              article_min_tokens=article_min_tokens, \n",
    "                              article_max_tokens=article_max_tokens):\n",
    "\n",
    "        corpus = gensim.corpora.WikiCorpus(filename, \n",
    "                                           dictionary=dictionary,\n",
    "                                           article_min_tokens=article_min_tokens,\n",
    "                                           article_max_tokens=article_max_tokens,\n",
    "                                           lemmatize=None)\n",
    "\n",
    "        for text in corpus.get_texts():\n",
    "            yield ' '.join(word for word in text)\n",
    "\n",
    "    vectorizer = TfidfVectorizer(min_df=20,max_df=0.8, max_features=2000000, use_idf=True)#, ngram_range=(1,2))\n",
    "    TfIdfMatrix = vectorizer.fit_transform(read_wikipedia_corpus(wiki_corpus))\n",
    "    coGTfMatrix = coo_matrix(TfIdfMatrix.T)\n",
    "    coGTfMatrix.data = coGTfMatrix.data*1.0\n",
    "    Ug, Sg, Vg = ssvds(coGTfMatrix, k=300)\n",
    "    f = open('/media/abelma/SSD2/wiki_es/UgTfIdfMatrix.pkl','bw')\n",
    "    pickle.dump(TfIdfMatrix,f)\n",
    "    f = open('/media/abelma/SSD2/wiki_es/vectorizer.pkl','bw')\n",
    "    pickle.dump(vectorizer,f)\n",
    "    \n",
    "This code only works in computers with high amount of RAM (>=16Gb). In the process every huge matrix was serialized to a pickle object, to initialize the RAM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "f = open('/opt/wiki_es/vectorizer.pkl','br')\n",
    "vectorizer = pickle.load(f)\n",
    "f = open('/opt/wiki_es/UgTfIdfMatrix.pkl','br')\n",
    "Ug = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5560445857905273"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = Ug[vectorizer.vocabulary_['rey']].reshape((1,-1))\n",
    "B = Ug[vectorizer.vocabulary_['reina']].reshape((1,-1))\n",
    "float(cosine_similarity(A,B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "See the [3.1-Playfull-Experiments-with-MSRPC](3.1-Playfull-Experiments-with-MSRPC) to see more in detail the result analysis, and a feature selection based on a paraphrase recognition problem.\n",
    "Results analysis:\n",
    "\n",
    "|similarity| vector type | Measure | Library | Similar/Dissimilar | Stopword Filter |\n",
    "|----------|-------------|---------|---------|--------------------|-----------------|\n",
    "| inf      | numpy       | kull-Le | gensim  | sim                | yes/no          |\n",
    "| 1.0000   | str         | jaccard | textsim | diss               | yes/no          |\n",
    "| 0.8571   | numpy       | jaccard | textsim | sim/diss           | yes/no          |\n",
    "|          |             | jaccard | gensim  | sim                | yes/no          |\n",
    "| 0.7302   | str         | kcosine | textsim | sim                | no              |\n",
    "| 0.7240   | numpy       | kcosine | textsim | sim                | yes/no          |\n",
    "|          |             | cosine  | sklearn | sim                | yes/no          |\n",
    "|          |             | helling | gensim  | sim                | yes/no          |\n",
    "|          | bow         | helling | gensim  | sim                | yes/no          |\n",
    "|          | bow         | cosine  | gensim  | sim                | yes/no          |\n",
    "| 0.6666   | str         | kcosine | textsim | sim                | yes             |\n",
    "| 0.6578   | bow         | jaccard | gensim  | sim                | yes/no          |\n",
    "| 0.5000   | str         | jaccard | textsim | sim                | yes             |\n",
    "| 0.4285   | str         | jaccard | textsim | sim                | no              |\n",
    "| 0.2759   | numpy       | cosine  | scipy   | sim                | yes/no          |\n",
    "| 0.1666   | str         | kcosine | textsim | diss               | no              |\n",
    "| 0.0000   | numpy       | kcosine | textsim | diss               | yes/no          |\n",
    "| 0.0000   |             | cosine  | sklearn | diss               | yes/no          |\n",
    "| 0.0000   | str         | kcosine | textsim | diss               | yes             |\n",
    "\n",
    "\n",
    "* As you can self analyze Gensim and Sklearn cosine have the same result. \n",
    "* The example sentences have words in common and in the context of \"Alice's Adventures in Wonderland\" by Lewis Carroll have the same mining, this book is part of the Gutenberg collection but only appears on Wikipedia dump as articles of few importance.\n",
    "* Except textsim package with str input in jaccard and cosine distances can differenciate between stopword filtered sentences or not.\n",
    "* The combinations that classify well similar and dissimilar sentences are:\n",
    "    + numpy - cosine - sklearn -> diff = 0.72\n",
    "    + str - jaccard - textsim -> diff = 0.6\n",
    "    + str - cosine - textsim -> diff = 0.57\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

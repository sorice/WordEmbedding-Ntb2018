{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSI Text Representation Model\n",
    "\n",
    "*Gensim software example.*\n",
    "\n",
    "**Prerequisites:** Skills in tokenization with nltk, conceptual knowledge about LSI Text Representation model.\n",
    "\n",
    "**data:** Gutenberg Corpus\n",
    "\n",
    "## Outline\n",
    "\n",
    "**Main Goal:** To practice how to create the Latent Semantic Index model with Gensim and NLTK. Then introduce how to extract information from this text representation model, and finally how to measure word similarity using the previous result.\n",
    "\n",
    "- Acquiring and wrangling data for model initialization. \n",
    "- Gensim LSI model generation/loading example\n",
    "- Sklearn, Scipy text similarity measures examples\n",
    "- Gensim original measures examples\n",
    "\n",
    "**Note: Sklearn LSI codes remains pendent.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is LSI?\n",
    "\n",
    "... [(xxx)](#xxx).\n",
    "\n",
    "**Note**: About Gensim and NLTK software please read the introductions notes about them in [2.01-TfIdf Notebook](2.01-TfIdf.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LsiModel, TfidfModel\n",
    "from gensim.corpora import TextCorpus, MmCorpus, Dictionary\n",
    "import os\n",
    "import nltk\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrangling Data Step 1 \n",
    "\n",
    "From txt collection to a list of strings, and from string-list to a list of word-list by sentence-list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_collection = []\n",
    "file_path = 'gutenberg/'\n",
    "file_list = list(os.popen('ls '+ file_path).read().split('\\n'))\n",
    "for file in file_list:\n",
    "    if file:\n",
    "        with open(os.path.join(file_path,file)) as doc:\n",
    "            doc_collection.append(doc.read())\n",
    "\n",
    "#Wrangling the data from list of doc-strings -> list of word-list by sentences\n",
    "sentences = []\n",
    "for doc in range(len(doc_collection)):\n",
    "    for sent in nltk.sent_tokenize(doc_collection[doc]):\n",
    "        sent_words = []\n",
    "        for word in nltk.word_tokenize(sent):\n",
    "            sent_words.append(word)\n",
    "        sentences.append(sent_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating the LSI Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 0.0003159046173095703\n"
     ]
    }
   ],
   "source": [
    "init = time.time()\n",
    "dictionary = Dictionary(sentences)\n",
    "corpus = [dictionary.doc2bow(text) for text in sentences]\n",
    "#tfidf = TfidfModel(corpus, dictionary)\n",
    "lsi = LsiModel(corpus,id2word=dictionary,num_topics=300)\n",
    "end = time.time()-init\n",
    "print('Total time:', end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.00056955806905143347),\n",
       " (1, 0.00067110806665921202),\n",
       " (2, 0.0019692485159702447),\n",
       " (3, 0.0014663607785970927),\n",
       " (4, -0.00070440171189507632),\n",
       " (5, 0.0025261036712473928),\n",
       " (6, -0.00022047840204233559),\n",
       " (7, 0.0030931735846703359),\n",
       " (8, -0.0023823150419961716),\n",
       " (9, 0.0021076307016137039)]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsi[dictionary.doc2bow(['Alice'])][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sklearn LSI-Cosine sentence similarity\n",
    "\n",
    "### Wrangling Data\n",
    "\n",
    "From string-sentences to numerical vectors.\n",
    "\n",
    "In this model the preprocessing is not needed because if we create de bow vectors of the sentences, the LSI model can't handle the numerical vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "sentence1 = 'the girl run into the hall'\n",
    "sentence2 = 'Here Alice run to the hall'\n",
    "\n",
    "sent1 = sentence1.split()\n",
    "sent2 = sentence2.split()\n",
    "\n",
    "vec_sent1 = dictionary.doc2bow(sent1)\n",
    "vec_sent2 = dictionary.doc2bow(sent2)\n",
    "\n",
    "vec_sent1_lsi = np.asarray(lsi[vec_sent1])\n",
    "vec_sent2_lsi = np.asarray(lsi[vec_sent2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.58373211],\n",
       "       [ 1.        , -0.68532563],\n",
       "       [ 2.        ,  0.18886818],\n",
       "       [ 3.        ,  0.22626765],\n",
       "       [ 4.        , -0.44856283],\n",
       "       [ 5.        ,  0.58655806],\n",
       "       [ 6.        ,  0.55979944],\n",
       "       [ 7.        , -0.07446109],\n",
       "       [ 8.        , -0.30344566],\n",
       "       [ 9.        ,  0.0765748 ]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(vec_sent1_lsi))\n",
    "vec_sent2_lsi[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having into account that LSI model works with topics, the generated vector have 2 components in everyone of the 300 elements that compound the numerical vector: the id of the topic, and their respective value.\n",
    "The las step in wrangling this data is to eliminate the id, writing the vector as a 1D array only of float values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_sent1_lsi = vec_sent1_lsi[...,1]\n",
    "vec_sent2_lsi = vec_sent2_lsi[...,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.58373211, -0.68532563,  0.18886818,  0.22626765, -0.44856283,\n",
       "        0.58655806,  0.55979944, -0.07446109, -0.30344566,  0.0765748 ])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(vec_sent1_lsi))\n",
    "vec_sent2_lsi[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.62991053090495575"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cosine_similarity(vec_sent1_lsi.reshape(1,-1),vec_sent2_lsi.reshape(1,-1))[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99999999795024486"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Filtering stopwords\n",
    "sent1s = ['girl','run','hall']\n",
    "sent2s = ['Alice','run','hall']\n",
    "\n",
    "vec_sent1s_lsi = lsi[dictionary.doc2bow(sent1s)]\n",
    "vec_sent2s_lsi = lsi[dictionary.doc2bow(sent2s)]\n",
    "\n",
    "vec_sent1s_lsi = np.asarray(vec_sent1s_lsi)\n",
    "vec_sent2s_lsi = np.asarray(vec_sent2s_lsi)\n",
    "\n",
    "vec_sent1_lsi = vec_sent1s_lsi[...,1]\n",
    "vec_sent2_lsi = vec_sent2s_lsi[...,1]\n",
    "\n",
    "cosine_similarity(vec_sent1s_lsi.reshape(1,-1),vec_sent2s_lsi.reshape(1,-1))[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scipy Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.708813408072\n",
      "2.0497551434e-09\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import cosine as cosine_scipy\n",
    "\n",
    "print(cosine_scipy(vec_sent1_lsi.reshape(1,-1),vec_sent2_lsi.reshape(1,-1)))\n",
    "print(cosine_scipy(vec_sent1s_lsi.reshape(1,-1),vec_sent2s_lsi.reshape(1,-1))) #Filtering stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gensim LSI sentence similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.matutils import kullback_leibler, jaccard, hellinger, cossim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.62991053090495497"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_sent1_lsi = np.asarray(lsi[vec_sent1])\n",
    "vec_sent2_lsi = np.asarray(lsi[vec_sent2])\n",
    "cossim(vec_sent1_lsi,vec_sent2_lsi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.29118659192809249"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Filtering stopwords\n",
    "sent1s = ['girl','run','hall']\n",
    "sent2s = ['Alice','run','hall']\n",
    "vec_sent1s_lsi = lsi[dictionary.doc2bow(sent1s)]\n",
    "vec_sent2s_lsi = lsi[dictionary.doc2bow(sent2s)]\n",
    "cossim(vec_sent1s_lsi,vec_sent2s_lsi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abelm/flask/lib/python3.5/site-packages/gensim/matutils.py:535: RuntimeWarning: invalid value encountered in sqrt\n",
      "  sim = np.sqrt(0.5 * sum((np.sqrt(value) - np.sqrt(vec2.get(index, 0.0)))**2 for index, value in iteritems(vec1)))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hellinger(vec_sent1_lsi,vec_sent2_lsi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "inf"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kullback_leibler(vec_sent1_lsi,vec_sent2_lsi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3.4701464250118317"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jaccard(vec_sent1_lsi,vec_sent2_lsi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Pair Word Overlap\n",
    "\n",
    "Lets try a different way to compound a sentence similarity, based on WordNet-Augmented-Word-Overlap similarity idea.\n",
    "\n",
    "$p = {\\sum_{w\\in\\ sent_1}max(df[w][w']) \\over len(sent_1)} \\ \\ \\ \\forall\\ w' \\in\\ sent_2$\n",
    "\n",
    "$q = {\\sum_{w'\\in\\ sent_2}max(df[w][w']) \\over len(sent_2)} \\ \\ \\ \\forall\\ w \\in\\ sent_1$\n",
    "\n",
    "$sim = \\left\\{ \\begin{array}{rcl} \n",
    "0  & if\\ p+q = 0\\\\\n",
    "{2 p*q \\over (p+q)}  & others\\\\\n",
    "\\end{array}\n",
    "\\right.$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.60306345]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent1 = ['the','girl','run','into','the','hall']\n",
    "sent2 = ['Here','Alice','run','to','the','hall']\n",
    "\n",
    "def harmonic_best_pair_word_sim(sent1,sent2, lsi):\n",
    "    p=0\n",
    "    for wi in sent1:\n",
    "        m = 0\n",
    "        winp = np.asarray(lsi[dictionary.doc2bow([wi])])[...,1].reshape(1,-1)\n",
    "        for wc in sent2:\n",
    "            wcnp = np.asarray(lsi[dictionary.doc2bow([wc])])[...,1].reshape(1,-1)\n",
    "            m = max(m, cosine_similarity(winp,wcnp))\n",
    "        p += m\n",
    "    p = p/len(sent1)\n",
    "\n",
    "    q=0\n",
    "    for wc in sent2:\n",
    "        m = 0\n",
    "        wcnp = np.asarray(lsi[dictionary.doc2bow([wc])])[...,1].reshape(1,-1)\n",
    "        for wi in sent1:\n",
    "            winp = np.asarray(lsi[dictionary.doc2bow([wi])])[...,1].reshape(1,-1)\n",
    "            m = max(m, cosine_similarity(winp,wcnp))\n",
    "        q += m\n",
    "    q = q/len(sent2)\n",
    "\n",
    "    sim = 2*p*q/(p+q or 1)\n",
    "    return sim\n",
    "\n",
    "harmonic_best_pair_word_sim(sent1,sent2, lsi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.25150628]]\n",
      "[[ 0.70268849]]\n"
     ]
    }
   ],
   "source": [
    "#If we change the sent1 by a very different meaning sent3\n",
    "sent3 = ['the','boy','eat','a','red','apple']\n",
    "\n",
    "print(harmonic_best_pair_word_sim(sent3,sent2,lsi))\n",
    "\n",
    "#With stopword filtering\n",
    "print(harmonic_best_pair_word_sim(sent1,sent2,lsi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "* As you can test the LSI is generated fast, because parallel computing is intrinsic on Gensim implementation.\n",
    "* LSI generate a kind of bow vector because works with topic vectors, then generate an array made by topic_id,value.\n",
    "* It is astounding that the cosine distance with stopword filtered change a lot (in a good manner) compared with the same sentence with stopwords.\n",
    "\n",
    "As you can self analyze Gensim and Sklearn cosine have the same result.\n",
    "\n",
    "# Recomendations\n",
    "\n",
    "* Made the same example with Wikipedia dump data, to test the similarity difference according to data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GlobVe Model\n",
    "\n",
    "*Glove+Gensim software example.*\n",
    "\n",
    "**Install:**\n",
    "pip install glove_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glove import Corpus, Glove\n",
    "import gensim\n",
    "from six import iteritems\n",
    "from gensim.corpora import TextCorpus, MmCorpus, Dictionary\n",
    "import time\n",
    "import nltk\n",
    "\n",
    "#Wiki corpus path\n",
    "corpus_path = '/media/DATA/wiki_es/'\n",
    "wiki_corpus = corpus_path+'dump/eswiki-20161201-pages-articles-multistream.xml.bz2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrangling Data \n",
    "\n",
    "From Wikipedia dump to glove.Corpus object.\n",
    "\n",
    "This process take many hours as you can read in *~/gensim/scripts/make_wikicorpus.py* docstring. The wole process of generating .mm corpus from the dump of wikipedia takes 10 ours or more in a powerfull machine. The next cells show how to generate de Glove model from the dump of wikipedia, and in the next session this will be done with .mm corpus obtained with Gensim. To run a simple example look at GlobVe notebook made with Gutenberg raw text corpus from NLTK library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_wikipedia_corpus(filename, dictionary, article_min_tokens, article_max_tokens):\n",
    "\n",
    "    # We don't want to do a dictionary construction step.\n",
    "    corpus = gensim.corpora.WikiCorpus(filename, \n",
    "                                       dictionary=dictionary,\n",
    "                                       article_min_tokens=article_min_tokens,\n",
    "                                       article_max_tokens=article_max_tokens,\n",
    "                                       lemmatize=None)\n",
    "\n",
    "    for text in corpus.get_texts():\n",
    "        yield text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Text does not contain num_docs on the first line.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1521606059.1279578\n",
      "----------- 1000\n",
      "1521606072.001308\n",
      "----------- 2000\n",
      "1521606094.1524615\n",
      "----------- 3000\n",
      "1521606112.4316704\n",
      "----------- 4000\n",
      "1521606126.350563\n",
      "----------- 5000\n",
      "1521606139.8059845\n",
      "----------- 6000\n",
      "1521606151.0000122\n",
      "----------- 7000\n",
      "1521606156.9325473\n",
      "----------- 8000\n",
      "1521606167.9938378\n",
      "----------- 9000\n",
      "1521606182.402235\n",
      "----------- 10000\n",
      "1521606196.7842052\n",
      "----------- 11000\n",
      "1521606211.248341\n",
      "----------- 12000\n",
      "1521606227.6211655\n",
      "----------- 13000\n",
      "1521606245.197114\n",
      "----------- 14000\n",
      "1521606262.9224286\n",
      "----------- 15000\n",
      "1521606281.8131185\n",
      "----------- 16000\n",
      "1521606300.8992255\n",
      "----------- 17000\n",
      "1521606319.0168703\n",
      "----------- 18000\n",
      "1521606333.1823945\n",
      "----------- 19000\n",
      "1521606350.817441\n",
      "----------- 20000\n",
      "1521606368.8233101\n",
      "----------- 21000\n",
      "1521606385.2184923\n",
      "----------- 22000\n",
      "1521606404.8467731\n",
      "----------- 23000\n",
      "1521606423.9464397\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-678018b5b5e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m                                        \u001b[0mdictionary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                                        \u001b[0marticle_min_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m                                        article_max_tokens=2000), window=10, )\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0minit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mcorpus_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'wiki-glove.model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/wordembd/lib/python3.5/site-packages/glove/corpus.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, corpus, window, ignore_missing)\u001b[0m\n\u001b[1;32m     62\u001b[0m                                                     \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdictionary_supplied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m                                                     \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m                                                     int(ignore_missing))\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mglove/corpus_cython.pyx\u001b[0m in \u001b[0;36mglove.corpus_cython.construct_cooccurrence_matrix (glove/corpus_cython.cpp:3193)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-e69c44bedd39>\u001b[0m in \u001b[0;36mread_wikipedia_corpus\u001b[0;34m(filename, dictionary, article_min_tokens, article_max_tokens)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0;32myield\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dictionary = Dictionary.load_from_text(corpus_path+'_wordids.txt.bz2')\n",
    "\n",
    "#Test this cell in the cluster, to very slow in a laptop\n",
    "corpus_model = Corpus()\n",
    "init = time.time()\n",
    "print(init)\n",
    "corpus_model.fit(read_wikipedia_corpus(wiki_corpus, \n",
    "                                       dictionary=dictionary, \n",
    "                                       article_min_tokens=50,\n",
    "                                       article_max_tokens=2000), window=10, )\n",
    "end = time.time() - init\n",
    "corpus_model.save(corpus_path+'wiki-glove.model')\n",
    "print('TfIdf Model Generated in %d seconds' % end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After 4 hours was impossible to get the model for the Spanish dump of wikipedia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to do this from .mm corpus? \n",
    "\n",
    "From Wikipedia.mm to a Gensim bow format corpus, an later to a text with one line per sentence (objective: iterate over lines without overload the computer memory)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Text does not contain num_docs on the first line.\n"
     ]
    }
   ],
   "source": [
    "wiki_id2word = Dictionary.load_from_text('/media/D/wiki_en/_wordids.txt.bz2')\n",
    "bow_corpus = MmCorpus('/media/D/wiki_en/_bow.mm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'murska'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print the word with index 1000 in the dictionary\n",
    "wiki_id2word[1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4181821"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bow_corpus.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing doc 0\n",
      "0.04697132110595703\n",
      "processing doc 1000000\n",
      "756.9963593482971\n",
      "processing doc 2000000\n",
      "1481.9766130447388\n",
      "processing doc 3000000\n",
      "2140.7227380275726\n",
      "processing doc 4000000\n",
      "3317.592767238617\n"
     ]
    }
   ],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('en')\n",
    "stopword_set = set(stopwords)\n",
    "init = time.time()\n",
    "#Recoveryn .mm text in a list of words by doc structure. \n",
    "with open('/media/D/wiki_en/wiki_text.txt','a') as model:\n",
    "    for i,doc in enumerate(bow_corpus):\n",
    "        wiki_text = ''\n",
    "        if i%1000000 ==0:\n",
    "            print('processing doc', i)\n",
    "            print(time.time()-init)\n",
    "\n",
    "        wiki_text = ' '.join([wiki_id2word[id2w] for id2w,frec in doc if wiki_id2word[id2w] not in stopword_set])\n",
    "        model.write(wiki_text+'\\n')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_wiki_text(path, init):\n",
    "    with open(path) as doc:\n",
    "        for i,line in enumerate(doc):\n",
    "            if i%10000==0:\n",
    "                print(i, time.time()-init)\n",
    "            yield line.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.00039386749267578125\n",
      "10000 6.154743194580078\n",
      "20000 14.900368213653564\n",
      "30000 25.731306076049805\n",
      "40000 37.31264615058899\n",
      "50000 49.834702014923096\n",
      "60000 62.695298194885254\n",
      "70000 75.88489246368408\n",
      "80000 89.51990628242493\n",
      "90000 103.76476693153381\n",
      "100000 118.72444653511047\n"
     ]
    }
   ],
   "source": [
    "init = time.time()\n",
    "wiki_corpus_model = Corpus()\n",
    "wiki_corpus_model.fit(read_wiki_text('/media/D/wiki_en/wiki_text.txt', init), window=10)\n",
    "wiki_corpus_model.save('gensim_data/wiki_glove_corpus.model')\n",
    "wiki_glove = Glove(no_components=300, learning_rate=0.05)\n",
    "wiki_glove.fit(wiki_corpus_model.matrix, epochs=2, no_threads=2, verbose=True)\n",
    "wiki_glove.add_dictionary(wiki_corpus_model.dictionary)\n",
    "wiki_glove.save('gensim_data/wiki_glove_glove.model')\n",
    "print(wiki_glove.word_vectors[wiki_glove.dictionary['girl']][:10])\n",
    "end = time.time()-init\n",
    "print('Fit GloVe model, adding dict and saving took:',end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrained Models\n",
    "\n",
    "To generate some of this models are very computing expensive, so it is better to load pre-trained models to calculate the correspondent word vector.\n",
    "\n",
    "[Wikipedia Stanford Glove Corpus](http://nlp.stanford.edu/data/glove.6B.zip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "1. Like word2vec this model allows parallelism training.\n",
    "2. Like word2vec this model generates a vector for every word appearing in the corpus with length = 'no_components'.\n",
    "3. Applying cumulative sum of ndarrays it is possible to obtain a sentence vector of the same length.\n",
    "4. Then using a regular scipy, sklearn, textsim tokendist or vector similarity distance the similarity between 2 sentences is possible.\n",
    "5. Using this mechanism it is possible to reproduce some **word embedding features** used in Paraphrase Recognition task.\n",
    "6. Generate GlobVe model with English Wikipedia it is no possible in this computer (i7 8Gb RAM).\n",
    "\n",
    "# Recommendations\n",
    "\n",
    "- Test GlobVe with spanish wikipedia."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

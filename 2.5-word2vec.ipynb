{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec Text Representation using Gutenberg Corpus\n",
    "\n",
    "**Prerequisites:** Skills in tokenization with nltk, knowledge of Word2Vec Text Representation model.\n",
    "\n",
    "## Outline\n",
    "\n",
    "**Main Goal:** To practice how to create Word2Vec models with Gensim and NLTK. Then introduce how to extract information from both text representation, and finally how to measure word similarity.\n",
    "\n",
    "- Gensim Corpus Inizialization\n",
    "- Word2Vec model example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About Gensim\n",
    "\n",
    "Gensim is a Python library for *topic modelling*, *document indexing*\n",
    "and *similarity retrieval* with large corpora. Target audience is the\n",
    "*natural language processing* (NLP) and *information retrieval* (IR)\n",
    "community. [Gensim Documentation](Gensim Doc)\n",
    "\n",
    "## About NLTK\n",
    "\n",
    "Natural Language ToolKit (NLTK) is a comprehensive Python library for natural language\n",
    "processing and text analytics. Originally designed for teaching, it has been adopted in the\n",
    "industry for research and development due to its usefulness and breadth of coverage. NLTK\n",
    "is often used for rapid prototyping of text processing programs and can even be used in\n",
    "production applications. [(Perkins2014)](#Perkins2014)\n",
    "\n",
    "## What is Word2Vec?\n",
    "\n",
    "Word2vec is a group of related models that are used to produce word embeddings. These models are shallow, two-layer neural networks that are trained to reconstruct linguistic contexts of words. Word2vec takes as its input a large corpus of text and produces a vector space, typically of several hundred dimensions, with each unique word in the corpus being assigned a corresponding vector in the space. Word vectors are positioned in the vector space such that words that share common contexts in the corpus are located in close proximity to one another in the space [(Mikolov2013)](#Mikolov2013)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import nltk\n",
    "import os\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrangling Data\n",
    "\n",
    "From txt collection to a list of strings, and from string-list to a list of word-list by sentence-list.\n",
    "\n",
    "This first method to load the whole text collection is based on \"os\" module, this is only a code snippet to practice a different ways to do it. NLTK, numpy, and other libraries have it's own methods to do the same process.\n",
    "\n",
    "In this case a text structure \"sentences\" with a list of words per sentence per line is generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_collection = []\n",
    "file_path = 'gutenberg/'\n",
    "file_list = list(os.popen('ls '+ file_path).read().split('\\n'))\n",
    "for file in file_list:\n",
    "    if file:\n",
    "        with open(os.path.join(file_path,file)) as doc:\n",
    "            doc_collection.append(doc.read())\n",
    "            \n",
    "#Wrangling the data from list of doc-strings -> list of word-list by sentences\n",
    "sentences = []\n",
    "for doc in range(len(doc_collection)):\n",
    "    for sent in nltk.sent_tokenize(doc_collection[doc]):\n",
    "        sent_words = []\n",
    "        for word in nltk.word_tokenize(sent):\n",
    "            sent_words.append(word)\n",
    "        sentences.append(sent_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating the Word2Vec Model\n",
    "\n",
    "**WARNING**: gensim.models.word2vec: Each 'sentences' item should be a list of words (usually unicode strings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 58.66724157333374\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "init = time.time()\n",
    "#first build vocabulary\n",
    "w2v = Word2Vec(iter=1)\n",
    "w2v.build_vocab(sentences)\n",
    "\n",
    "#second train the model / save it / and then load it\n",
    "w2v = Word2Vec(sentences, min_count=1, size=300)\n",
    "w2v.save('models/w2v_model')\n",
    "w2v = gensim.models.Word2Vec.load('gensim_data/w2v_model')\n",
    "\n",
    "#third train the model with more sentences\n",
    "w2v.train(sentences,total_words=20000000,epochs=w2v.iter)\n",
    "end = time.time()-init\n",
    "print('Total time:', end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Mock', 0.49112191796302795),\n",
       " ('Bell', 0.48581546545028687),\n",
       " ('Rosamond', 0.4763166904449463),\n",
       " ('eagerly', 0.4708961248397827),\n",
       " ('cautiously', 0.46099987626075745),\n",
       " ('sharply', 0.46045875549316406),\n",
       " ('impatiently', 0.4462878704071045),\n",
       " ('Billy', 0.4441705346107483),\n",
       " ('aloud', 0.44107112288475037),\n",
       " ('gravely', 0.43723881244659424)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.most_similar(positive=['Alice'],negative=['man'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.29933944, -0.35046065, -0.76184052, -0.22597112,  0.10589588,\n",
       "        0.55989736,  0.60162735,  0.14057656, -0.27160674,  0.77934808], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.wv['Alice'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sklearn Word2Vec-Cosine sentence similarity\n",
    "\n",
    "### Wrangling Data\n",
    "\n",
    "From string-sentences to \"Continue Bag of Word\" numerical vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = 'the girl run into the hall'\n",
    "sentence2 = 'Here Alice run to the hall'\n",
    "\n",
    "sent1 = sentence1.split()\n",
    "sent2 = sentence2.split()\n",
    "\n",
    "sent1s = 'girl run hall'\n",
    "sent2s = 'Alice run hall'\n",
    "\n",
    "sent1sl = sent1s.split()\n",
    "sent2sl = sent2s.split()\n",
    "\n",
    "#If we change the sent1 by a very different meaning sent3\n",
    "sent3 = ['the','boy','eat','a','red','apple']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def preproc_data(sentence1, sentence2, model):\n",
    "    \n",
    "    w2v_sent1 = []\n",
    "    w2v_sent2 = []\n",
    "\n",
    "    for word in sent1:\n",
    "        try:\n",
    "            w2v_sent1.append(w2v.wv[word])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    for word in sent2:\n",
    "        try:\n",
    "            w2v_sent2.append(w2v.wv[word])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    w2v_sent1 = sum(np.asarray(w2v_sent1))\n",
    "    w2v_sent2 = sum(np.asarray(w2v_sent2))\n",
    "    A = w2v_sent1.reshape(1,-1)\n",
    "    B = w2v_sent2.reshape(1,-1)\n",
    "    \n",
    "    return A,B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.6003623 , -0.16434795,  1.7552073 ,  0.27997163, -5.97910786,\n",
       "       -0.77644032, -5.9618144 , -5.70739174,  3.43792582, -1.5847491 ], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_sent1, w2v_sent2 = preproc_data(sentence1,sentence2,w2v)\n",
    "print(len(w2v_sent1[0]))\n",
    "w2v_sent2[0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.93066299"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cosine_similarity(w2v_sent1,w2v_sent2)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.87735528"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Filtering stopwords\n",
    "w2v_sent1s, w2v_sent2s = preproc_data(sent1s,sent2s,w2v)\n",
    "cosine_similarity(w2v_sent1s,w2v_sent2s)[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scipy Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0693369839679\n",
      "0.122644664832\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import cosine as cosine_scipy\n",
    "\n",
    "print(cosine_scipy(w2v_sent1,w2v_sent2))\n",
    "print(cosine_scipy(w2v_sent1s,w2v_sent2s)) #Filtering stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine using Gensim w2v of a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_sent1 = w2v.wv[sent1]\n",
    "vec_sent2 = w2v.wv[['corriÃ³','al','hueco']]\n",
    "\n",
    "#cosine(vec_sent1,vec_sent2)\n",
    "vec_sent1_ = vec_sent1.sum(axis=0)\n",
    "vec_sent2_ = vec_sent2.sum(axis=0)\n",
    "\n",
    "1-cosine_scipy(vec_sent1_,vec_sent2_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gensim w2v.n_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.72427952777680515"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.n_similarity(['the','girl','run','into','the','hall'],['Here','Alice','run','to','the','hall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.76351450038282376"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.n_similarity(['girl','run','hall'],['Alice','run','hall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48189778837229824"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.n_similarity(['the','boy','eat','a','red','apple'],\n",
    "                   ['Here','Alice','run','to','the','hall'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gensim w2v.similarity\n",
    "\n",
    "A score constructed with this method based on an international article.[John2016](#John2016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between Alice and girl: 0.763514499882\n"
     ]
    }
   ],
   "source": [
    "# get similarity between 2 words with word2vec\n",
    "print('Similarity between Alice and girl:', w2v.similarity('Alice','girl'))\n",
    "\n",
    "# to get similarity betwee 2 sentences with word2vec create it like John2016, ALPHA=0.25\n",
    "\n",
    "#To test if sent1 == sent3, change sentence2 by sentence3 in the 2nd loop\n",
    "#sentence3 = ['the','girl','run','into','the','hall']\n",
    "\n",
    "def sent_sim_jonh2016(sent1, sent2, model):\n",
    "    \"\"\"type sent1,sent2: list of strings\"\"\"\n",
    "    \n",
    "    sim_vector = []\n",
    "    ALPHA = 0.25\n",
    "\n",
    "    for wordA in sent1:\n",
    "        for wordB in sent2:\n",
    "            try:\n",
    "                sim = w2v.similarity(wordA,wordB)\n",
    "                if sim > ALPHA:\n",
    "                    sim_vector.append(sim)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    return sum(sim_vector)/len(sim_vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence w2v.similarity with stopwords 0.951933983071\n",
      "Sentence w2v.similarity with stopwords 0.763514464125\n"
     ]
    }
   ],
   "source": [
    "print('Sentence w2v.similarity with stopwords', word_vector_cosine_sim(sent1,sent2, w2v))\n",
    "print('Sentence w2v.similarity without stopwords', word_vector_cosine_sim(sent1sl,sent2sl, w2v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Pair Word Overlap\n",
    "\n",
    "Lets try a different way to compound a sentence similarity, based on WordNet-Augmented-Word-Overlap similarity idea.\n",
    "\n",
    "$p = {\\sum_{w\\in\\ sent_1}max(df[w][w']) \\over len(sent_1)} \\ \\ \\ \\forall\\ w' \\in\\ sent_2$\n",
    "\n",
    "$q = {\\sum_{w'\\in\\ sent_2}max(df[w][w']) \\over len(sent_2)} \\ \\ \\ \\forall\\ w \\in\\ sent_1$\n",
    "\n",
    "$sim = \\left\\{ \\begin{array}{rcl} \n",
    "0  & if\\ p+q = 0\\\\\n",
    "{2 p*q \\over (p+q)}  & others\\\\\n",
    "\\end{array}\n",
    "\\right.$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sent1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-e7f04c5eee9c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mharmonic_best_pair_word_sim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msent2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2v\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'sent1' is not defined"
     ]
    }
   ],
   "source": [
    "def harmonic_best_pair_word_sim(sent1,sent2, w2v):\n",
    "    p=0\n",
    "    for wi in sent1:\n",
    "        m = 0\n",
    "        for wc in sent2:\n",
    "            m = max(m, w2v.similarity(wi,wc))\n",
    "        p += m\n",
    "    p = p/len(sent1)\n",
    "\n",
    "    q=0\n",
    "    for wc in sent2:\n",
    "        m = 0\n",
    "        for wi in sent1:\n",
    "            m = max(m, w2v.similarity(wi,wc))\n",
    "        q += m\n",
    "    q = q/len(sent2)\n",
    "\n",
    "    sim = 2*p*q/(p+q or 1)\n",
    "    return sim\n",
    "\n",
    "harmonic_best_pair_word_sim(sent1,sent2, w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'harmonic_best_pair_word_sim' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ecb55cf3b776>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Sentence w2v_harmonic_best_pair_word with stopwords'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mharmonic_best_pair_word_sim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msent2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw2v\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Sentence w2v_harmonic_best_pair_word without stopwords'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mharmonic_best_pair_word_sim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msent2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw2v\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Different sentence w2v_harmonic_best_pair_word similarity'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mharmonic_best_pair_word_sim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msent2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw2v\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'harmonic_best_pair_word_sim' is not defined"
     ]
    }
   ],
   "source": [
    "print('Sentence w2v_harmonic_best_pair_word with stopwords', harmonic_best_pair_word_sim(sent1,sent2,w2v))\n",
    "print('Sentence w2v_harmonic_best_pair_word without stopwords',harmonic_best_pair_word_sim(sent1,sent2,w2v))\n",
    "print('Different sentence w2v_harmonic_best_pair_word similarity', harmonic_best_pair_word_sim(sent3,sent2,w2v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gensim TfIdf-Hellinger sentence similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abelm/flask/lib/python3.5/site-packages/gensim/matutils.py:538: RuntimeWarning: invalid value encountered in sqrt\n",
      "  sim = np.sqrt(0.5 * ((np.sqrt(vec1) - np.sqrt(vec2))**2).sum())\n"
     ]
    }
   ],
   "source": [
    "from gensim.matutils import kullback_leibler, jaccard, hellinger, cossim\n",
    "\n",
    "print(hellinger(w2v_sent1,w2v_sent2))\n",
    "print(kullback_leibler(w2v_sent1, w2v_sent2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "- Word2vec seems to have a sparcity problem or in this notebook the amount of texts are not enough to have good results.\n",
    "- The best similarities using this text representation models must be implemented with innovatives ideas.\n",
    "- The original gensim accuracy test output is different to this one.\n",
    "\n",
    "# Recomendations\n",
    "\n",
    "- See the notebooks training Gensim models with Wikipedia dump, review gensim distances and distances trated here.\n",
    "- Try to test other text representation models like Weigthed Matrix Factorization to study if the problem of sparcity persist.\n",
    "- Try to train w2v model with more documents and test the Best-Pair word overlap similarity.\n",
    "- n_symilarity and similarity methods seems to re-train when you execute the related notebook cells, and the similarity values changed util arrive to 0.999. Read and test this with more documents.\n",
    "\n",
    "<a id='referencias'></a>\n",
    "# Referencias\n",
    "\n",
    "<a id='Perkins2014'></a>\n",
    "[1] *[Perkins2014]* Jacov Perkins. \n",
    "Book **Python 3 Text Processing with NLTK 3 Cookbook**. 2014. \n",
    "p. 7 **ISBN**: 978-1-78216-785-3\n",
    "\n",
    "<a id='Mikolov2013'></a>\n",
    "[2] *[Mikolov2013]* Tomas Mikolov et al. **Efficient Estimation of Word Representations in Vector Space**. Publisher [arXiv](https://arxiv.org/abs/1301.3781), 2013.\n",
    "\n",
    "<a id='John2016'></a>\n",
    "[3] *[John2016]* John, Adebayo Kolawole and Caro, Luigi Di and Boella, Guido. **NORMAS at SemEval-2016 Task 1: SEMSIM: A Multi-Feature Approach to Semantic Text Similarity**. Publisher ACM, 2016."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

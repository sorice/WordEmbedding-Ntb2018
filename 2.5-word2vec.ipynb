{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec Text Representation using Gutenberg Corpus\n",
    "\n",
    "**Prerequisites:** Skills in tokenization with nltk, knowledge of Word2Vec Text Representation model.\n",
    "\n",
    "## Outline\n",
    "\n",
    "**Main Goal:** To practice how to create Word2Vec models with Gensim and NLTK. Then introduce how to extract information from both text representation, and finally how to measure word similarity.\n",
    "\n",
    "- Gensim Corpus Inizialization\n",
    "- Word2Vec model example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About Gensim\n",
    "\n",
    "Gensim is a Python library for *topic modelling*, *document indexing*\n",
    "and *similarity retrieval* with large corpora. Target audience is the\n",
    "*natural language processing* (NLP) and *information retrieval* (IR)\n",
    "community. [Gensim Documentation](Gensim Doc)\n",
    "\n",
    "## About NLTK\n",
    "\n",
    "Natural Language ToolKit (NLTK) is a comprehensive Python library for natural language\n",
    "processing and text analytics. Originally designed for teaching, it has been adopted in the\n",
    "industry for research and development due to its usefulness and breadth of coverage. NLTK\n",
    "is often used for rapid prototyping of text processing programs and can even be used in\n",
    "production applications. [(Perkins2014)](#Perkins2014)\n",
    "\n",
    "## What is Word2Vec?\n",
    "\n",
    "Word2vec is a group of related models that are used to produce word embeddings. These models are shallow, two-layer neural networks that are trained to reconstruct linguistic contexts of words. Word2vec takes as its input a large corpus of text and produces a vector space, typically of several hundred dimensions, with each unique word in the corpus being assigned a corresponding vector in the space. Word vectors are positioned in the vector space such that words that share common contexts in the corpus are located in close proximity to one another in the space [(Mikolov2013)](#Mikolov2013)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import nltk\n",
    "import os\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrangling Data\n",
    "\n",
    "From txt collection to a list of strings, and from string-list to a list of word-list by sentence-list.\n",
    "\n",
    "This first method to load the whole text collection is based on \"os\" module, this is only a code snippet to practice a different ways to do it. NLTK, numpy, and other libraries have it's own methods to do the same process.\n",
    "\n",
    "In this case a text structure \"sentences\" with a list of words per sentence per line is generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_collection = []\n",
    "file_path = 'gutenberg/'\n",
    "file_list = list(os.popen('ls '+ file_path).read().split('\\n'))\n",
    "for file in file_list:\n",
    "    if file:\n",
    "        with open(os.path.join(file_path,file)) as doc:\n",
    "            doc_collection.append(doc.read())\n",
    "            \n",
    "#Wrangling the data from list of doc-strings -> list of word-list by sentences\n",
    "sentences = []\n",
    "for doc in range(len(doc_collection)):\n",
    "    for sent in nltk.sent_tokenize(doc_collection[doc]):\n",
    "        sent_words = []\n",
    "        for word in nltk.word_tokenize(sent):\n",
    "            sent_words.append(word)\n",
    "        sentences.append(sent_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating the Word2Vec Model\n",
    "\n",
    "**WARNING**: gensim.models.word2vec: Each 'sentences' item should be a list of words (usually unicode strings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec Model Generated in 59 seconds\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "try:\n",
    "    w2v = Word2Vec.load('models/gutenberg_w2v.model')\n",
    "    print('Word2Vec Model Generated in 59 seconds')\n",
    "except:\n",
    "    print('No puedo entrar aquÃ­')\n",
    "    init = time.time()\n",
    "    #first build vocabulary\n",
    "    w2v = Word2Vec(iter=1)\n",
    "    w2v.build_vocab(sentences)\n",
    "\n",
    "    #second train the model / save it / and then load it\n",
    "    w2v = Word2Vec(sentences, min_count=1, size=300)\n",
    "    w2v.save('models/gutenberg_w2v.model')\n",
    "    w2v = gensim.models.Word2Vec.load('gensim_data/w2v_model')\n",
    "\n",
    "    #third train the model with more sentences\n",
    "    w2v.train(sentences,total_words=20000000,epochs=w2v.iter)\n",
    "    end = time.time()-init\n",
    "    print('Total time:', end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Tenderly', 0.5983538627624512),\n",
       " ('`Uncle', 0.581244945526123),\n",
       " ('1788', 0.5553467273712158),\n",
       " (\"'Leonora\", 0.5489853620529175),\n",
       " ('unsays', 0.5169966220855713),\n",
       " ('wilted', 0.5126117467880249),\n",
       " ('promiscuously', 0.5119398236274719),\n",
       " ('politely', 0.5090441703796387),\n",
       " ('submissively', 0.5081309676170349),\n",
       " ('ago.', 0.5049775838851929)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.wv.most_similar(positive=['Alice'],negative=['man'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.4553154 ,  0.3860206 , -0.0982976 ,  0.45534748, -0.3113477 ,\n",
       "       -0.16704908,  0.48424447, -0.3274756 ,  0.08231983, -0.1183285 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.wv['Alice'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sklearn Word2Vec-Cosine sentence similarity\n",
    "\n",
    "### Wrangling Data\n",
    "\n",
    "From string-sentences to \"Continue Bag of Word\" numerical vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = 'the girl run into the hole'\n",
    "sentence2 = 'Here Alice run to the hole'\n",
    "\n",
    "sent1 = sentence1.split()\n",
    "sent2 = sentence2.split()\n",
    "\n",
    "sent1s = 'girl run hole'\n",
    "sent2s = 'Alice run hole'\n",
    "\n",
    "sent1sl = sent1s.split()\n",
    "sent2sl = sent2s.split()\n",
    "\n",
    "#If we change the sent1 by a very different meaning sent3\n",
    "sent3 = ['the','boy','eat','a','red','apple']\n",
    "sent3s = ['boy','eat','red','apple']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def preproc_data(sent, model):\n",
    "    \n",
    "    vec_sent = []\n",
    "\n",
    "    for i,word in enumerate(sent):\n",
    "        try:\n",
    "            vec_sent.append(model.wv[word])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    vec_sent = sum(np.asarray(vec_sent))\n",
    "    result = vec_sent.reshape(1,-1)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.8443149 , -0.04467377, -0.11465675,  1.6633564 ,  1.9606867 ,\n",
       "       -0.5129897 , -1.0194454 , -0.8266785 ,  1.5195833 , -0.21492743],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_sent1 = preproc_data(sent1,w2v)\n",
    "w2v_sent2 = preproc_data(sent2,w2v)\n",
    "print(len(w2v_sent1[0]))\n",
    "w2v_sent2[0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8332392"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cosine_similarity(w2v_sent1,w2v_sent2)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8332392"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Filtering stopwords\n",
    "w2v_sent1s, w2v_sent2s = preproc_data(sent1s,sent2s,w2v)\n",
    "cosine_similarity(w2v_sent1s,w2v_sent2s)[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scipy Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1667608618736267\n",
      "0.1667608618736267\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import cosine as cosine_scipy\n",
    "\n",
    "print(cosine_scipy(w2v_sent1,w2v_sent2))\n",
    "print(cosine_scipy(w2v_sent1s,w2v_sent2s)) #Filtering stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gensim Particular Measures\n",
    "\n",
    "Gensim jaccard and cosine are impossible to measure because the p2v_bow vector is needed, but not exist.\n",
    "\n",
    "## Cosine using Gensim w2v of a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w2v sentence vector similarity without transformation 0.17152277\n",
      "w2v sentence vector similarity with transformation 0.82981557\n"
     ]
    }
   ],
   "source": [
    "vec_sent1 = w2v.wv[sent1]\n",
    "vec_sent2 = w2v.wv[sent2]\n",
    "\n",
    "vec_sent1_ = vec_sent1.sum(axis=0).reshape(1,-1)\n",
    "vec_sent2_ = vec_sent2.sum(axis=0).reshape(1,-1)\n",
    "\n",
    "print('w2v sentence vector similarity without transformation',\n",
    "      cosine_similarity(vec_sent1,vec_sent2)[0][0])\n",
    "print('w2v sentence vector similarity with transformation',\n",
    "      cosine_similarity(vec_sent1_,vec_sent2_)[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gensim w2v.n_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.737802905397651\n",
      "0.829815479780645\n",
      "0.8406460034906343\n"
     ]
    }
   ],
   "source": [
    "print(w2v.wv.n_similarity(sent3,sent2))\n",
    "print(w2v.wv.n_similarity(sent1,sent2))\n",
    "print(w2v.wv.n_similarity(sent1sl,sent2sl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gensim w2v.similarity\n",
    "\n",
    "A score constructed with this method based on an international article.[John2016](#John2016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7787217808243306"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.wv.similarity('woman','man')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_sim_jonh2016(sent1, sent2, model):\n",
    "    \"\"\"type sent1,sent2: list of strings\"\"\"\n",
    "    \n",
    "    sim_vector = []\n",
    "    ALPHA = 0.25\n",
    "\n",
    "    for wordA in sent1:\n",
    "        for wordB in sent2:\n",
    "            try:\n",
    "                sim = w2v.wv.similarity(wordA,wordB)\n",
    "                if sim > ALPHA:\n",
    "                    sim_vector.append(sim)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    return sum(sim_vector)/(len(sim_vector) or 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar sentences w2v.similarity 0.5067870774528688\n",
      "Similar sentences w2v.similarity without stopwords 0.627630266069921\n"
     ]
    }
   ],
   "source": [
    "print('Similar sentences w2v.similarity', sent_sim_jonh2016(sent1,sent2, w2v))\n",
    "print('Similar sentences w2v.similarity without stopwords', sent_sim_jonh2016(sent1sl,sent2sl, w2v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gensim TfIdf-Hellinger sentence similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abelma/wordembd/lib/python3.5/site-packages/gensim/matutils.py:906: RuntimeWarning: invalid value encountered in sqrt\n",
      "  sim = np.sqrt(0.5 * ((np.sqrt(vec1) - np.sqrt(vec2))**2).sum())\n"
     ]
    }
   ],
   "source": [
    "from gensim.matutils import kullback_leibler, jaccard, hellinger, cossim\n",
    "\n",
    "print(hellinger(w2v_sent1,w2v_sent2))\n",
    "print(kullback_leibler(w2v_sent1, w2v_sent2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Pair Word Overlap\n",
    "\n",
    "Lets try a different way to compound a sentence similarity, based on WordNet-Augmented-Word-Overlap similarity idea.\n",
    "\n",
    "$p = {\\sum_{w\\in\\ sent_1}max(df[w][w']) \\over len(sent_1)} \\ \\ \\ \\forall\\ w' \\in\\ sent_2$\n",
    "\n",
    "$q = {\\sum_{w'\\in\\ sent_2}max(df[w][w']) \\over len(sent_2)} \\ \\ \\ \\forall\\ w \\in\\ sent_1$\n",
    "\n",
    "$sim = \\left\\{ \\begin{array}{rcl} \n",
    "0  & if\\ p+q = 0\\\\\n",
    "{2 p*q \\over (p+q)}  & others\\\\\n",
    "\\end{array}\n",
    "\\right.$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def harmonic_best_pair_word_sim(sent1,sent2,model):\n",
    "    p=0\n",
    "    for wordA in sent1:\n",
    "        m = 0\n",
    "        for wordB in sent2:\n",
    "            try:\n",
    "                m = max(m, model.wv.similarity(wordA,wordB))\n",
    "            except:\n",
    "                pass\n",
    "        p += m\n",
    "    p = p/len(sent1)\n",
    "\n",
    "    q=0\n",
    "    for wordA in sent2:\n",
    "        m = 0\n",
    "        for wordB in sent1:\n",
    "            try:\n",
    "                m = max(m, model.wv.similarity(wordA,wordB))\n",
    "            except:\n",
    "                pass\n",
    "        q += m\n",
    "    q = q/len(sent2)\n",
    "\n",
    "    sim = 2*p*q/(p+q or 1)\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dissimilar sentences w2v_harmonic_best_pair_word similarity 0.5880608452662112\n",
      "Dissimilar sentences without stopwords w2v_harmonic_best_pair_word similarity 0.34912268984175787\n",
      "Similar sentences w2v_harmonic_best_pair_word 0.7593893198382271\n",
      "Similar sentences w2v_harmonic_best_pair_word without stopwords 0.8217402173991791\n"
     ]
    }
   ],
   "source": [
    "print('Dissimilar sentences w2v_harmonic_best_pair_word similarity', \n",
    "      harmonic_best_pair_word_sim(sent3,sent2,w2v))\n",
    "print('Dissimilar sentences without stopwords w2v_harmonic_best_pair_word similarity',\n",
    "      harmonic_best_pair_word_sim(sent3s,sent2s,w2v))\n",
    "print('Similar sentences w2v_harmonic_best_pair_word', \n",
    "      harmonic_best_pair_word_sim(sent1,sent2,w2v))\n",
    "print('Similar sentences w2v_harmonic_best_pair_word without stopwords',\n",
    "      harmonic_best_pair_word_sim(sent1sl,sent2sl,w2v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "- The best similarities using this text representation models must be implemented with innovatives ideas. For example: ``sent_sim_jonh2016`` and ``harmonic_best_pair_word_sim``.\n",
    "- In almost all cases the stopword filtering increment the similarity between similar sentences and diminished similarity between different sentences.\n",
    "- Gensim Hellinger and Kullback Leibler still been useless.\n",
    "\n",
    "# Recomendations\n",
    "\n",
    "- See the notebooks training Gensim models with Wikipedia dump, review gensim distances and distances trated here.\n",
    "- Try to test other text representation models like Weigthed Matrix Factorization.\n",
    "- Try to train w2v model with more documents and test the Best-Pair word overlap similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='references'></a>\n",
    "# References\n",
    "\n",
    "<a id='Perkins2014'></a>\n",
    "[1] *[Perkins2014]* Jacov Perkins. \n",
    "Book **Python 3 Text Processing with NLTK 3 Cookbook**. 2014. \n",
    "p. 7 **ISBN**: 978-1-78216-785-3\n",
    "\n",
    "<a id='Mikolov2013'></a>\n",
    "[2] *[Mikolov2013]* Tomas Mikolov et al. **Efficient Estimation of Word Representations in Vector Space**. Publisher [arXiv](https://arxiv.org/abs/1301.3781), 2013.\n",
    "\n",
    "<a id='John2016'></a>\n",
    "[3] *[John2016]* John, Adebayo Kolawole and Caro, Luigi Di and Boella, Guido. **NORMAS at SemEval-2016 Task 1: SEMSIM: A Multi-Feature Approach to Semantic Text Similarity**. Publisher ACM, 2016."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

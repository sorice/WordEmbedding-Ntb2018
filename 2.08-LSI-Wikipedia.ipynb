{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSI Text Representation using Wikipedia Corpus\n",
    "\n",
    "*Gensim software example.*\n",
    "\n",
    "**Prerequisites:** Skills in tokenization with nltk, knowledge of LSI Text Representation model.\n",
    "\n",
    "## Outline\n",
    "\n",
    "**Main Goal:** To practice how to create the Latent Semantic Index model with Gensim and NLTK. Then introduce how to extract information from this text representation, and finally how to measure word similarity with this model.\n",
    "\n",
    "- Gensim Corpus Inizialization\n",
    "- LSI model example\n",
    "\n",
    "**Note: Sklearn LSI codes still pendent.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "from gensim.models import LsiModel, TfidfModel\n",
    "from gensim.corpora import TextCorpus, MmCorpus, Dictionary\n",
    "from gensim.models.word2vec import Text8Corpus\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Text does not contain num_docs on the first line.\n"
     ]
    }
   ],
   "source": [
    "#Change this configurations with your paths to Wiki corpus\n",
    "corpus_path = '/media/DATA/wiki_es/'\n",
    "#Loading resources generated priviously with Gensim package\n",
    "dictionary = Dictionary.load_from_text(corpus_path+'_wordids.txt.bz2')\n",
    "bow_corpus = MmCorpus(corpus_path+'_bow.mm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating the LSI Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfIdf Model Generated in 6329 seconds\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    lsi =LsiModel.load(corpus_path+'wiki-lsi.model',mmap='r')\n",
    "    print('TfIdf Model Generated in 6329 seconds')\n",
    "except:\n",
    "    init = time.time()\n",
    "    lsi = LsiModel(bow_corpus,id2word=dictionary,num_topics=300)\n",
    "    end = time.time()-init\n",
    "    lsi.save(corpus_path+'wiki-lsi.model')\n",
    "    print('Total time:', end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.000714352979674119),\n",
       " (1, 0.0007150450110253348),\n",
       " (2, 0.00034488263827122745),\n",
       " (3, 0.0036861730666044525),\n",
       " (4, 0.002268878648457448),\n",
       " (5, -0.00021203173328911284),\n",
       " (6, 0.0008200432054711397),\n",
       " (7, -0.001305789733660302),\n",
       " (8, -0.0007860860529528039),\n",
       " (9, -0.00043901833517678027)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsi[dictionary.doc2bow(['run'])][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrangling Data for Similarity Measures\n",
    "\n",
    "From txt collection to a list of strings, and from string-list to a list of word-list by sentence-list.\n",
    "\n",
    "From string-sentences to numerical vectors.\n",
    "\n",
    "In this model the preprocessing is not needed, because if we create de bow vectors of the sentences the LSI model can handle the numerical vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = 'la niña corrió hacia el hueco'\n",
    "sentence2 = 'Alicia corrió hacia el hueco'\n",
    "sent1 = sentence1.split() #sentence in list of words format\n",
    "sent2 = sentence2.split()\n",
    "#Filtering stopwords by hand\n",
    "sent1s = 'niña corrió hueco'\n",
    "sent2s = 'Alicia corrió hueco'\n",
    "sent1sl = sent1s.split()\n",
    "sent2sl = sent2s.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_sent1 = dictionary.doc2bow(sent1)\n",
    "vec_sent2 = dictionary.doc2bow(sent2)\n",
    "\n",
    "bowvec_sent1_lsi = np.asarray(lsi[vec_sent1])\n",
    "bowvec_sent2_lsi = np.asarray(lsi[vec_sent2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.00000000e+00, 2.12801488e-03],\n",
       "       [1.00000000e+00, 2.02432313e-03],\n",
       "       [2.00000000e+00, 3.05863651e-04],\n",
       "       [3.00000000e+00, 2.14804254e-03],\n",
       "       [4.00000000e+00, 1.78819965e-04],\n",
       "       [5.00000000e+00, 5.01868029e-04],\n",
       "       [6.00000000e+00, 6.10981640e-04],\n",
       "       [7.00000000e+00, 1.54455533e-03],\n",
       "       [8.00000000e+00, 1.43817691e-03],\n",
       "       [9.00000000e+00, 1.66228135e-03]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(bowvec_sent1_lsi))\n",
    "bowvec_sent2_lsi[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having into account that LSI model works with topics, the generated vector have 2 components in everyone of the 300 elements that compound the numerical vector: the id of the topic, and their respective value.\n",
    "The las step in wrangling this data is to eliminate the id, writing the vector as a 1D array only of float values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_sent1_lsi = bowvec_sent1_lsi[...,1]\n",
    "vec_sent2_lsi = bowvec_sent2_lsi[...,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.00212801, 0.00202432, 0.00030586, 0.00214804, 0.00017882,\n",
       "       0.00050187, 0.00061098, 0.00154456, 0.00143818, 0.00166228])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(vec_sent1_lsi))\n",
    "vec_sent2_lsi[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying Sklearn Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_sent1_lsi.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5766073355368637"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cosine_similarity(vec_sent1_lsi.reshape(1,-1),vec_sent2_lsi.reshape(1,-1))[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5766073355368637"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Filtering stopwords\n",
    "vec_sent1s_lsi = lsi[dictionary.doc2bow(sent1sl)]\n",
    "vec_sent2s_lsi = lsi[dictionary.doc2bow(sent2sl)]\n",
    "\n",
    "bowvec_sent1s_lsi = np.asarray(vec_sent1s_lsi)\n",
    "bowvec_sent2s_lsi = np.asarray(vec_sent2s_lsi)\n",
    "\n",
    "vec_sent1s_lsi = bowvec_sent1s_lsi[...,1]\n",
    "vec_sent2s_lsi = bowvec_sent2s_lsi[...,1]\n",
    "\n",
    "cosine_similarity(vec_sent1s_lsi.reshape(1,-1),vec_sent2s_lsi.reshape(1,-1))[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scipy Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.42339266446313617\n",
      "0.42339266446313617\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import cosine as cosine_scipy\n",
    "\n",
    "print(cosine_scipy(vec_sent1_lsi.reshape(1,-1),vec_sent2_lsi.reshape(1,-1)))\n",
    "print(cosine_scipy(vec_sent1s_lsi.reshape(1,-1),vec_sent2s_lsi.reshape(1,-1))) #Filtering stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gensim LSI sentence similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity: 0.5766073355368642\n",
      "Cosine similarity without stopwords 0.5766073355368642\n"
     ]
    }
   ],
   "source": [
    "from gensim.matutils import kullback_leibler, jaccard, hellinger, cossim\n",
    "print('Cosine similarity:',cossim(bowvec_sent1_lsi,bowvec_sent2_lsi))\n",
    "print('Cosine similarity without stopwords',cossim(bowvec_sent1s_lsi,bowvec_sent2s_lsi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abelma/wordembd/lib/python3.5/site-packages/gensim/matutils.py:906: RuntimeWarning: invalid value encountered in sqrt\n",
      "  sim = np.sqrt(0.5 * ((np.sqrt(vec1) - np.sqrt(vec2))**2).sum())\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hellinger(vec_sent1_lsi,vec_sent2_lsi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "inf"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kullback_leibler(vec_sent1_lsi,vec_sent2_lsi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jaccard(vec_sent1s_lsi,vec_sent2s_lsi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Pair Word Overlap\n",
    "\n",
    "Lets try a different way to compound a sentence similarity, based on WordNet-Augmented-Word-Overlap similarity idea.\n",
    "\n",
    "$p = {\\sum_{w\\in\\ sent_1}max(df[w][w']) \\over len(sent_1)} \\ \\ \\ \\forall\\ w' \\in\\ sent_2$\n",
    "\n",
    "$q = {\\sum_{w'\\in\\ sent_2}max(df[w][w']) \\over len(sent_2)} \\ \\ \\ \\forall\\ w \\in\\ sent_1$\n",
    "\n",
    "$sim = \\left\\{ \\begin{array}{rcl} \n",
    "0  & if\\ p+q = 0\\\\\n",
    "{2 p*q \\over (p+q)}  & others\\\\\n",
    "\\end{array}\n",
    "\\right.$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3636363636363637"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def harmonic_best_pair_word_sim(sent1,sent2, lsi):\n",
    "    p=0\n",
    "    for wi in sent1:\n",
    "        m = 0\n",
    "        for wc in sent2:\n",
    "            try:\n",
    "                winp = np.asarray(lsi[dictionary.doc2bow([wi])])[...,1].reshape(1,-1)\n",
    "                wcnp = np.asarray(lsi[dictionary.doc2bow([wc])])[...,1].reshape(1,-1)\n",
    "                m = max(m, cosine_similarity(winp,wcnp))\n",
    "            except:\n",
    "                pass\n",
    "        p += m\n",
    "    p = p/len(sent1)\n",
    "\n",
    "    q=0\n",
    "    for wc in sent2:\n",
    "        m = 0\n",
    "        for wi in sent1:\n",
    "            try:\n",
    "                wcnp = np.asarray(lsi[dictionary.doc2bow([wc])])[...,1].reshape(1,-1)\n",
    "                winp = np.asarray(lsi[dictionary.doc2bow([wi])])[...,1].reshape(1,-1)\n",
    "                m = max(m, cosine_similarity(winp,wcnp))\n",
    "            except:\n",
    "                pass\n",
    "        q += m\n",
    "    q = q/len(sent2)\n",
    "\n",
    "    sim = 2*p*q/(p+q or 1)\n",
    "    return sim[0][0]\n",
    "\n",
    "harmonic_best_pair_word_sim(sent1,sent2, lsi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01150097850428035\n",
      "0.3636363636363637\n"
     ]
    }
   ],
   "source": [
    "#If we change the sent1 by a very different meaning sent3\n",
    "sent3 = ['el','niño','comió','una','manzana','roja']\n",
    "\n",
    "print(harmonic_best_pair_word_sim(sent3,sent2,lsi))\n",
    "\n",
    "#With stopword filtering\n",
    "print(harmonic_best_pair_word_sim(sent1,sent2,lsi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "* As you can test the LSI is generated fast, because parallel computing is intrinsic on Gensim implementation.\n",
    "* LSI generate a kind of bow vector because works with topic vectors, then generate an array made by topic_id,value.\n",
    "* It is astounding that the cosine distance with stopword filtered change a lot (in a good manner) compared with the same sentence with stopwords.\n",
    "\n",
    "As you can self analyze Gensim and Sklearn cosine have the same result.\n",
    "\n",
    "# Recomendations\n",
    "\n",
    "* Made the same example with Wikipedia dump data, to test the similarity difference according to data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

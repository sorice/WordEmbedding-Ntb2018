{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<small><i>This notebook was put together by [Abel Meneses-Abad](http://www.menesesabad.com) for PyData 2018. Source and license info is on [GitHub](https://github.com/sorice/wordembeded/).</i></small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Introduction to Word Embedding Models in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks to Jake Vanderplas for his innumerables contributions to python world and science with python. I use part of the structure that I learn from him to made this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goals of this Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Introduce the basics of Word Embedding Text Representation models**, and some useful code snippets in practice.\n",
    "- **Apply Semantic Text Similarity using these models**, so you can see the tools availables and an example of application for this knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebooks Index\n",
    "\n",
    "The following links are to notebooks containing the tutorial materials.\n",
    "Note that many of these require files that are in the directory structure of the [github repository](https://github.com/sorice/wordembeded/) in which they are contained.\n",
    "Many resources for this tutorial must be downloaded from their original source, like some NLTK text corpus used and the wikipedia dump.\n",
    "A strong recommendation is to start studying the tutorial at home, and make the data generation from wikipedia dump, usign Gensim package. This could take a long time that you don't want to lose on real presentation.\n",
    "\n",
    "### 1. Preliminaries\n",
    "\n",
    "  + [01-Preliminaries.ipynb](01-Preliminaries.ipynb)\n",
    "  \n",
    "### 2. Word Embedding Models\n",
    "\n",
    "  + [02.1 Term Frecuency - Inverse Document Frecuency Model](02.1-TfIdf.ipynb)\n",
    "  + [02.2 TfIdf with Wikipedia Corpus Notebook](2.2-TfIdf-Wikipedia.ipynb)\n",
    "  + [02.3 GloVe Model](2.3-GloVe.ipynb)\n",
    "  + [02.4 GloVe with Wikipedia Corpus](2.4-GlobVe-Wikipedia.ipynb)\n",
    "  + [02.5 Word2Vec Model](2.5-word2vec.ipynb)\n",
    "  + [02.6 Word2Vec with Wikipedia Corpus](2.6-word2vec-Wikipedia.ipynb)\n",
    "  + [02.7 LSI Model](2.7-LSI.ipynb)\n",
    "  + [02.8 LSI with Wikipedia Corpus](2.6-word2vec-Wikipedia.ipynb)\n",
    "  + [02.9 Paragraph2Vec Model](2.9-Paragraph2Vec.ipynb)\n",
    "  + [2.10 P2V Model with Wikipedia Corpus](2.10-Paragraph2Vec-Wikipedia.ipynb)\n",
    "  \n",
    "  \n",
    "### 3. Full Example Using WordEmbedding in Paraphrase Recognition\n",
    "\n",
    "  + [3.1 Playfull Experiments with MSRP Corpus](3.1-Playfull-Experiments-with-MSRPC.ipynb)\n",
    "  \n",
    "### 4. Future Works\n",
    "\n",
    "   + [Sense2Vec Model](4.1-Sense2Vec.ipynb)\n",
    "   + [Have SVD over TF-matrix any effect in Paraphrase Recognition?](4.2-Tf_applying_SVD.ipynb)\n",
    "   + [Paragram2Vec Model]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial requires the following packages:\n",
    "\n",
    "- Python version 2.6-2.7 or 3.3-3.4\n",
    "- `jupyter` version 1.0 or later, with notebook support: http://jupyter.org\n",
    "- `notebook` version 5.4 or later, with notebook support: http://jupyter.org\n",
    "- `numpy` version 1.14 or later: http://www.numpy.org/\n",
    "- `scipy` version 1.0 or later: http://www.scipy.org/\n",
    "- `sklearn` version 0.19 or later: http://scikit-learn.org\n",
    "\n",
    "- `gensim` version 3.3 or later, with notebook support: https://radimrehurek.com/gensim/index.html\n",
    "- `nltk` version 3.2.5 or later, with notebook support: http://nltk.org\n",
    "\n",
    "The easiest way to get these is to use the [conda](http://store.continuum.io/) environment manager.\n",
    "I suggest downloading and installing [miniconda](http://conda.pydata.org/miniconda.html).\n",
    "\n",
    "The following command will install all required packages:\n",
    "```\n",
    "$ conda install numpy scipy sklearn jupyter-notebook\n",
    "```\n",
    "\n",
    "Alternatively, you can download and install with:\n",
    "```\n",
    "$ pip install numpy scipy sklearn jupyter notebook\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking your installation\n",
    "\n",
    "You can run the following code to check the versions of the packages on your system:\n",
    "\n",
    "(in IPython notebook, press `shift` and `return` together to execute the contents of a cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IPython: 1.0.0\n",
      "notebook 5.4.0\n",
      "numpy: 1.14.0\n",
      "scipy: 1.0.0\n",
      "scikit-learn: 0.19.1\n",
      "gensim: 3.3.0\n",
      "nltk: 3.2.5\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import jupyter\n",
    "print('IPython:', jupyter.__version__)\n",
    "\n",
    "import notebook\n",
    "print('notebook', notebook.__version__)\n",
    "\n",
    "import numpy\n",
    "print('numpy:', numpy.__version__)\n",
    "\n",
    "import scipy\n",
    "print('scipy:', scipy.__version__)\n",
    "\n",
    "import sklearn\n",
    "print('scikit-learn:', sklearn.__version__)\n",
    "\n",
    "import gensim\n",
    "print('gensim:', gensim.__version__)\n",
    "\n",
    "import nltk\n",
    "print('nltk:', nltk.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "For paraphrase recognition examples de Microsoft Paraphrase Corpus [(Dolan2004)](#Dolan2004) will be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Gutenberg corpus NLTK selections:** https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/gutenberg.zip \n",
    "- **scikit-learn:** http://scikit-learn.org (see especially the narrative documentation)\n",
    "- **matplotlib:** http://matplotlib.org (see especially the gallery section)\n",
    "- **IPython:** http://ipython.org (also check out http://nbviewer.ipython.org)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='References'></a>\n",
    "# References\n",
    "\n",
    "\n",
    "[1] *[Dolan2004]* Dolan, Bill & Quirk, Chris & Brockett, Chris (2004). \n",
    "**Unsupervised Construction of Large Paraphrase Corpora: Exploiting Massively Parallel News Sources.** \n",
    "Published on Proceedings of the 20th International Conference on Computational Linguistics (COLING 2004). ACM. \n",
    "<a id='Dolan2004'></a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

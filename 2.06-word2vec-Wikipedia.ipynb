{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec Text Representation using Wikipedia Corpus\n",
    "\n",
    "**Prerequisites:** Skills in tokenization with nltk, knowledge of Word2Vec Text Representation model.\n",
    "\n",
    "## Outline\n",
    "\n",
    "**Main Goal:** To practice how to create Word2Vec models with Gensim and NLTK and Wikipedia corpus.\n",
    "\n",
    "- Gensim Corpus Inizialization\n",
    "- Word2Vec model example\n",
    "\n",
    "**Note**: About Gensim and NLTK software please read the introductions notes about them in [2.01-TfIdf Notebook](2.01-TfIdf.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import nltk\n",
    "import os\n",
    "import re\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.corpora import TextCorpus, MmCorpus, Dictionary\n",
    "from gensim.models.word2vec import Text8Corpus\n",
    "import time\n",
    "\n",
    "#Wiki corpus path\n",
    "corpus_path = '/media/DATA/wiki_es/'\n",
    "wiki_corpus = corpus_path+'dump/eswiki-20161201-pages-articles-multistream.xml.bz2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrangling Data\n",
    "\n",
    "From txt collection to a list of strings, and from string-list to a list of word-list by sentence-list.\n",
    "\n",
    "Corpus.get_texts is implemented using iterators as input and not work with generators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_wikipedia_corpus(filename, dictionary, article_min_tokens, article_max_tokens):\n",
    "\n",
    "    # We don't want to do a dictionary construction step.\n",
    "    corpus = gensim.corpora.WikiCorpus(filename, \n",
    "                                       dictionary=dictionary,\n",
    "                                       article_min_tokens=article_min_tokens,\n",
    "                                       article_max_tokens=article_max_tokens,\n",
    "                                       lemmatize=None)\n",
    "\n",
    "    for text in corpus.get_texts():\n",
    "        yield text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating the Word2Vec Model\n",
    "\n",
    "**WARNING**: gensim.models.word2vec: Each 'sentences' item should be a list of words (usually unicode strings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Text does not contain num_docs on the first line.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1521730513.281354\n",
      "----------- 100000\n",
      "1521731006.004047\n",
      "----------- 200000\n",
      "1521731308.615893\n",
      "----------- 300000\n",
      "1521731567.400948\n",
      "----------- 400000\n",
      "1521731863.391605\n",
      "----------- 500000\n",
      "1521732169.979917\n",
      "----------- 600000\n",
      "1521732419.9087088\n",
      "----------- 700000\n",
      "1521732613.5474608\n",
      "----------- 800000\n",
      "1521732847.45133\n",
      "----------- 900000\n",
      "1521733104.1776474\n",
      "----------- 1000000\n",
      "1521733350.9373498\n",
      "----------- 1100000\n",
      "1521733585.289727\n",
      "Corpus of 1103059 articles, model vocabulary size = 360330 generated in 3095 seconds\n"
     ]
    }
   ],
   "source": [
    "#Building vocabulary, this step is obligated before trained the model, could take several minutes.\n",
    "dictionary = Dictionary.load_from_text(corpus_path+'_wordids.txt.bz2')\n",
    "init = time.time()\n",
    "print(init)\n",
    "\n",
    "w2v = Word2Vec(iter=1,                 #Number of iterations (epochs) over the corpus.\n",
    "               min_count=20,           #Ignores all words with total frequency lower than this.\n",
    "               size=300,               #Dimensionality of the feature vectors.\n",
    "               max_vocab_size=2000000, #Limits the RAM during vocabulary building.\n",
    "               sg=0,                   #Defines the training algorithm. If 0 skip-gram is employed.\n",
    "              )\n",
    "w2v.build_vocab(read_wikipedia_corpus(wiki_corpus, \n",
    "                                       dictionary=dictionary, \n",
    "                                       article_min_tokens=50,     #Minimum tokens in article.\n",
    "                                       article_max_tokens=5000),  #Maximum tokens in article.\n",
    "                )\n",
    "end = time.time()-init\n",
    "print('Corpus of %d articles, model vocabulary size = %d generated in %d seconds' % (w2v.corpus_count ,len(w2v.wv.vocab),end))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once you init the word2vector model with its parameters and build the vocabulary, \n",
    "# then train the model with more sentences\n",
    "init = time.time()\n",
    "print(init)\n",
    "w2v.train(Text8Corpus(wiki_corpus),total_words=len(w2v.wv.vocab),epochs=w2v.iter)\n",
    "end = time.time()-init\n",
    "print('Word2Vec Model Generated in %d seconds' % end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v.save('/media/abelma/SSD2/wiki-w2v.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load de word2vec model\n",
    "w2v = Word2Vec.load('/media/abelma/SSD2/wiki-w2v.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('pareció', 0.27089589834213257),\n",
       " ('áñez', 0.26660650968551636),\n",
       " ('pteridophytoa', 0.24674996733665466),\n",
       " ('nidificante', 0.2368468940258026),\n",
       " ('carpes', 0.23490853607654572),\n",
       " ('kitty', 0.23462679982185364),\n",
       " ('cahir', 0.23371116816997528),\n",
       " ('phyllanthaceae', 0.2335861325263977),\n",
       " ('enriquecían', 0.2326796054840088),\n",
       " ('plagaron', 0.2292286455631256)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.wv.most_similar(positive=['niño'])#,negative=['hombre'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.6270967e-03, -1.2212507e-03, -1.0092995e-03,  1.5547395e-03,\n",
       "        1.5413483e-03,  8.1122958e-04, -9.3063823e-04, -3.8523821e-04,\n",
       "        4.9518021e-05,  8.3216251e-04], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.wv['rey'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrained Models\n",
    "\n",
    "To generate some of this models are very computing expensive, so it is better to load pre-trained models to calculate the correspondent word vector.\n",
    "\n",
    "[Wikipedia Word2Vec](https://github.com/idio/wiki2vec/raw/master/torrents/enwiki-gensim-word2vec-1000-nostem-10cbow.torrent)\n",
    "\n",
    "[1.5 Gb Google News Word2Vec Corpus](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing)\n",
    "\n",
    "[Homepage for Google's Word2Vec code and pre-trained models](https://code.google.com/archive/p/word2vec/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sklearn Word2Vec-Cosine sentence similarity\n",
    "\n",
    "### Wrangling Data\n",
    "\n",
    "From string-sentences to \"Continue Bag of Word\" numerical vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get similarity betwee 2 sentences with word2vec create it like John2016\n",
    "sentence1 = 'la niña corrió hacia el hueco'\n",
    "sentence2 = 'Alicia corrió hacia el hueco'\n",
    "sent1 = sentence1.split() #sentence in list of words format\n",
    "sent2 = sentence2.split()\n",
    "#Filtering stopwords by hand\n",
    "sent1s = 'niña corrió hueco'\n",
    "sent2s = 'Alicia corrió hueco'\n",
    "sent1sl = sent1s.split()\n",
    "sent2sl = sent2s.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def preproc_data(sentence1, sentence2, model):\n",
    "    \n",
    "    w2v_sent1 = []\n",
    "    w2v_sent2 = []\n",
    "\n",
    "    for i,word in enumerate(sent1):\n",
    "        try:\n",
    "            w2v_sent1.append(w2v.wv[word])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    for i,word in enumerate(sent2):\n",
    "        try:\n",
    "            w2v_sent2.append(w2v.wv[word])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    w2v_sent1 = sum(np.asarray(w2v_sent1))\n",
    "    w2v_sent2 = sum(np.asarray(w2v_sent2))\n",
    "    A = w2v_sent1.reshape(1,-1)\n",
    "    B = w2v_sent2.reshape(1,-1)\n",
    "    \n",
    "    return A,B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.00054531,  0.00163083,  0.00311696, -0.00242069,  0.00249035,\n",
       "        0.00299105, -0.00119631,  0.00505188,  0.00258618,  0.00072918],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nvec_sent1_w2v, nvec_sent2_w2v = preproc_data(sent1,sent2,w2v)\n",
    "nvec_sent1s_w2v, nvec_sent2s_w2v = preproc_data(sent1s,sent2s,w2v)\n",
    "print(len(nvec_sent1_w2v[0]))\n",
    "nvec_sent2s_w2v[0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.83529264"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cosine_similarity(nvec_sent1_w2v,nvec_sent2_w2v)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.83529264"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(nvec_sent1s_w2v,nvec_sent2s_w2v)[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scipy Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16470736265182495\n",
      "0.16470736265182495\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import cosine as cosine_scipy\n",
    "\n",
    "print(cosine_scipy(nvec_sent1_w2v,nvec_sent2_w2v))\n",
    "print(cosine_scipy(nvec_sent1s_w2v,nvec_sent2s_w2v)) #Filtering stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine using Gensim w2v of a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5004435181617737"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_sent1 = w2v.wv[sent1]\n",
    "vec_sent2 = w2v.wv[['corrió','al','hueco']]\n",
    "\n",
    "#cosine(vec_sent1,vec_sent2)\n",
    "vec_sent1_ = vec_sent1.sum(axis=0)\n",
    "vec_sent2_ = vec_sent2.sum(axis=0)\n",
    "\n",
    "1-cosine_scipy(vec_sent1_,vec_sent2_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gensim w2v.n_similarity\n",
    "\n",
    "This method fails when the word is not on the model, try to apply normalization and not capital letters.\n",
    "Test this line:\n",
    "\n",
    "    w2v.n_similarity(sent1,sent2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abelma/wordembd/lib/python3.5/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `n_similarity` (Method will be removed in 4.0.0, use self.wv.n_similarity() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7712078136193977"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.n_similarity(['la', 'niña', 'corrió', 'hacia', 'el', 'hueco'],['alicia', 'corrió', 'hacia', 'el', 'hueco'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abelma/wordembd/lib/python3.5/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `n_similarity` (Method will be removed in 4.0.0, use self.wv.n_similarity() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8432498313143995"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.n_similarity(['niña','corrió','hueco'],['corrió','hueco'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abelma/wordembd/lib/python3.5/site-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `n_similarity` (Method will be removed in 4.0.0, use self.wv.n_similarity() instead).\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.01038645543748053"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Testing diferent meaning sentences\n",
    "w2v.n_similarity(['el','niño','come','una','manzana','roja'],\n",
    "                   ['ella','corrió','al','hueco'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gensim w2v.similarity\n",
    "\n",
    "A score constructed with this method based on an international article.[John2016](#John2016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get similarity betwee 2 sentences with word2vec create it like John2016\n",
    "def sent_sim_jonh2016(sent1, sent2, model, ALPHA):\n",
    "    \"\"\"type sent1,sent2: list of strings\"\"\"\n",
    "    \n",
    "    sim_vector = []\n",
    "\n",
    "    for wordA in sent1:\n",
    "        for wordB in sent2:\n",
    "            try:\n",
    "                sim = w2v.similarity(wordA,wordB)\n",
    "                if sim > ALPHA:\n",
    "                    sim_vector.append(sim)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    return sum(sim_vector)/(len(sim_vector) or 1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence w2v.similarity with stopwords 0.7047605146066211\n",
      "Sentence w2v.similarity without stopwords 1.0\n",
      "0.107300718000851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abelma/wordembd/lib/python3.5/site-packages/ipykernel_launcher.py:10: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "ALPHA = 0.1\n",
    "print('Sentence w2v.similarity with stopwords', sent_sim_jonh2016(sent1, sent2, w2v, ALPHA))\n",
    "print('Sentence w2v.similarity without stopwords',sent_sim_jonh2016(sent1sl, sent2sl, w2v, ALPHA))\n",
    "print(sent_sim_jonh2016(['el','niño','come','una','manzana','roja'],['ella','corrió','al','hueco'],w2v, ALPHA))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Pair Word Overlap\n",
    "\n",
    "Lets try a different way to compound a sentence similarity, based on WordNet-Augmented-Word-Overlap similarity idea.\n",
    "\n",
    "$p = {\\sum_{w\\in\\ sent_1}max(df[w][w']) \\over len(sent_1)} \\ \\ \\ \\forall\\ w' \\in\\ sent_2$\n",
    "\n",
    "$q = {\\sum_{w'\\in\\ sent_2}max(df[w][w']) \\over len(sent_2)} \\ \\ \\ \\forall\\ w \\in\\ sent_1$\n",
    "\n",
    "$sim = \\left\\{ \\begin{array}{rcl} \n",
    "0  & if\\ p+q = 0\\\\\n",
    "{2 p*q \\over (p+q)}  & others\\\\\n",
    "\\end{array}\n",
    "\\right.$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between woman and girl: 0.1297296040355166\n",
      "Harmonic mean word overlap w2v.similarity 0.5921575099918465\n"
     ]
    }
   ],
   "source": [
    "# get similarity between 2 words with word2vec\n",
    "print('Similarity between woman and girl:', w2v.wv.similarity('woman','girl'))\n",
    "\n",
    "sent1 = ['the','girl','run','into','the','hall']\n",
    "sent2 = ['Here','Alice','run','to','the','hall']\n",
    "\n",
    "def harmonic_best_pair_word_sim(sent1,sent2, w2v):\n",
    "    p=0\n",
    "    for wi in sent1:\n",
    "        m = 0\n",
    "        for wc in sent2:\n",
    "            try:\n",
    "                m = max(m, w2v.wv.similarity(wi,wc))\n",
    "            except:\n",
    "                pass\n",
    "        p += m\n",
    "    p = p/len(sent1)\n",
    "\n",
    "    q=0\n",
    "    for wc in sent2:\n",
    "        m = 0\n",
    "        for wi in sent1:\n",
    "            try:\n",
    "                m = max(m, w2v.wv.similarity(wi,wc))\n",
    "            except:\n",
    "                pass\n",
    "        q += m\n",
    "    q = q/len(sent2)\n",
    "\n",
    "    sim = 2*p*q/(p+q or 1)\n",
    "    return sim\n",
    "\n",
    "print('Harmonic mean word overlap w2v.similarity',harmonic_best_pair_word_sim(sent1,sent2, w2v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05429410777519685\n",
      "0.681364062418572\n"
     ]
    }
   ],
   "source": [
    "#If we change the sent1 by a very different meaning sent3\n",
    "sent3 = ['the','boy','eat','a','red','apple']\n",
    "print(harmonic_best_pair_word_sim(sent3,sent2,w2v))\n",
    "\n",
    "#With stopword filtering\n",
    "print(harmonic_best_pair_word_sim(sent1sl,sent2sl,w2v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "- The best similarities using this text representation models must be implemented with innovatives ideas.\n",
    "- The original gensim accuracy test output is different to this one.\n",
    "\n",
    "# Recomendations\n",
    "\n",
    "- Try to test other text representation models like Weigthed Matrix Factorization to study if the problem of sparcity.\n",
    "- Try to train w2v model with more documents and test the Best-Pair word overlap similarity.\n",
    "\n",
    "<a id='referencias'></a>\n",
    "# Referencias\n",
    "\n",
    "<a id='Perkins2014'></a>\n",
    "[1] *[Perkins2014]* Jacov Perkins. \n",
    "Book **Python 3 Text Processing with NLTK 3 Cookbook**. 2014. \n",
    "p. 7 **ISBN**: 978-1-78216-785-3\n",
    "\n",
    "<a id='Mikolov2013'></a>\n",
    "[2] *[Mikolov2013]* Tomas Mikolov et al. **Efficient Estimation of Word Representations in Vector Space**. Publisher [arXiv](https://arxiv.org/abs/1301.3781), 2013.\n",
    "\n",
    "<a id='John2016'></a>\n",
    "[3] *[John2016]* John, Adebayo Kolawole and Caro, Luigi Di and Boella, Guido. \n",
    "**NORMAS at SemEval-2016 Task 1: SEMSIM: A Multi-Feature Approach to Semantic Text Similarity**. \n",
    "Publisher ACM, 2016."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
